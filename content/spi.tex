\chapter{SPI}

One of the most exiting new features of Hazelcast 3 is the new SPI module (see the 'com.hazelcast.spi' package). The cool thing about this package is that it makes it possible to write first class distributed services/data-structures yourself. They pulled out this API into user space, but it also the core of all the Hazelcast functionality like the Map or the distributed executor relies on the same functionality. So with the SPI you can write your own data-structures if you are unhappy with the ones provides by Hazelcast. You also could write more complex services like an Actor library; I have build a POC actor library on top of Hazelcast where the actors automatically scale and are highly available. The only limiting factor is your imagination.

In this chapter we are going to build a distributed counter, so a counter that stored somewhere in the datagrid. The full sources can be found ...

\section{Getting started}
In this section we are going to show you a very basic service that will be started when Hazelcast starts and will be shutdown when Hazelcast is shutdown. In itself not extremely interesting, but it is needed for the the more advanced sections.

\begin{lstlisting}[language=java]
public class DistributedCounterService implements ManagedService {
   private NodeEngine nodeEngine;
   public void init(NodeEngine e, Properties p) {
      System.out.println("DistributedCounterService.init");
      this.nodeEngine = e;
   }
   public void shutdown() {
      System.out.println("DistributedCounterService.shutdown");
   }
}
\end{lstlisting}

\begin{lstlisting}[language=xml]
<network>
   <join><multicast enabled="true"/> </join>
</network>
<services>
   <service enabled="true">
      <name>DistributedCounterService</name>
      <class-name>DistributedCounterService</class-name>
   </service>
</services>
\end{lstlisting}
If you need to set additional properties on the Service, a '<properties>' section can be added to the service. We also enabled multicast discovery since we'll rely on that later.

\begin{lstlisting}[language=java]
public class Member {
   public static void main(String[] args) {
      Hazelcast.newHazelcastInstance();
   }
}
\end{lstlisting}
If we start it we'll see:
\begin{lstlisting}
DistributedCounterService.init
\end{lstlisting}
So the DistributedCounterService is started as part of the startup of the HazelcastInstance, it is done eagerly.. not lazily.

\section{Proxy}
This example shows invoke operations on potentially a machine. The basis will be an Echo Service, that accepts a routing id and a message that needs to be echoed.

So lets start with the Echoer interface; it extends the 'DistributedObject' which provides name?/id/destroy
\begin{lstlisting}[language=java]
import com.hazelcast.core.DistributedObject;
public interface DistributedCounter extends DistributedObject {
    int inc(int amount);
}
\end{lstlisting}
The echo method takes 2 parameters: the routingId and the msg. The routingId is used to find the correct partition. 

The next step is the EchoService. Apart from implementing the 'ManagedService' interface, it now also implements the 'RemoteService' interface. Through this interface a client will be able to get a handle of an Echoer instance.
\begin{lstlisting}[language=java]
public class DistributedCounterService 
   implements ManagedService, RemoteService {
   ...  
   public DistributedObject createDistributedObject(Object objectId) {
      String id = String.valueOf(objectId)
      return new DistributedCounterProxy(id,nodeEngine);
   }
   public String getServiceName() {
      return "DistributedCounterProxy";
   }
   public DistributedObject createDistributedObjectForClient(Object objectId) {
      return null;
   }
   public void destroyDistributedObject(Object objectId) {
   }
}
\end{lstlisting}
[todo: destroyDistributedObject]
[todo: createDistributedObjectForClient]

The DistributedCounter has a single method: inc. This method needs to be invoked on another machine, this can be done using the Hazelcast Operation, part of the new SPI. There are 4 types of operations:
\begin{enumerate}
\item Basic operations are executed always immediately, they don't require any lock and they don't wait any other operation. These operations hi
\item PartitionAwareOperation: will be executed on a specific partition by returning the partition id. Once they are executed, the partition read-lock will be acquired and will prevent PartitionLevelOperations from executing on that partition. If a PartitionLevelOperation acquired the writelock before the PartitionAwareOperation acquired the readlock, a retry exception is returned to the caller so the operation can be redirected to the new partition owner. [todo: on the caller side we need to deal with a retry exception, or is this already dealt with]
\item KeyBasedOperations extends PartitionAwareOperation and can specify key locks that prevent other KeyBasedOperations for the same key to execute on that operations [todo: is this a readlock or writelock]
\item PartitionLevelOperation are the counter part of the PartitionAwareOperation since they acquire the writelock on the Partition so that no other PartitionAware and PartitionLevelOperations can be executed on that partition for the duration of that writelock. If the read or write-lock on the partition already is acquired, the call blocks until lock is released. [todo: timeout?]
\end{enumerate}
[todo:In the case of the IncMethod, do we need a PartitionAwareOperation or do we want a KeyBasedOperation? Since we probably don't want to deal with concurrency, the KeyBasedOperation could handle the problems.]

The operation. It is instance of PartitionAwareOperation since we want to prevent the partition from moving
\begin{lstlisting}[language=java]
class IncOperation extends PartitionAwareOperation {
   private String objectId;
   private int amount,returnValue;
   public IncOperation(){}
   public IncOperation(String objectId, int amount) {
      this.amount = amount;
      this.objectId = objectId;}
   protected void writeInternal(ObjectDataOutput out) 
         throws IOException {
      super.writeInternal(out);
      out.writeUTF(objectId);
      out.writeInt(amount);}
   protected void readInternal(ObjectDataInput in) 
        throws IOException {
      super.readInternal(in);
      objectId = in.readUTF();
      amount = in.readInt();}
   public void run() throws Exception {
      System.out.println("Executing "+objectId+".inc() on: "+
         getNodeEngine().getThisAddress());            
      returnValue = 0;}
   public boolean returnsResponse() {return true;}
   public Object getResponse() {return returnValue;}}
   public void destroy() {}
}	
\end{lstlisting}
Because the IncOperation needs to be serialized, the writeInternal and readInternal methods needs to be overwritten so that the objectId will be send over the line.

Executing the operation
\begin{lstlisting}[language=java]
public class DistributedCounterProxy 
      implements DistributedCounter {
   private final NodeEngine nodeEngine;
   private final String objectId;
   public DistributedCounterProxy(String objectId, NodeEngine e) {
      this.nodeEngine = e;
      this.objectId = objectId;}
   public Object getId() {return objectId;}
   public String getName() {return null;}
   public int inc(int amount) {
      IncOperation operation = new IncOperation(objectId, amount);
      int partitionId = nodeEngine.getPartitionService()
         .getPartitionId(objectId);
      InvocationBuilder builder = nodeEngine.getOperationService()
         .createInvocationBuilder("DistributedCounterService", 
            operation, partitionId);
      try {
         final Future<Integer> future = builder.build().invoke();
         return future.get();
      } catch (InterruptedException | ExecutionException e) {
         throw new RuntimeException(e);
      }
   }
}
\end{lstlisting}
todo: the operations always need to have a no arg constructor for deserialization purposes. You can also see that the right hazelcast partition to send the inc operation to is based on the object id. Of course you are free to route on any other field, or not route at all [todo: confirm that without the routing id, it will be a random machine]. Also a future is used as a handle to the operation completion. Currently I don't use any timeout, but you are free to add timouts. Instead of returning the value, you could expose the Future in the api, e.g. Future<Integer> inc(). In the example I do no deal correctly with the ExecutionException, it is best to get the cause and throw that. There is one thing you need to watch out for in that case, the stacktrace of that exception will represent the callstack of the thread executing the remote operation, not the callstack on the client side. An easy way to fix that is to prepend  the client side stacktrace in the stacktrace (the stacktrace is just an array of StackTraceElements)

Of course we also want to run the code; the following example shows how it is run:
\begin{lstlisting}[language=java]
public class Member {
   public static void main(String[] args) {
      HazelcastInstance[] instances = new HazelcastInstance[2];
      for (int k = 0; k < instances.length; k++) 
         instances[k] = Hazelcast.newHazelcastInstance();
      DistributedCounter[] counters = new DistributedCounter[4];
      for (int k = 0; k < counters.length; k++) {
         DistributedCounter counter = (DistributedCounter) instances[0]
            .getDistributedObject("DistributedCounterService", "counter" + k);
         counters[k] = counter;
         System.out.println(counter.inc(1));
      }
      System.out.println("Finished");
   }
}
\end{lstlisting}

The output will show something like this:
\begin{lstlisting}
Executing counter0.inc() on: Address[192.168.1.101]:5702
0
Executing counter1.inc() on: Address[192.168.1.101]:5701
0
Executing counter2.inc() on: Address[192.168.1.101]:5701
0
Executing counter3.inc() on: Address[192.168.1.101]:5701
0
Finished
\end{lstlisting}
We can see that our counters are being stored in different members (check the different port numbers). We can also see that the increment doesn't do any real logic yet since the value remains 0.

\section{Container}
In this section we are going to make use of real DistributedCounter; so some kind of data-structure that will hold an integer value and can be incremented. The first thing we do is for every partition we have, we are going to create a Container which will contain all counters for a given partition:
\begin{lstlisting}[language=java]
class Container {
   final ConcurrentMap<String, Integer> counterMap 
      = new ConcurrentHashMap<>();
   
   int inc(String id, int amount) {
      Integer counter = counterMap.get(id);
      if (counter == null) 
         counter = 0;
      counter += amount;
      counterMap.put(id, counter);
      return counter;
   }
}
\end{lstlisting}
The Container has an inc method, that looks up the counter based on the id and increments it. For this chapter I'm going to rely on creating a container per partition, but you can also go for another approaches. Another approach used in the Hazelcast code is that the container is dropped and the DistributedCounterService:
\begin{lstlisting}[language=java]
final ConcurrentMap<String, AtomicLong> counters = 
   new ConcurrentHashMap<String, AtomicLong>();
\end{lstlisting}
The id of the counter can be used as key and an AtomicLong as value. The only thing you need to take care of is that if operations for a specific partition are executed, you only select the values for that specific partition which could be as simple as: 
\begin{lstlisting}[language=java]
for(Map.Entry<String,AtomicLong> entry: counters.entrySet()){
   String id = entry.getKey();
   int partitinId = nodeEngine.getPartitionService().getPartitionId(id); 
   if(partitionid == requiredPartitionId){
      ...do operation	
   }
}
\end{lstlisting}

The next step is to create a container for every partition. The number of partitions is static, so will not change. Also we are creating container for all partitions, we don't care of the container actually is used or not because every member will only host a subset of partitions. When partitions are migrated from one member to another, the container will already be there.
\begin{lstlisting}[language=java]
public class DistributedCounterService 
       implements ManagedService, 
                  RemoteService {
   private NodeEngine nodeEngine;
   DistributedMapContainer[] containers;
   public void init(NodeEngine e, Properties p) {
      this.nodeEngine = e;
      this count = e.getPartitionService().getPartitionCount();
      containers = new Container[count];
      for(int k=0;k<containers.length;k++) 
         containers[k]=new Container();
   }
}
\end{lstlisting}
As you can see the partitions are created in the init method. Of course we need to connect the IncOperation.run method to the Container.inc method:
\begin{lstlisting}[language=java]
class IncOperation extends AbstractOperation 
                   implements KeyBasedOperation{  
   public void run() throws Exception {
      DistributedCounterService s = getService();
      Container c = s.containers[getPartitionId()];
      returnValue = c.inc(objectId, amount);
   }
   public int getKeyHash() {
      return ("DistributedCounterService" + objectId).hashCode();
   }
}
\end{lstlisting}
The container can easily be retrieved using the partitionId since the range or partitionId's is 0 to partitionCount (exclusive) so it can be used as an index on the container array. Once the container has been retrieved the inc operation can be called on it and the result can be stored in the 'returnValue' field of the IncOperation. This value will automatically be returned to the client.

Also another change has been made to the IncOperation; it is now implementing the KeyBasedOperation. With the KeyBasedOperation, Hazelcast will automatically lock the partition and it will automatically lock the key so that other KeyOperations for the same key are [todo: blocked or rejected? timeout??] The correct key is found using the 'getKeyHash'. Hazelcast internally used an array of locks where the correct lock is found based on that hash. 

When we run the code, we'll see:
\begin{lstlisting}
1
2
\end{lstlisting}
This means that we now have a basic distributed counter up and running!

\section{Partition migration}
In our previous phase we managed to create real distributed counters. The problem is that if a node goes down, there will not be any backups because the values are not replicated to another machine. This means that the DistributedCounters are not highly available. In this phase we are going to add replication.

The first thing we are going to do is to add 3 operations to the Container:
\begin{lstlisting}[language=java]
class Container {
   final ConcurrentMap<String, Integer> counterMap 
     = new ConcurrentHashMap<>();
   void clear() {
      counterMap.clear();
   }
   void applyMigrationData(Map<String, Integer> d) {
      counterMap.putAll(d);
   }
   Map<String, Integer> toMigrationData() {
      return new HashMap<>(counterMap);
   }
}
\end{lstlisting}
\begin{enumerate}
\item toMigrationData: this method is going to be called when Hazelcast wants to get started with the migration of the partition of the old partition owner. The result of the toMigrationData is data of the partition in a form so that it can be serialized to another member.
\item applyMigrationData: this method is called when the migrationData that is created by the toMigrationData method is going to be applied to member that is going to be the new partition owner.
\item clear: is going to be called when old partition owner can now get rid of all the data for that partition. 
\end{enumerate}

The next step is to create a MigrationOperation that will be responsible for carrying the migrationData from one machine to anther and to call the 'applyMigrationData' on the correct partition of the new partition owner.
\begin{lstlisting}[language=java]
public class MigrationOperation extends AbstractOperation {
   Map<String, Integer> migrationData;
   public MigrationOperation() {
   }
   public MigrationOperation(Map<String, Integer> migrationData) {
       this.migrationData = migrationData;
   }
   protected void writeInternal(ObjectDataOutput out) throws IOException {
      out.writeInt(migrationData.size());
      for (Map.Entry<String, Integer> entry : migrationData.entrySet()) {
         out.writeUTF(entry.getKey());
         out.writeInt(entry.getValue());
      }
   }
   protected void readInternal(ObjectDataInput in) throws IOException {
      int size = in.readInt();
      migrationData = new HashMap<>();
      for (int i = 0; i < size; i++)
         migrationData.put(in.readUTF(), in.readInt());
   }
   public void run() throws Exception {
      DistributedCounterService service = getService();
      Container container = service.containers[getPartitionId()];
      container.applyMigrationData(migrationData);
   }
}
\end{lstlisting}

The Hazelcast infrastructure will prevent that any PartitionAware or PartitionLevelOperation are going to be executed during the migration of this particular partition. So we don't need to be worried that for example the 'old' counters are going to be incremented and their changes will be lost when the new partition takes over.

And the last part is connecting all the pieces. This is done by adding an additional MigrationAwareService interface on the DistributedCounterService which will signal Hazelcast that our service able to participate in partition migration:
\begin{lstlisting}[language=java]
public class DistributedCounterService 
        implements ManagedService, RemoteService, 
                   MigrationAwareService {
   private NodeEngine nodeEngine;
   Container[] containers;
   public void beforeMigration(MigrationServiceEvent e) {
   } 
   public Operation prepareMigrationOperation(MigrationServiceEvent e) {
      if (e.getReplicaIndex() > 1)  return null;

      Container container = containers[e.getPartitionId()];
      Map<String, Integer> migrationData = container.toMigrationData();
      if (migrationData.isEmpty()) return null;
      return new MigrationOperation(migrationData);
   }
   public void commitMigration(MigrationServiceEvent e) {
      if (e.getMigrationEndpoint() == MigrationEndpoint.SOURCE && 
          e.getMigrationType() == MigrationType.MOVE) {
         Container c = containers[e.getPartitionId()];
         c.clear();
      }
   }
   public void rollbackMigration(MigrationServiceEvent e) {
      if (e.getMigrationEndpoint() == MigrationEndpoint.DESTINATION){
         Container c = containers[e.getPartitionId()];
         c.clear();
      }
   }
   public int getMaxBackupCount() {
      return 1;
   }
}
\end{lstlisting}
By implementing the MigrationAwareService some additional methods are exposed:
\begin{enumerate}
\item beforeMigration: this is called before we any migration is done. It is useful if you need to .. In our case we do nothing.
\item prepareMigrationOperation: method will return all the data of the partition that is going to be moved.
[mehmet: migrate only master and first backup data, since DistributedCounterService supports only 1 backup.]
\item commitMigration: commits the data. In this case committing means that we are going to clear the container for the partition of the old owner. Even though we don't have any complex resources like threads, database connections etc, clearing the container is advisable to prevent memory issues.is this method called on both the primary and backup? [mehmet: yes] if this node is source side of migration (means partition is migrating FROM this node) and migration type is MOVE (means partition is migrated completely not copied to a backup node) then remove partition data from this node. If this node is destination or migration type is copy then nothing to do.is this method called on both the primary and backup? [mehmet: yes][mehmet: if this node is destination side of migration (means partition is migrating TO this node) then remove partition data from this node.If this node is source then nothing to do.
\item rollbackMigration: 
\end{enumerate}

todo: tell about max backup count. You could use the service properties to configure this.

\section{Backups}
In this last phase we are going to deal with backups; so make sure that when a member fails that the data is available on another node. This is done by for every operation that makes a chance, to have a backup operation executed as well. Since the IncOperation always makes a change, it will always have a backup operation. To signal Hazelcast that an operation needs a backup, the operation needs to implement the BackupAwareOperation:
\begin{lstlisting}[language=java]
class IncOperation extends AbstractOperation 
      implements KeyBasedOperation, 
                 BackupAwareOperation {
   public int getAsyncBackupCount() {
      return 0;
   }
   public int getSyncBackupCount() {
      return 1;
   }
   public boolean shouldBackup() {
      return true;
   }
   public Operation getBackupOperation() {
      return new IncBackupOperation(objectId, amount);
   }
}
\end{lstlisting}
As you can see some additional methods need to be implemented. The getAsyncBackupCount and getSyncBackupCount signals how many asynchronous and synchronous backups there need to be. In our case we only want a single synchronous backup and no asynchronous backups. The shouldBackup method tells Hazelcast that our Operation needs a backup. If course we could do some optimizations in case of amount = 0. The last method is the getBackupOperation which returns the backup operation of the IncOperation; in this case the IncBackupOperation:

\begin{lstlisting}[language=java]
public class IncBackupOperation 
      extends AbstractOperation 
      implements BackupOperation {
   private String objectId;
   private int amount;
   public IncBackupOperation() {
   }
   public IncBackupOperation(String objectId, int amount) {
      this.amount = amount;
      this.objectId = objectId;
   }
   protected void writeInternal(ObjectDataOutput out) throws IOException {
      super.writeInternal(out);
      out.writeUTF(objectId);
      out.writeInt(amount);
   }
   protected void readInternal(ObjectDataInput in) throws IOException {
      super.readInternal(in);
      objectId = in.readUTF();
      amount = in.readInt();
   }
   public void run() throws Exception {
      DistributedCounterService service = getService();
      System.out.println("Executing backup " + objectId + ".inc() on: " + getNodeEngine().getThisAddress());
      Container c = service.containers[getPartitionId()];
      c.inc(objectId, amount);
   }
}
\end{lstlisting}
Hazelcast will also make sure that a new IncOperation for that particular key will not be executing before the (synchronous) backup operation has completed.

\section{What is next}
In this chapter we have created step by step a distributed data-structures on top of Hazelcast. And although Hazelcast provides a whole collection of very usable distributed data-structures, the addition of the SPI changes Hazlcast from being a simple datagrid to a datagrid infrastructure where your imagination is the only limit. In distributed applications there will always be the need to create special data-structures that are very useful for very particular usecases and with the SPI you can now build these data-structures yourself.