\chapter{SPI}

One of the most exiting new features of Hazelcast 3 is the new SPI module (see the 'com.hazelcast.spi' package). The cool thing about this package is that it makes it possible to write first class distributed services/data-structures yourself. They pulled out this API into user space, but it also the core of all the Hazelcast functionality like the Map or the distributed executor relies on the same functionality. So with the SPI you can write your own data-structures if you are unhappy with the ones provides by Hazelcast. You also could write more complex services like an Actor library; I have build a POC actor library on top of Hazelcast where the actors automatically scale and are highly available. The only limiting factor is your imagination.

In this chapter we are going to build a distributed counter:
\begin{lstlisting}[language=java]
public interface Counter{
    int inc(int amount);
}
\end{lstlisting}
This counter will be stored in Hazelcast and the same counter can be called by different members. It will also be scalable; so the capacity for the number of counters scales with the number of members in the cluster. And it will be highly available, so if a member hosting that counter fails, a backup will already be available on a different member and the system will continue as if nothing happened. We are going to do this step by step; in each section a new piece of functionality is going to be added.

\section{Getting started}
In this section we are going to show you a very basic CounterService which lifecycle is going to be managed by Hazelcast. In itself not extremely interesting, but it is the first part that needs to be in placed for the Counter functionality.

The CounterService needs to implement the ManagedService
\begin{lstlisting}[language=java]
public class CounterService implements ManagedService {
   private NodeEngine nodeEngine;
   public void init(NodeEngine e, Properties p) {
      System.out.println("CounterService.init");
      this.nodeEngine = e;
   }
   public void shutdown() {
      System.out.println("CounterService.shutdown");
   }
   public void reset() {
   }
}
\end{lstlisting}
As you can see 2 methods are added: the init and the shutdown. These methods will be called by Hazelcast when the HazelcastInstance is initialized and is shutdown. The init method also received the NodeEngine, which provides access to the internals, like the PartitionService, of Hazelcast. The next step is to enable the CounterService; in our case we are going to make use of the hazelcast.xml:
\begin{lstlisting}[language=xml]
<network>
   <join><multicast enabled="true"/> </join>
</network>
<services>
   <service enabled="true">
      <name>CounterService</name>
      <class-name>CounterService</class-name>
   </service>
</services>
\end{lstlisting}
If you need to set additional properties on the Service, a '<properties>' section can be added to the service. These properties will be injected in the CounterService.init method. We also enabled multicast discovery since we'll rely on that later.

Of course we want to see this in action:
\begin{lstlisting}[language=java]
public class Member {
   public static void main(String[] args) {
      Hazelcast.newHazelcastInstance();
   }
}
\end{lstlisting}
If we start it we'll see the following output:
\begin{lstlisting}
CounterService.init
\end{lstlisting}
So the CounterService is eagerly started as part of the startup of the HazelcastInstance.

\section{Proxy}
In the previous section we created a CounterService that gets started when Hazelcast starts, but apart from that it doesn't do anything yet. In this section we are going connect the Counter interface to the CounterService and we are going to do a remote call on member hosting the actual counter data/logic and we'll return the result.

The first thing we are going to do is to let the Counter implement the DistributedObject interface to indicate that it is a distributed object. Also some methods will be exposed like getName,getId and destroy:
\begin{lstlisting}[language=java]
public interface Counter extends DistributedObject {
    int inc(int amount);
}
\end{lstlisting}

The next step is fixing the CounterService. Apart from implementing the 'ManagedService' interface, it now also implements the 'RemoteService' interface. Through this interface a client will be able to get a handle of an Counter proxies. 
\begin{lstlisting}[language=java]
public class CounterService 
   implements ManagedService, RemoteService {
   ...  
   public DistributedObject createDistributedObject(Object objectId) {
      String id = String.valueOf(objectId)
      return new CounterProxy(id,nodeEngine);
   }
   public String getServiceName() {
      return "CounterService";
   }
   public DistributedObject createDistributedObjectForClient(Object objectId) {
      return null;
   }
   public void destroyDistributedObject(Object objectId) {
   }
}
\end{lstlisting}
[todo: destroyDistributedObject] [mehmet: destroy is implemented for proxies extending AbstractDistributedObject]
[todo: createDistributedObjectForClient] [mehmet: after my poc for binary client impl, we can change or remove this method!]

As you can see the createDistributedObject returns a CounterProxy. This proxy is a local representation to (potentially) remote managed data/logic.  
\begin{lstlisting}[language=java]
public class CounterProxy 
      implements Counter {
   private final NodeEngine nodeEngine;
   private final String objectId;
   public CounterProxy(String objectId, NodeEngine e) {
      this.nodeEngine = e;
      this.objectId = objectId;
   }
   public Object getId() {return objectId;}
   public String getName() {return objectId;}
   public int inc(int amount) {
      IncOperation operation = new IncOperation(objectId, amount);   
      int partitionId = nodeEngine.getPartitionService()
         .getPartitionId(objectId);
      InvocationBuilder builder = nodeEngine.getOperationService()
         .createInvocationBuilder("CounterService", 
            operation, partitionId);
      Invocation invocation = builder.build();
      try {
         final Future<Integer> future = invocation.invoke();
         return future.get();
      } catch(Exception e){
         throw ExceptionUtil.rethrow(e);
      }
   }
}
\end{lstlisting}
The CounterProxy doesn't contain counter state; it is just a local representative of remote data/functionality. Therefor the CounterProxy.inc method needs to be invoked on the machine for hosting the partition that contains the real counter. This can be done using the Hazelcast SPI which takes care of sending operations to the correct machine, acquire locks if needed, execute the operation, and returning the results.

If you take a closer look at the inc method, the first thing we do is to create the IncOperation with the given objectId and the amount. The next step is to get the partitionId; this is done based on the objectId so that all operations for a given objectId will always result in the same partitionId. Then we create an Invocation based on the operation and the partitionId using the InvocationBuilder. This is where the connection is made between the operation and the partition. 

The last part of the logic is actually invoking the Invocation and waiting for its result. This is done using a Future which gives us the ability to synchronize on completion of that remote executed operation and to get the results. In our case we do a simple get since we don't care about a timeout, but for real system it often is better to use a timeout since most operations should complete in a certain amount of time. If they don't complete, it could be a sign of problems and waiting indefinitely could lead to stalling systems without any form of error logging.

If the execution of the operation fails with an exception, an ExecutionException is thrown and needs to be dealt with. Luckily Hazelcast provides a utility function for that: ExceptionUtil.rethrow(Throwable). If you want to keep the checked exception, you need to deal with exception handling yourself and the ExceptionUtil  is not of much use. 
A nifty improvement for debugging is that that if remote exception is  stacktrace of the cause of the Exception is enhanced so that it includes client and serverside stacktrace. This way you can see on both sides which method caused problems.

If the exception is an InterruptedException, you can do 2 things. Either propagate the InterruptedException since it is a good practice for blocking methods. In this case you first need to do something like this:
\begin{lstlisting}[language=java]
  try {
     final Future<Integer> future = invocation.invoke();
     return future.get();
  } catch(InterruptedException e){
     throw e;
  } catch(Exception e){
     throw ExceptionUtil.rethrow(e);
  }
\end{lstlisting}
In our case we don't care about the InterruptedException and therefor we catch all exceptions and let it be handled by the ExceptionUtil and it will be wrapped in a HazelcastException and the interrupt status will be set.

Currently it isn't possible to abort an Operation by calling the future.cancel. Perhaps this will be added in a later release.

There are 4 types of operations:
\begin{enumerate}
\item Basic operations are executed always immediately, they don't require any lock and they don't wait any other operation.
\item PartitionAwareOperation: will be executed on a specific partition. Before they are executed, the partition read-lock will be acquired preventing MigrationOperation from executing on that partition. If a MigrationOperation acquired the write-lock before the PartitionAwareOperation acquired the read-lock, a retry exception is returned to the caller so the operation can be redirected to the new partition owner. This exception will be handled by Hazelcast, but you can influence its behavior. By configuring the InvocationBuilder you can change the number of try counts (defaults to 100), the pause between retries (defaults to 500ms), But you can also Operations can deal with exceptions by overriding the Operation.onException(Throwable throwable) method.
\item KeyBasedOperations extends PartitionAwareOperation and can specify key locks that prevent other KeyBasedOperations for the same key to execute on that key. The lock that is acquired for the key is an exclusive lock, so concurrent executions of  KeyBasedOperations for the same key is not possible. If this causes a performance/scalability problems, instead of relying on the exclusive locking are more fine grained custom approach can be applied. For example, if we want to introduce a Counter.get, we could store the counter value in a wrapper object as a volatile field so that only a volatile read is needed.
\item MigrationOperation are the counter part of the PartitionAwareOperation since they acquire the write-lock on the partition so that no other PartitionAware, KeyBasedOperations and MigrationOperation can be executed on that partition for the duration of that write-lock. If the read or write-lock on the partition already is acquired, the call blocks until lock is released. [todo: timeout?] MigrationOperations should only be used by Hazelcast internally.
\end{enumerate}


In our example the IncOperation is going to make use of the KeyBasedOperation to prevent dealing with concurrency control ourselves:
\begin{lstlisting}[language=java]
class IncOperation 
      extends PartitionAwareOperation, 
              KeyBasedOperation{
   private String objectId;
   private int amount,returnValue;
   public IncOperation(){}
   public IncOperation(String objectId, int amount) {
      this.amount = amount;
      this.objectId = objectId;}
   public boolean returnsResponse() {return true;}
   public Object getResponse() {return returnValue;}}
   public void destroy() {}
   protected void writeInternal(ObjectDataOutput out) 
         throws IOException {
      super.writeInternal(out);
      out.writeUTF(objectId);
      out.writeInt(amount);
   }
   protected void readInternal(ObjectDataInput in) 
        throws IOException {
      super.readInternal(in);
      objectId = in.readUTF();
      amount = in.readInt();
   }
   public void run() throws Exception {
      System.out.println("Executing "+objectId+".inc() on: "+
         getNodeEngine().getThisAddress());            
      returnValue = 0;
   }
   public int getKeyHash() {
      return ("CounterService" + objectId).hashCode();
   }
}	
\end{lstlisting}
Hazelcast will determine the key to lock with the getKeyHash method. Internally Hazelcast uses a similar approach as being used as the lock-stripe in the ConcurrentHashMap. The keyhash is used as a basis for the index, so same keyhashes acquire the same lock. In theory it can happen that a lock collision is encountered, but the lock stripe is quite big: a static value of 100.000.

[todo: what about timeouts?]

Another thing worth mentioning, is that because the IncOperation needs to be serialized, the writeInternal and readInternal methods needs to be overwritten, so that the objectId and amount are serialized and will be available when this operation runs. For deserialization it also is mandatory that the operation has a no arg constructor.

Of course we also want to run the code:
\begin{lstlisting}[language=java]
public class Member {
   public static void main(String[] args) {
      HazelcastInstance[] instances = new HazelcastInstance[2];
      for (int k = 0; k < instances.length; k++) 
         instances[k] = Hazelcast.newHazelcastInstance();
      Counter[] counters = new Counter[4];
      for (int k = 0; k < counters.length; k++) {
         Counter counter = (Counter) instances[0]
            .getDistributedObject("CounterService", "counter" + k);
         counters[k] = counter;
         System.out.println(counter.inc(1));
      }
      System.out.println("Finished");
   }
}
\end{lstlisting}
In this example we start some Hazelcast instances and then we create  some Counters which are incremented. When this we run this member, we'll get output like this:
\begin{lstlisting}
Executing counter0.inc() on: Address[192.168.1.101]:5702
0
Executing counter1.inc() on: Address[192.168.1.101]:5701
0
Executing counter2.inc() on: Address[192.168.1.101]:5701
0
Executing counter3.inc() on: Address[192.168.1.101]:5701
0
Finished
\end{lstlisting}
We can see that our counters are being stored in different members (check the different port numbers). We can also see that the increment doesn't do any real logic yet since the value remains 0. This problem we are going to solve in the next section.

\section{Container}
In this section we are going to upgrade the functionality so that it features a real distributed counter. So some kind of data-structure will hold an integer value and can be incremented. The first thing we do is for every partition there is in the system, we are going to create a Container which will contain all counters for a given partition:
\begin{lstlisting}[language=java]
class Container {
   final ConcurrentMap<String, Integer> counterMap 
      = new ConcurrentHashMap<>();
   
   int inc(String id, int amount) {
      Integer counter = counterMap.get(id);
      if (counter == null) 
         counter = 0;
      counter += amount;
      counterMap.put(id, counter);
      return counter;
   }
}
\end{lstlisting}
The Container has an inc method, that looks up the counter based on the id and increments it. 

For this chapter I rely on Container instance per partition, but you have complete freedom how to go about it. A different approach used in the Hazelcast is that the Container is dropped and the CounterService has a map of counters:
\begin{lstlisting}[language=java]
final ConcurrentMap<String, Integer> counters = 
   new ConcurrentHashMap<String, Integer>();
\end{lstlisting}
The id of the counter can be used as key and an Integer as value. The only thing you need to take care of is that if operations for a specific partition are executed, you only select the values for that specific partition. This can be as simple as: 
\begin{lstlisting}[language=java]
for(Map.Entry<String,Integer> entry: counters.entrySet()){
   String id = entry.getKey();
   int partitinId = nodeEngine.getPartitionService().getPartitionId(id); 
   if(partitionid == requiredPartitionId){
      ...do operation	
   }
}
\end{lstlisting}
Its a personal taste which solution is preferred. Personally I like the container approach since there will not be any mutable shared state between partitions. It also makes operations on partitions simpler since you don't need to filter out data that doesn't belong to a certain partition.

The next step is to create container per partition which will be stored in an array since the number of partitions is static. In practice each member will only host a subset of the partitions so since we create a container per partition up front, it could be that a container isn't used. But since its size in small, it isn't very important and also makes the examples a bit simpler.
\begin{lstlisting}[language=java]
public class CounterService 
       implements ManagedService, 
                  RemoteService {
   private NodeEngine nodeEngine;
   DistributedMapContainer[] containers;
   public void init(NodeEngine e, Properties p) {
      this.nodeEngine = e;
      int count = e.getPartitionService().getPartitionCount();
      containers = new Container[count];
      for(int k=0;k<containers.length;k++) 
         containers[k]=new Container();
   }
   ...
}
\end{lstlisting}
As you can see the partitions are created in the init method. The last step is connecting the IncOperation.run method to the Container.inc method:
\begin{lstlisting}[language=java]
class IncOperation extends AbstractOperation 
                   implements KeyBasedOperation{  
   public void run() throws Exception {
      CounterService s = getService();
      Container c = s.containers[getPartitionId()];
      returnValue = c.inc(objectId, amount);
   }
   ...  
}
\end{lstlisting}
The container can easily be retrieved using the partitionId since the range or partitionId's is 0 to partitionCount (exclusive) so it can be used as an index on the container array. Once the container has been retrieved the inc operation can be called on it and the result can be stored in the 'returnValue' field of the IncOperation. This value will automatically be returned to the client.

When we run the code, we'll see:
\begin{lstlisting}
1
2
\end{lstlisting}
This means that we now have a basic distributed counter up and running!

\section{Partition migration}
In our previous phase we managed to create real distributed counters. The only problem is that when members are leaving or joining the cluster, that the partitions are not migrating to different members. In this section we are going to just that: partition migration.

The first thing we are going to do is to add 3 new operations to the Container:
\begin{lstlisting}[language=java]
class Container {
   final ConcurrentMap<String, Integer> counterMap = 
       new ConcurrentHashMap<>();
   void clear() {
      counterMap.clear();
   }
   void applyMigrationData(Map<String, Integer> d) {
      counterMap.putAll(d);
   }
   Map<String, Integer> toMigrationData() {
      return new HashMap<>(counterMap);
   }
   ...
}
\end{lstlisting}
\begin{enumerate}
\item toMigrationData: this method is going to be called when Hazelcast wants to get started with the migration of the partition on the member that currently owns the partition. The result of the toMigrationData is data of the partition in a form so that it can be serialized to another member.
\item applyMigrationData: this method is called when the migrationData that is created by the toMigrationData method is going to be applied to member that is going to be the new partition owner.
\item clear: is going to be called for 2 reasons: when the partition migration has succeeded and the old partition owner can get rid of all the data in the partition. And also when the partition migration operation fails and given the 'new' partition owner needs to roll back its changes.
\end{enumerate}

The next step is to create a CounterMigrationOperation that will be responsible for carrying the migrationData from one machine to anther and to call the 'applyMigrationData' on the correct partition of the new partition owner.
\begin{lstlisting}[language=java]
public class CounterMigrationOperation 
       extends AbstractOperation {
   Map<String, Integer> migrationData;
   public CounterMigrationOperation() {
   }
   public CounterMigrationOperation(Map<String, Integer> d) {
       this.migrationData = d;
   }
   protected void writeInternal(ObjectDataOutput out) 
         throws IOException {
      out.writeInt(migrationData.size());
      for (Map.Entry<String, Integer> entry : migrationData.entrySet()) {
         out.writeUTF(entry.getKey());
         out.writeInt(entry.getValue());
      }
   }
   protected void readInternal(ObjectDataInput in) 
         throws IOException {
      int size = in.readInt();
      migrationData = new HashMap<>();
      for (int i = 0; i < size; i++)
         migrationData.put(in.readUTF(), in.readInt());
   }
   public void run() throws Exception {
      CounterService service = getService();
      Container container = service.containers[getPartitionId()];
      container.applyMigrationData(migrationData);
   }
}
\end{lstlisting}
The Hazelcast infrastructure will prevent that any PartitionAwareOperation or KeyBasedOperation are going to be executed during the execution of the MigrationOperation. So we don't need to be worried that for example the 'old' counters are going to be incremented and their changes will be lost when the new partition takes over. The CounterMigrationOperation should not extend MigrationOperation, but should extend AbstractOperation. The reason is that before the CounterMigrationOperation is executed, the write-lock on the partition already is acquired since we don't want any counter updates from happening while the migration data is build up. If it would happen, it could be that we miss updates. 

The last part is connecting all the pieces. This is done by adding an additional MigrationAwareService interface on the CounterService which will signal Hazelcast that our service is able to participate in partition migration:
\begin{lstlisting}[language=java]
public class CounterService 
        implements ManagedService, RemoteService, 
                   MigrationAwareService {
   private NodeEngine nodeEngine;
   Container[] containers;
   public void beforeMigration(MigrationServiceEvent e) {
   } 
   public Operation prepareMigrationOperation(MigrationServiceEvent e) {
      if (e.getReplicaIndex() > 1)  return null;

      Container container = containers[e.getPartitionId()];
      Map<String, Integer> migrationData = container.toMigrationData();
      if (migrationData.isEmpty()) return null;
      return new CounterMigrationOperation(migrationData);
   }
   public void commitMigration(MigrationServiceEvent e) {
      if (e.getMigrationEndpoint() == MigrationEndpoint.SOURCE && 
          e.getMigrationType() == MigrationType.MOVE) {
         Container c = containers[e.getPartitionId()];
         c.clear();
      }
   }
   public void rollbackMigration(MigrationServiceEvent e) {
      if (e.getMigrationEndpoint() == MigrationEndpoint.DESTINATION){
         Container c = containers[e.getPartitionId()];
         c.clear();
      }
   }
   public int getMaxBackupCount() {
      return 1;
   }
   ...
}
\end{lstlisting}
By implementing the MigrationAwareService some additional methods are exposed:
\begin{enumerate}
\item beforeMigration: this is called before we any migration is done. It is useful if you need to .. In our case we do nothing.
\item prepareMigrationOperation: method will return all the data of the partition that is going to be moved.
[mehmet: migrate only master and first backup data, since CounterService supports only 1 backup.]
\item commitMigration: commits the data. In this case committing means that we are going to clear the container for the partition of the old owner. Even though we don't have any complex resources like threads, database connections etc, clearing the container is advisable to prevent memory issues.is this method called on both the primary and backup? [mehmet: yes] if this node is source side of migration (means partition is migrating FROM this node) and migration type is MOVE (means partition is migrated completely not copied to a backup node) then remove partition data from this node. If this node is destination or migration type is copy then nothing to do.is this method called on both the primary and backup? [mehmet: yes][mehmet: if this node is destination side of migration (means partition is migrating TO this node) then remove partition data from this node.If this node is source then nothing to do.
\item rollbackMigration: 
\end{enumerate}

todo: tell about max backup count. You could use the service properties to configure this.

\section{Backups}
In this last phase we are going to deal with backups; so make sure that when a member fails that the data is available on another node. This is done by for every operation that makes a change, to have a backup operation executed as well. Since the IncOperation always makes a change, it will always have a backup operation. To signal Hazelcast that an operation needs a backup, the operation needs to implement the BackupAwareOperation:
\begin{lstlisting}[language=java]
class IncOperation extends AbstractOperation 
      implements KeyBasedOperation, 
                 BackupAwareOperation {
   public int getAsyncBackupCount() {
      return 0;
   }
   public int getSyncBackupCount() {
      return 1;
   }
   public boolean shouldBackup() {
      return true;
   }
   public Operation getBackupOperation() {
      return new IncBackupOperation(objectId, amount);
   }
   ...
}
\end{lstlisting}
As you can see some additional methods need to be implemented. The getAsyncBackupCount and getSyncBackupCount signals how many asynchronous and synchronous backups there need to be. In our case we only want a single synchronous backup and no asynchronous backups. The shouldBackup method tells Hazelcast that our Operation needs a backup. If course we could do some optimizations in case of amount = 0. The last method is the getBackupOperation which returns the backup operation of the IncOperation; in this case the IncBackupOperation:

\begin{lstlisting}[language=java]
public class IncBackupOperation 
      extends AbstractOperation 
      implements BackupOperation {
   private String objectId;
   private int amount;
   public IncBackupOperation() {
   }
   public IncBackupOperation(String objectId, int amount) {
      this.amount = amount;
      this.objectId = objectId;
   }
   protected void writeInternal(ObjectDataOutput out) 
         throws IOException {
      super.writeInternal(out);
      out.writeUTF(objectId);
      out.writeInt(amount);
   }
   protected void readInternal(ObjectDataInput in) 
         throws IOException {
      super.readInternal(in);
      objectId = in.readUTF();
      amount = in.readInt();
   }
   public void run() throws Exception {
      CounterService service = getService();
      System.out.println("Executing backup " + objectId + 
         ".inc() on: " + getNodeEngine().getThisAddress());
      Container c = service.containers[getPartitionId()];
      c.inc(objectId, amount);
   }
}
\end{lstlisting}
Hazelcast will also make sure that a new IncOperation for that particular key will not be executing before the (synchronous) backup operation has completed.

Of course we also want to see the backup functionality in action.
\begin{lstlisting}[language=java]
public class Member {
   public static void main(String[] args) throws Exception {
      HazelcastInstance[] instances = new HazelcastInstance[2];
      for (int k = 0; k < instances.length; k++) 
         instances[k] = Hazelcast.newHazelcastInstance();

      Counter counter = instances[0].getDistributedObject(
         "CounterService", "counter" + 0);
      counter.inc(1);
   }
}
\end{lstlisting}
When we run this Member we'll get the following output:
\begin{lstlisting}
Executing counter0.inc() on: Address[192.168.1.103]:5702
Executing backup counter0.inc() on: Address[192.168.1.103]:5701
Finished
\end{lstlisting}
As you can see, not only the IncOperation has executed, also the backup operation is executed. You can also see that the operations have been executed on different cluster members to guarantee high availability. One of the experiments you could do it to modify the test code so you have a cluster of members in different JVM's and see what happens with the counters when you kill one or more JVM's. 

\section{Wait notify}
In this section we are going to make of the wait/notify system. We are going to add an await method to the counter; in this await method a thread can wait until the value of a counter is equal or bigger than the supplied one.

\begin{lstlisting}[language=java]
public interface Counter extends DistributedObject {
   int inc(int amount);
   void await(int value)throws InterruptedException;
}
\end{lstlisting}


todo: timeout; interrupted exception

\section{What is next}
In this chapter we have created step by step a distributed data-structures on top of Hazelcast. And although Hazelcast provides a whole collection of very usable distributed data-structures, the addition of the SPI changes Hazlcast from being a simple datagrid to a datagrid infrastructure where your imagination is the only limit. In distributed applications there will always be the need to create special data-structures that are very useful for very particular usecases and with the SPI you can now build these data-structures yourself.
