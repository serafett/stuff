\chapter{SPI}

One of the most exiting new features of Hazelcast 3 is the new SPI module (see the 'com.hazelcast.spi' package). The cool thing about this package is that it makes it possible to write first class distributed services/data-structures yourself. They pulled out this API into user space, but it also the core of all the Hazelcast functionality like the Map or the distributed executor relies on the same functionality. So with the SPI you can write your own data-structures if you are unhappy with the ones provides by Hazelcast. You also could write more complex services like an Actor library; I have build a POC actor library on top of Hazelcast where the actors automatically scale and are highly available. The only limiting factor is your imagination.

In this chapter we are going to build a distributed counter, so a counter that stored somewhere in the datagrid. The full sources can be found ...

\begin{lstlisting}[language=java]
public interface DistributedCounter{
    int inc(int amount);
}
\end{lstlisting}

\section{Getting started}
In this section we are going to show you a very basic service that will be started when Hazelcast starts and will be shutdown when Hazelcast is shutdown. In itself not extremely interesting, but it is needed for the the more advanced sections.

\begin{lstlisting}[language=java]
public class DistributedCounterService implements ManagedService {
   private NodeEngine nodeEngine;
   public void init(NodeEngine e, Properties p) {
      System.out.println("DistributedCounterService.init");
      this.nodeEngine = e;
   }
   public void shutdown() {
      System.out.println("DistributedCounterService.shutdown");
   }
}
\end{lstlisting}

\begin{lstlisting}[language=xml]
<network>
   <join><multicast enabled="true"/> </join>
</network>
<services>
   <service enabled="true">
      <name>DistributedCounterService</name>
      <class-name>DistributedCounterService</class-name>
   </service>
</services>
\end{lstlisting}
If you need to set additional properties on the Service, a '<properties>' section can be added to the service. We also enabled multicast discovery since we'll rely on that later.

\begin{lstlisting}[language=java]
public class Member {
   public static void main(String[] args) {
      Hazelcast.newHazelcastInstance();
   }
}
\end{lstlisting}
If we start it we'll see:
\begin{lstlisting}
DistributedCounterService.init
\end{lstlisting}
So the DistributedCounterService is started as part of the startup of the HazelcastInstance, it is done eagerly.. not lazily.

\section{Proxy}
In the previous section we created a DistributedCounterService that gets started when Hazelcast starts, but apart from that it doesn't do anything yet. In this section we are going connect the DistributedCounter interface to the DistributedCounterService and we are going to do a remote call on member hosting the actual counter data/logic and we'll return the result.

The first thing we are going to do is to let the DistributedCounter implement the DistributedObject interface to indicate that it is a distributed object. Also some methods will be exposed like getName,getId and destroy:
\begin{lstlisting}[language=java]
public interface DistributedCounter extends DistributedObject {
    int inc(int amount);
}
\end{lstlisting}

The next step is fixing the DistributedCounterService. Apart from implementing the 'ManagedService' interface, it now also implements the 'RemoteService' interface. Through this interface a client will be able to get a handle of an DistributedCounter proxies. 
\begin{lstlisting}[language=java]
public class DistributedCounterService 
   implements ManagedService, RemoteService {
   ...  
   public DistributedObject createDistributedObject(Object objectId) {
      String id = String.valueOf(objectId)
      return new DistributedCounterProxy(id,nodeEngine);
   }
   public String getServiceName() {
      return "DistributedCounterService";
   }
   public DistributedObject createDistributedObjectForClient(Object objectId) {
      return null;
   }
   public void destroyDistributedObject(Object objectId) {
   }
}
\end{lstlisting}
[todo: destroyDistributedObject]
[todo: createDistributedObjectForClient]

As you can see the createDistributedObject returns a DistributedCounterProxy. This proxy is a local representation to (potentially) remote managed data/logic.  
\begin{lstlisting}[language=java]
public class DistributedCounterProxy 
      implements DistributedCounter {
   private final NodeEngine nodeEngine;
   private final String objectId;
   public DistributedCounterProxy(String objectId, NodeEngine e) {
      this.nodeEngine = e;
      this.objectId = objectId;
   }
   public Object getId() {return objectId;}
   public String getName() {return null;}
   public int inc(int amount) {
      IncOperation operation = new IncOperation(objectId, amount);   
      int partitionId = nodeEngine.getPartitionService()
         .getPartitionId(objectId);
      InvocationBuilder builder = nodeEngine.getOperationService()
         .createInvocationBuilder("DistributedCounterService", 
            operation, partitionId);
      try {
         final Future<Integer> future = builder.build().invoke();
         return future.get();
      } catch (ExecutionException e) {
         final Throwable cause = e.getCause();
         if (cause instanceof RuntimeException) {
            throw (RuntimeException) cause;
         } else if (cause instanceof Error) {
            throw (Error) cause;
         } else {
            throw new RuntimeException(cause);
         }
      } catch (InterruptedException e) {
         Thread.currentThread().interrupt();
         throw new RuntimeException(e);
      }
   }
}
\end{lstlisting}
The DistributedCounterProxy doesn't contain counter state; it is just a local representative of remote data/functionality. Therefor the inc method of DistributedCounterProxy needs to be invoked on the machine responsible for hosting the real counter. This can be done using the Hazelcast Operation, part of the new SPI. 

If you take a closer look at the inc method, the first thing we do is to create the IncOperation with the given objectId and the amount. The next step is to get the partitionId; this is done based on the objectId so that all operations for a given objectId will always result in the same partitionId. Then we create an InvocationBuilder based on the operation and the partitionId. This is where the connection is made between the operation and the partition. 

The last part of the logic is actually invoking the operation and waiting for its result. This is done using a Future which gives us the ability to synchronize on completion of that remote executed operation and to get the results. In our case we do a simple get since we don't care about a timeout, but for real system it often is better to use a timeout since most operations should complete in a certain amount of time. If they don't complete, it could be a sign of problems and waiting indefinitely could lead to stalling systems without any form of error logging.

Checked exceptions; could be put in the interface.

Interrupted exception; can be put in the interface to indicate blocking operation. 

It can happen that a 

There is one thing you need to watch out for in that case, the stacktrace of that exception will represent the callstack of the thread executing the remote operation, not the callstack of the thread executing the DistributedCounterProxy.inc method. An easy way to fix that is to prepend the client side stacktrace to the stacktrace (the stacktrace is just an array of StackTraceElements)

todo; can the operation be interrupted if the future.cancel is called?
todo; will hazelcast take care of resending an operation to the correct machine when the partitionlock is exclusive?


There are 4 types of operations:
\begin{enumerate}
\item Basic operations are executed always immediately, they don't require any lock and they don't wait any other operation. These operations hi
\item PartitionAwareOperation: will be executed on a specific partition by returning the partition id. Once they are executed, the partition read-lock will be acquired and will prevent PartitionLevelOperations from executing on that partition. If a PartitionLevelOperation acquired the writelock before the PartitionAwareOperation acquired the readlock, a retry exception is returned to the caller so the operation can be redirected to the new partition owner. [todo: on the caller side we need to deal with a retry exception, or is this already dealt with]
\item KeyBasedOperations extends PartitionAwareOperation and can specify key locks that prevent other KeyBasedOperations for the same key to execute on that operations [todo: is this a readlock or writelock]
\item PartitionLevelOperation are the counter part of the PartitionAwareOperation since they acquire the writelock on the Partition so that no other PartitionAware and PartitionLevelOperations can be executed on that partition for the duration of that writelock. If the read or write-lock on the partition already is acquired, the call blocks until lock is released. [todo: timeout?]
\end{enumerate}
In our example the IncOperation is going to make use of the KeyBasedOperation to prevent dealing with concurrency control; Hazelcast will lock the correct partition and key:
\begin{lstlisting}[language=java]
class IncOperation 
      extends PartitionAwareOperation, 
              KeyBasedOperation{
   private String objectId;
   private int amount,returnValue;
   public IncOperation(){}
   public IncOperation(String objectId, int amount) {
      this.amount = amount;
      this.objectId = objectId;}
   public boolean returnsResponse() {return true;}
   public Object getResponse() {return returnValue;}}
   public void destroy() {}
   protected void writeInternal(ObjectDataOutput out) 
         throws IOException {
      super.writeInternal(out);
      out.writeUTF(objectId);
      out.writeInt(amount);
   }
   protected void readInternal(ObjectDataInput in) 
        throws IOException {
      super.readInternal(in);
      objectId = in.readUTF();
      amount = in.readInt();
   }
   public void run() throws Exception {
      System.out.println("Executing "+objectId+".inc() on: "+
         getNodeEngine().getThisAddress());            
      returnValue = 0;
   }
   public int getKeyHash() {
      return ("DistributedCounterService" + objectId).hashCode();
   }
}	
\end{lstlisting}
Hazelcast will determine the key to lock with the getKeyHash method. Internally Hazelcast uses a similar approach as being used in the ConcurrentHashMap where a stripe of locks is used and the keyhash is used as a basis for the index. So the same keyhash will make sure the same key is locked. Of course it can happen that you get unexpected collisions due to a bad keyhashing method or just by the nature of this approach. 
[todo: can the size of the lockstripe be influenced]
[todo: what about timeouts?]

Another thing worth mentioning is that because the IncOperation needs to be serialized, the writeInternal and readInternal methods needs to be overwritten so that the objectId and amount are serialized and will be available when this operation runs. For deserialization it also is needed that the operation always needs to have a no arg constructor.

Of course we also want to run the code; the following example shows how it is run:
\begin{lstlisting}[language=java]
public class Member {
   public static void main(String[] args) {
      HazelcastInstance[] instances = new HazelcastInstance[2];
      for (int k = 0; k < instances.length; k++) 
         instances[k] = Hazelcast.newHazelcastInstance();
      DistributedCounter[] counters = new DistributedCounter[4];
      for (int k = 0; k < counters.length; k++) {
         DistributedCounter counter = (DistributedCounter) instances[0]
            .getDistributedObject("DistributedCounterService", "counter" + k);
         counters[k] = counter;
         System.out.println(counter.inc(1));
      }
      System.out.println("Finished");
   }
}
\end{lstlisting}

The output will show something like this:
\begin{lstlisting}
Executing counter0.inc() on: Address[192.168.1.101]:5702
0
Executing counter1.inc() on: Address[192.168.1.101]:5701
0
Executing counter2.inc() on: Address[192.168.1.101]:5701
0
Executing counter3.inc() on: Address[192.168.1.101]:5701
0
Finished
\end{lstlisting}
We can see that our counters are being stored in different members (check the different port numbers). We can also see that the increment doesn't do any real logic yet since the value remains 0.

\section{Container}
In this section we are going to make use of real DistributedCounter; so some kind of data-structure that will hold an integer value and can be incremented. The first thing we do is for every partition there is in the system, we are going to create a Container which will contain all counters for a given partition:
\begin{lstlisting}[language=java]
class Container {
   final ConcurrentMap<String, Integer> counterMap 
      = new ConcurrentHashMap<>();
   
   int inc(String id, int amount) {
      Integer counter = counterMap.get(id);
      if (counter == null) 
         counter = 0;
      counter += amount;
      counterMap.put(id, counter);
      return counter;
   }
}
\end{lstlisting}
The Container has an inc method, that looks up the counter based on the id and increments it. 

For this chapter I rely on Container class per partition, but you have complete freedom how to go about it. A different approach used in the Hazelcast code is that the Container is dropped and the DistributedCounterService you just have a map of counters:
\begin{lstlisting}[language=java]
final ConcurrentMap<String, AtomicLong> counters = 
   new ConcurrentHashMap<String, Integer>();
\end{lstlisting}
The id of the counter can be used as key and an Integer as value. The only thing you need to take care of is that if operations for a specific partition are executed, you only select the values for that specific partition. This can be as simple as: 
\begin{lstlisting}[language=java]
for(Map.Entry<String,Integer> entry: counters.entrySet()){
   String id = entry.getKey();
   int partitinId = nodeEngine.getPartitionService().getPartitionId(id); 
   if(partitionid == requiredPartitionId){
      ...do operation	
   }
}
\end{lstlisting}
Its a personal taste which solution is preferred. Personally I like the container approach since there will not be any mutable shared state between partitions. It also makes operations on partitions simpler since you don't need to filter out data that doesn't belong to a certain partition.

The next step is to create a containers. The number of partitions is static, so will not change so we are going to create a container per partition and store it in an array. The array will have the same length as the number of partitions. In practice each member will only host a subset of the partitions so since we create a container per partition up front, it could be that a container isn't used. But since its size is small, it isn't very important and also makes the examples a bit simpler.
\begin{lstlisting}[language=java]
public class DistributedCounterService 
       implements ManagedService, 
                  RemoteService {
   private NodeEngine nodeEngine;
   DistributedMapContainer[] containers;
   public void init(NodeEngine e, Properties p) {
      this.nodeEngine = e;
      this count = e.getPartitionService().getPartitionCount();
      containers = new Container[count];
      for(int k=0;k<containers.length;k++) 
         containers[k]=new Container();
   }
}
\end{lstlisting}
As you can see the partitions are created in the init method. The last step is connecting the IncOperation.run method to the Container.inc method:
\begin{lstlisting}[language=java]
class IncOperation extends AbstractOperation 
                   implements KeyBasedOperation{  
   public void run() throws Exception {
      DistributedCounterService s = getService();
      Container c = s.containers[getPartitionId()];
      returnValue = c.inc(objectId, amount);
   }  
}
\end{lstlisting}
The container can easily be retrieved using the partitionId since the range or partitionId's is 0 to partitionCount (exclusive) so it can be used as an index on the container array. Once the container has been retrieved the inc operation can be called on it and the result can be stored in the 'returnValue' field of the IncOperation. This value will automatically be returned to the client.

When we run the code, we'll see:
\begin{lstlisting}
1
2
\end{lstlisting}
This means that we now have a basic distributed counter up and running!

\section{Partition migration}
In our previous phase we managed to create real distributed counters. The problem is that if a node goes down, there will not be any backups because the values are not replicated to another machine. This means that the DistributedCounters are not highly available. In this phase we are going to add replication.

The first thing we are going to do is to add 3 operations to the Container:
\begin{lstlisting}[language=java]
class Container {
   final ConcurrentMap<String, Integer> counterMap 
     = new ConcurrentHashMap<>();
   void clear() {
      counterMap.clear();
   }
   void applyMigrationData(Map<String, Integer> d) {
      counterMap.putAll(d);
   }
   Map<String, Integer> toMigrationData() {
      return new HashMap<>(counterMap);
   }
}
\end{lstlisting}
\begin{enumerate}
\item toMigrationData: this method is going to be called when Hazelcast wants to get started with the migration of the partition of the old partition owner. The result of the toMigrationData is data of the partition in a form so that it can be serialized to another member.
\item applyMigrationData: this method is called when the migrationData that is created by the toMigrationData method is going to be applied to member that is going to be the new partition owner.
\item clear: is going to be called when old partition owner can now get rid of all the data for that partition. 
\end{enumerate}

The next step is to create a MigrationOperation that will be responsible for carrying the migrationData from one machine to anther and to call the 'applyMigrationData' on the correct partition of the new partition owner.
\begin{lstlisting}[language=java]
public class MigrationOperation extends AbstractOperation {
   Map<String, Integer> migrationData;
   public MigrationOperation() {
   }
   public MigrationOperation(Map<String, Integer> migrationData) {
       this.migrationData = migrationData;
   }
   protected void writeInternal(ObjectDataOutput out) 
         throws IOException {
      out.writeInt(migrationData.size());
      for (Map.Entry<String, Integer> entry : migrationData.entrySet()) {
         out.writeUTF(entry.getKey());
         out.writeInt(entry.getValue());
      }
   }
   protected void readInternal(ObjectDataInput in) 
         throws IOException {
      int size = in.readInt();
      migrationData = new HashMap<>();
      for (int i = 0; i < size; i++)
         migrationData.put(in.readUTF(), in.readInt());
   }
   public void run() throws Exception {
      DistributedCounterService service = getService();
      Container container = service.containers[getPartitionId()];
      container.applyMigrationData(migrationData);
   }
}
\end{lstlisting}

The Hazelcast infrastructure will prevent that any PartitionAware or PartitionLevelOperation are going to be executed during the migration of this particular partition. So we don't need to be worried that for example the 'old' counters are going to be incremented and their changes will be lost when the new partition takes over.

And the last part is connecting all the pieces. This is done by adding an additional MigrationAwareService interface on the DistributedCounterService which will signal Hazelcast that our service able to participate in partition migration:
\begin{lstlisting}[language=java]
public class DistributedCounterService 
        implements ManagedService, RemoteService, 
                   MigrationAwareService {
   private NodeEngine nodeEngine;
   Container[] containers;
   public void beforeMigration(MigrationServiceEvent e) {
   } 
   public Operation prepareMigrationOperation(MigrationServiceEvent e) {
      if (e.getReplicaIndex() > 1)  return null;

      Container container = containers[e.getPartitionId()];
      Map<String, Integer> migrationData = container.toMigrationData();
      if (migrationData.isEmpty()) return null;
      return new MigrationOperation(migrationData);
   }
   public void commitMigration(MigrationServiceEvent e) {
      if (e.getMigrationEndpoint() == MigrationEndpoint.SOURCE && 
          e.getMigrationType() == MigrationType.MOVE) {
         Container c = containers[e.getPartitionId()];
         c.clear();
      }
   }
   public void rollbackMigration(MigrationServiceEvent e) {
      if (e.getMigrationEndpoint() == MigrationEndpoint.DESTINATION){
         Container c = containers[e.getPartitionId()];
         c.clear();
      }
   }
   public int getMaxBackupCount() {
      return 1;
   }
}
\end{lstlisting}
By implementing the MigrationAwareService some additional methods are exposed:
\begin{enumerate}
\item beforeMigration: this is called before we any migration is done. It is useful if you need to .. In our case we do nothing.
\item prepareMigrationOperation: method will return all the data of the partition that is going to be moved.
[mehmet: migrate only master and first backup data, since DistributedCounterService supports only 1 backup.]
\item commitMigration: commits the data. In this case committing means that we are going to clear the container for the partition of the old owner. Even though we don't have any complex resources like threads, database connections etc, clearing the container is advisable to prevent memory issues.is this method called on both the primary and backup? [mehmet: yes] if this node is source side of migration (means partition is migrating FROM this node) and migration type is MOVE (means partition is migrated completely not copied to a backup node) then remove partition data from this node. If this node is destination or migration type is copy then nothing to do.is this method called on both the primary and backup? [mehmet: yes][mehmet: if this node is destination side of migration (means partition is migrating TO this node) then remove partition data from this node.If this node is source then nothing to do.
\item rollbackMigration: 
\end{enumerate}

todo: tell about max backup count. You could use the service properties to configure this.

\section{Backups}
In this last phase we are going to deal with backups; so make sure that when a member fails that the data is available on another node. This is done by for every operation that makes a chance, to have a backup operation executed as well. Since the IncOperation always makes a change, it will always have a backup operation. To signal Hazelcast that an operation needs a backup, the operation needs to implement the BackupAwareOperation:
\begin{lstlisting}[language=java]
class IncOperation extends AbstractOperation 
      implements KeyBasedOperation, 
                 BackupAwareOperation {
   public int getAsyncBackupCount() {
      return 0;
   }
   public int getSyncBackupCount() {
      return 1;
   }
   public boolean shouldBackup() {
      return true;
   }
   public Operation getBackupOperation() {
      return new IncBackupOperation(objectId, amount);
   }
}
\end{lstlisting}
As you can see some additional methods need to be implemented. The getAsyncBackupCount and getSyncBackupCount signals how many asynchronous and synchronous backups there need to be. In our case we only want a single synchronous backup and no asynchronous backups. The shouldBackup method tells Hazelcast that our Operation needs a backup. If course we could do some optimizations in case of amount = 0. The last method is the getBackupOperation which returns the backup operation of the IncOperation; in this case the IncBackupOperation:

\begin{lstlisting}[language=java]
public class IncBackupOperation 
      extends AbstractOperation 
      implements BackupOperation {
   private String objectId;
   private int amount;
   public IncBackupOperation() {
   }
   public IncBackupOperation(String objectId, int amount) {
      this.amount = amount;
      this.objectId = objectId;
   }
   protected void writeInternal(ObjectDataOutput out) 
         throws IOException {
      super.writeInternal(out);
      out.writeUTF(objectId);
      out.writeInt(amount);
   }
   protected void readInternal(ObjectDataInput in) 
         throws IOException {
      super.readInternal(in);
      objectId = in.readUTF();
      amount = in.readInt();
   }
   public void run() throws Exception {
      DistributedCounterService service = getService();
      System.out.println("Executing backup " + objectId + 
         ".inc() on: " + getNodeEngine().getThisAddress());
      Container c = service.containers[getPartitionId()];
      c.inc(objectId, amount);
   }
}
\end{lstlisting}
Hazelcast will also make sure that a new IncOperation for that particular key will not be executing before the (synchronous) backup operation has completed.

\section{What is next}
In this chapter we have created step by step a distributed data-structures on top of Hazelcast. And although Hazelcast provides a whole collection of very usable distributed data-structures, the addition of the SPI changes Hazlcast from being a simple datagrid to a datagrid infrastructure where your imagination is the only limit. In distributed applications there will always be the need to create special data-structures that are very useful for very particular usecases and with the SPI you can now build these data-structures yourself.