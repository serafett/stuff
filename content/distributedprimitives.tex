\chapter{Distributed Primitives}

If you have programmed in Java you have probably worked with concurrency primitives like the synchronized statement (the intrinsic lock) or perhaps even the new concurrency library that was introduced in Java 5 under java.util.concurrent like the Executor, Lock and AtomicReference. And with every new release of Java all kinds of new functionality is added like the Fork/Join framework in Java 7.

This concurrency functionality is useful if you want to write a Java application that use multiple threads. The focus of this functionality however is to provide concurrency abstractions to be used in a single JVM, although you can always connect multiple JVM's together either using raw sockets or a more higher level remoting library (RMI, Servlets etc)

Hazelcast provides direct support for synchronization primitives that are distributed. The basic distributed features that are provided are:
\begin{enumerate}
\item failover; so if one machine fails, the system will still be working
\item scalability; just add more machines and you will get more capacity. Although this doesn't mean that your system will automatically become better scalable when you introduce Hazelcast, 
\end{enumerate}

In this chapter the following distributed primitives will be explained:
\begin{enumerate}
\item AtomicNumber
\item Lock
\item ISemaphore
\item ICountdownLatch
\item IdGenerator
\end{enumerate}

\section{AtomicNumber}

The AtomicNumber is the distributed version of the java.util.concurrent.atomic.AtomicLong, so if you have used it before, working with the AtomicNumber should feel very similar. The main differences are the way it is created and that the interface is a bit different.

The AtomicNumber exposes most of the operations the AtomicLong provides like:
\begin{enumerate}
\item get
\item set
\item getAndSet
\item compareAndSet
\item incrementAndGet
\end{enumerate}

We'll demonstrate the AtomicNumber by creating an instance and incrementing it one million times:
\begin{lstlisting}[language=java]
public class AtomicNumberMain{
    public static void main(String[] args){
        AtomicNumber counter = Hazelcast.getAtomicNumber("counter");
        int count = 1000 * 1000;
        for(int k=0;k<count;k++){
            counter.incrementAndGet();
        }
        System.out.printf("Count is %s\n",counter.get());
    }
}
\end{lstlisting}
If you start the AtomicNumberMain, you will eventually see:
\begin{verbatim}
Count is 1000000
\end{verbatim}
If you run this application in parallel, then the total count should be equal to one million times the number of applications you have started.

The AtomicNumbers is very useful in lock free algorithms, although you need to take care that uncontrolled repeating can lead to live-locking; the system is appears to do something since no progress is made, but in reality it is only burning cpu cycles and in case of the AtomicNumber, it will also be consuming network resources. In most cases it is best to limit the number of retries and throw an exception when the number of retries is exceeded.

If the AtomicNumber is contention point in your system, there are a few ways of dealing with it depending on your requirements. One of the options it to create a stripe (essentially an array) of AtomicNumbers to reduce pressure; but this approach can't always be applied. Another options is to keep changes locally and only publish them to the AtomicNumber once and a while. There are a few downsides here; you could loose information if a node goes down and is that the newest value is not always immediately visible to the outside world. 

Hazelcast only provides support for the long, but you can always simulate for other types:
\begin{enumerate}
\item boolean: 0 for true and 1 for false.
\item double: a 64 bit double can be encoded into 64 bits, which can be stored in a long 
      which is also 64 bits.
\end{enumerate}

\section{Distributed Lock}
A lock (also called a mutex) is a synchronization primitive that makes it possible that only a single thread is able to access to a critical section of code; if multiple threads at the same moment would access that critical section concurrently, you would get race problems. 

Hazelcast provides a distributed lock implementation and makes it possible to create a critical section within a cluster of JVM's; so only a single thread from one of the JVM's in the cluster is allowed to acquire that lock. Other threads that want to acquire the lock, no matter if they are on the same JVM's or not, will not be able to acquire it and depending on the locking method they called, they either block or fail. The com.hazelcast.core.ILock; extends the java.util.concurrent.locks.Lock interface, so using the lock is quite simple.

To show you how a race problem can be solved with a Hazelcast lock, I have created a small example containing a race problem.
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class RacyNode {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hazelcastInstance = Hazelcast.getDefaultInstance();
        AtomicNumber number1 = hazelcastInstance.getAtomicNumber("number1");
        AtomicNumber number2 = hazelcastInstance.getAtomicNumber("number2");
        System.out.println("Started");
        for (int k = 0; k < 1000000; k++) {
            if (k % 10000 == 0) System.out.println("at: " + k);
            if (k % 2 == 0) {
                long n1 = number1.get();
                Thread.sleep(100);
                long n2 = number2.get();
                if (n1 - n2 != 0) System.out.println("Datarace detected!");
            } else {
                number1.incrementAndGet();
                number2.incrementAndGet();
            }
        }
        System.out.println("Finished");
    }
}
\end{lstlisting}
The idea is that a thread is going to increment 2 numbers half of the time and in the other half it will compare the 2 numbers and expects them to be equal. If they are not equal, we have a race problem and "Difference detected!" will be printed.

If we run only a single instance of the RacyNode, we won't get any race problem since either the 2 numbers will be incremented, or the 2 numbers will be printed. There will no be an interleaving of these operations.

But if we start multiple instances of the RaceNode, we'll start to see the following:
\begin{verbatim}
Datarace detected!
Datarace detected!
...
\end{verbatim}	
The reason this race problem is occurring is that the writing and the reading of the 2 numbers is not placed in a critical section. With the Hazelcast distributed lock we can easily solve this problem:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class RaceFreeNode {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hazelcastInstance = Hazelcast.getDefaultInstance();
        AtomicNumber number1 = hazelcastInstance.getAtomicNumber("number1");
        AtomicNumber number2 = hazelcastInstance.getAtomicNumber("number2");
        ILock lock = hazelcastInstance.getLock("lock");
        System.out.println("Started");
        for (int k = 0; k < 10000; k++) {
            if (k % 100 == 0) System.out.println("at: " + k);
            lock.lock();
            try {
                if (k % 2 == 0) {
                    long n1 = number1.get();
                    Thread.sleep(10);
                    long n2 = number2.get();
                    if (n1 - n2 != 0) System.out.println("Datarace detected!");
                } else {
                    number1.incrementAndGet();
                    number2.incrementAndGet();
                }
            } finally {
                lock.unlock();
            }
        }
        System.out.println("Finished");
    }
}
\end{lstlisting}
When this code is executed; you will not see "Datarace detected!". This is because the lock provides a critical section around writing and reading of the numbers. 

A few things worth knowing about the Hazelcast lock:
\begin{enumerate}
\item No Condition support: With the java.util.concurrent.locks.Lock it also is possible to create a java.util.concurrent.locks.Condition object to wait for a certain condition to happen; e.g. a custom made queue just received an element. Unfortunately the Condition functionality is not supported by Hazelcast at the moment.
\item Hazelcast lock is reentrant, so you can acquire it multiple times in a single thread without causing a deadlock, of course you need to release it as many times as you have acquired it, to make it available to other threads.
\item The lock should always be acquired outside of a try/finally block. Else it can happen that the lock acquire failed, but an unlock is still executed.
\item Fairness: TODO
\item Keep locks as short as possible. If locks are kept too long, it can lead to performance problems or worse: deadlock.
\item With locks it is easy to run into deadlocks if you don't know what you are doing; so make sure that you do. Having code you don't control running inside your locks is asking for problems. Make sure you understand exactly the scope of the lock.
\item Locks are automatically released when a node has acquired a lock and this node goes down. This prevents threads that are waiting for a lock to wait indefinitely and this is important for failover to work in a distributed system. The downside however is that if a node goes down that acquired the lock and started making changes, that other nodes could start to see partial changes. In these cases either the system could do some self repair or else a transaction potentially can solve the problem.
\item a lock must always be released by the same lock that acquired it, otherwise look at the ISemaphore.
\item there are no configuration options available for the lock
\end{enumerate}

\section{ISemaphore}

A semaphore is a synchronization aid that can be used if you want to control how many threads concurrently execute some activity, e.g. using a resource. Each semaphore has a number of permits which should be equal to the maximum number of concurrent threads. 

Hazelcast provides a distributed version of the java.util.concurrent.Semaphore named the com.hazelcast.core.ISemaphore. When a permit needs to be acquired the following can happen:
\begin{enumerate}
\item a permit is available; the number of permits in the semaphore is decreased by one and the calling thread can continue. After that thread is finished with the resource, it should release the permit.
\item no permit is available; the calling thread will block until a permit comes available, a timeout happens, the thread is interrupted or when the semaphore is destroyed an InstanceDestroyedException will be thrown.
\end{enumerate}

The big difference between a semaphore and a lock is that the former has no concept of ownership. So with a lock the thread that acquired the lock must release it, and with a semaphore any thread can release it. 

I explain the semaphore with an example. To simulate a resource we have an AtomicNumber initialized with the value 0. We are going to use this resources 1000 times. Every time the thread starts to use that resource it increments it and once completed it decrements the resource.
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class SemaphoreNode {
    public static void main(String[] args)throws Exception{
        HazelcastInstance hazelcastInstance = Hazelcast.getDefaultInstance();
        ISemaphore semaphore = hazelcastInstance.getSemaphore("semaphore");
        AtomicNumber resource = hazelcastInstance.getAtomicNumber("resource");
        for(int k=0;k<1000;k++){
            System.out.println("At iteration: "+k);
            System.out.println("Active Threads: " + resource.get());
            semaphore.acquire();
            try{
                resource.incrementAndGet();
                Thread.sleep(1000);
                resource.decrementAndGet();
            }finally{
                semaphore.release();
            }
        }
        System.out.println("Finished");
    }
}
\end{lstlisting}

We want to limit the concurrent access to the resource by allowing for at most 3 thread. We do this by adding the following configuration:

\begin{lstlisting}[language=xml]
<hazelcast xsi:schemaLocation="http://www.hazelcast.com/schema/config hazelcast-config-2.3.xsd"
           xmlns="http://www.hazelcast.com/schema/config"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
    <network>
        <join>
            <multicast enabled="true"/>
        </join>
    </network>
    <semaphore name="semaphore">
        <initial-permits>3</initial-permits>
    </semaphore>
</hazelcast>
\end{lstlisting}

Now start the SemaphoreNode 5 times and you will see output like this:
\begin{verbatim}
At iteration: 0
Active Threads: 1
At iteration: 1
Active Threads: 2
At iteration: 2
Active Threads: 3
At iteration: 3
Active Threads: 3
At iteration: 4
Active Threads: 3
\end{verbatim}
As you can see the maximum number of concurrent users of that resource, always are equal or smaller than 3. As an experiment you can remove the semaphore acquire/release statements and see for yourself that there is no longer control on the number of concurrent usages of the resources.

TODO: text is copy paste
\emph{fairness:} The Hazelcast distributed semaphore implementation guarantees that threads invoking any of the acquire methods are selected to obtain permits in the order in which their invocation of those methods was processed(first-in-first-out; FIFO). Note that FIFO ordering necessarily applies to specific internal points of execution within the cluster. So, it is possible for one member to invoke acquire before another, but reach the ordering point after the other, and similarly upon return from the method.

TODO: text is copy paste
\emph{Attach:} The Hazelcast semaphore also allows you to attach()/detach() permits to the caller address. This provides a safety mechanism in case that address becomes disconnected from the cluster. Attached permits will automatically be released to the semaphore if a disconnection occurs. An address can also have an excess number of detached permits and is represented by a negative attached permit count. This is the number of permits that the semaphore will automatically be reduced by if a disconnection occurs.

TODO: Semaphore factory.

\section{ICountDownLatch}
The CountDownLatch was introduced in Java 1.5 and is a synchronization aid that makes it possible for threads to wait until a set of operations being performed in another thread  complete. Very simplistically; a CountDownLatch could be seen as a gate containing a counter; behind this gate threads can wait till the counter reaches zero. In my experience CountDownLatches often are used when you have some kind of processing operation, and you one or more threads that wait till this operation is completed so they can do their logic.

In Hazelcast there also is a CountDownLatch; the org.hazelcast.core.ICountDownLatch. The following example is made up by 2 parts; 1 part is the 'Leader'. It will create a latch and do a 'countdown' to release it:

And we start a single leader, 
\begin{lstlisting}[language=java]
public class Leader{
    public static void main(String[] args)throws Exception{
        HazelcastInstance hazelcastInstance = Hazelcast.getDefaultInstance();
        ICountDownLatch latch = hazelcastInstance.getCountDownLatch("countDownLatch");
        
        System.out.println("Starting");
        //we init the latch with 1, since we only need to complete a single step.
        latch.setCount(1); 
        //do some sleeping to simulate doing something    
        Thread.sleep(5000);
        //now we do a countdown which opens the latch and all waiting
        //followers are notified.
        latch.countDown();
        System.out.println("Leader finished");
        //we need to clean up the latch
        latch.destroy();
    }
}
\end{lstlisting}

TODO: What happens when the latch is destroyed before all the followers are notitifed?

And we also need followers; so Java applications that are going to wait for the latch to notify them.

\begin{lstlisting}[language=java]
public class Follower{
    public static void main(String[] args)throws Exception{
        ICountDownLatch latch = Hazelcast.getCountDownLatch("countDownLatch");
        System.out.println("Waiting");
        latch.await();
        System.out.println("Complete!");
    }
}
\end{lstlisting}
We can spawn as many of these waiters as you want, and all will be displaying.

\begin{verbatim}
Waiting
\end{verbatim}

Once the leader counts down to zero, all followers will be notified and show the following output:

\begin{verbatim}
Waiting
Complete!
\end{verbatim}

This example show a CountdownSemaphore with only a single 'step'. But if the process has n steps, the countdownlatch can be initialized with n. In the following example a process that is made up of 2 steps is shown.

\begin{lstlisting}[language=java]
public class MultiStepLeader{
    public static void main(String[] args)throws Exception{
        ICountDownLatch latch = Hazelcast.getCountDownLatch("countDownLatch");
        latch.setCount(2);

        Thread.sleep(60000); 
        latch.countDown();
        System.out.println("Leader completed first step");
        
        Thread.sleep(60000); 
        latch.countDown();
        System.out.println("Leader completed both steps");
        latch.destroy();
    }
}
\end{lstlisting}

Although the ICountdownLatch is a very useful synchronization aid, it probably isn't one you will use on a daily basis.

In practice you will probably not have all the countdown latches needed created up front. A way to deal with this is to let the 'Leader' create a ICountDownLatch when it starts with its steps, and send the id of this ICountdownLatch to all the followers. The followers can then retrieve the CountdownLatch by calling Hazelcast.getCountdownLatch(..).

Important: unlike Java's implementation, Hazelcast's ICountDownLatch count can be re-set after a countdown has finished but not during an active count. This allows the same proxy instance to be reused.

-- TODO: Text is copied.
The Hazelcast member that successfully invokes setCount(int) becomes the owner of the countdown and is responsible for staying connected to the cluster until the count reaches zero. If the owner  becomes disconnected prior to count reaching zero all awaiting threads will be notified. This provides a safety mechanism in the distributed environment.

\section{IdGenerator}

In the previous section we introduced the AtomicNumber and one of the things it can be used for is to generated unique id's within a cluster. Although it will work, it probably isn't the most scalable solution since all member will content on incrementing the value. If you are only interested in id's and not in the order the id's are generated and don't care when there is a risk of skipping id's, you can have a look at the IdGenerator.

Underneath is a small example of the id generator:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class IdGeneratorNode {
    public static void main(String[] args) {
        HazelcastInstance hazelcast = Hazelcast.getDefaultInstance();
        IdGenerator idGenerator = hazelcast.getIdGenerator("idGenerator");
        for (int k = 0; k < 1000000; k++) {
            long id = idGenerator.newId();
            System.out.printf("Id : %s\n", id);
        }
    }
}
\end{lstlisting}

If you start this multiple times, you will see in the console that there will not be any duplicate id's

The way the org.hazelcast.core.IdGenerator works is that each node claims a segment of id's to generate, e.g. 0..999999. This is done behind the scenes by incrementing a shared AtomicNumber. After it has claimed its segment, it can increment a local counter (so there is no network traffic). Only when the id's in its segments are used, then a new segment needs to be claimed and this is done by increasing the shared AtomicNumber.

This means that node's can generate id's much quicker (local increment vs a distributed one). And it will also scale a lot better because of reduced contention; instead of incrementing that AtomicNumber on every new id, only when all the id's in the segment are used, the AtomicNumber will be incremented.

Of course there are some downsides you need to be aware of:
\begin{enumerate}
\item the generated id's probably will be out of order
\item if a node goes down without using its range, there might be gaps.
\end{enumerate}
But for id generation, these limitations in most cases are not an issue. Apart from the IdGenerator, there are other options for creating cluster wide unique id's. One of them is the java.util.UUID, although it will take up more space than a long. 

\emph{If the cluster restarts} then id generation will start from 0.

\section{What is next?}
In this chapter we went over the various synchronization primitives that are directly exposes by Hazelcast. You can always build higher synchronization primitives on top of these lower ones.

TODO: Deadlocks, Livelocks, starvation etc. If you know how a hammer and a saw work, it doesn't mean that you immediately can build a house with it.
