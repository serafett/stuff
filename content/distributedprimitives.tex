\chapter{Distributed Primitives}

If you have programmed in Java you have probably worked with concurrency primitives like the synchronized statement (the intrinsic lock) or perhaps even the new concurrency library that was introduced in Java 5 under java.util.concurrent like the Executor, Lock and AtomicReference. And with every new release of Java all kinds of new functionality is added like the Fork/Join framework in Java 7.

This concurrency functionality is useful if you want to write a Java application that use multiple threads. The focus of this functionality however is to provide concurrency abstractions to be used in a single JVM, although you can always connect multiple JVM's together either using raw sockets or a more higher level remoting library (RMI, Servlets etc)

Hazelcast provides direct support for synchronization primitives that are distributed. The basic distributed features that are provided are:
\begin{enumerate}
\item failover; so if one machine fails, the system will still be working
\item scalability; just add more machines and you will get more capacity. Although this doesn't mean that your system will automatically become better scalable when you introduce Hazelcast, 
\end{enumerate}
In this chapter the following distributed primitives will be explained:
\begin{enumerate}
\item AtomicNumber
\item Lock
\item ISemaphore
\item ICountdownLatch
\item IdGenerator
\end{enumerate}
\section{AtomicNumber}
The AtomicNumber is the distributed version of the java.util.concurrent.atomic.AtomicLong, so if you have used it before, working with the AtomicNumber should feel very similar. The main difference is the way it is created.

The AtomicNumber exposes most of the operations the AtomicLong provides like get, set, getAndSet, compareAndSet and incrementAndGet, there is of course a big difference in performance since remote calls are involved.

We'll demonstrate the AtomicNumber by creating an instance and incrementing it one million times:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class AtomicNumberMember {
    public static void main(String[] args) {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        AtomicNumber counter = hzInstance.getAtomicNumber("counter");
        for (int k = 0; k < 1000 * 1000; k++) {
            if (k % 200000 == 0) System.out.println("At: "+k);
            counter.incrementAndGet();
        }
        System.out.printf("Count is %s\n", counter.get());
    }
}
\end{lstlisting}
If you start the AtomicNumberMember, you will see this:
\begin{lstlisting}
At: 0
At: 200000
At: 400000
At: 600000
At: 800000
Count is 1000000
\end{lstlisting}
If you run multiple instances of this member, then the total count should be equal to one million times the number of members you have started.

The AtomicNumbers is very useful in lock free algorithms, although you need to take care that uncontrolled repeating can lead to live-locking; the system is appears to do something since no progress is made, but in reality it is only burning cpu cycles and network resources. In most cases it is best to limit the number of retries and throw an exception when it is exceeded.

If the AtomicNumber is a contention point in your system, there are a few ways of dealing with it depending on your requirements. One of the options it to create a stripe (essentially an array) of AtomicNumbers to reduce pressure; but this approach can't always be applied. Another options is to keep changes locally and only publish them to the AtomicNumber once and a while. There are a few downsides here; you could loose information if a member goes down and is that the newest value is not always immediately visible to the outside world. 

Hazelcast only provides support for the long, but you can always simulate for other types:
\begin{enumerate}
\item boolean: 0 for true and 1 for false.
\item double: a 64 bit double can be encoded into 64 bits, which can be stored in a long 
      which is also 64 bits.
\end{enumerate}

\section{Distributed Lock}
A lock (also called a mutex) is a synchronization primitive that makes it possible that only a single thread is able to access to a critical section of code; if multiple threads at the same moment would access that critical section concurrently, you would get race problems. 

Hazelcast provides a distributed lock implementation and makes it possible to create a critical section within a cluster of JVM's; so only a single thread from one of the JVM's in the cluster is allowed to acquire that lock. Other threads that want to acquire the lock, no matter if they are on the same JVM's or not, will not be able to acquire it and depending on the locking method they called, they either block or fail. The com.hazelcast.core.ILock; extends the java.util.concurrent.locks.Lock interface, so using the lock is quite simple.

To show you how a race problem can be solved with a Hazelcast lock, I have created a small example containing a race problem.
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class RacyMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        AtomicNumber number1 = hzInstance.getAtomicNumber("number1");
        AtomicNumber number2 = hzInstance.getAtomicNumber("number2");
        System.out.println("Started");
        for (int k = 0; k < 1000000; k++) {
            if (k % 10000 == 0) System.out.println("at: " + k);
            if (k % 2 == 0) {
                long n1 = number1.get();
                Thread.sleep(100);
                long n2 = number2.get();
                if (n1 - n2 != 0) System.out.println("Datarace detected!");
            } else {
                number1.incrementAndGet();
                number2.incrementAndGet();
            }
        }
        System.out.println("Finished");
    }
}
\end{lstlisting}
The idea is that a thread is going to increment 2 numbers half of the time and in the other half it will compare the 2 numbers and expects them to be equal. If they are not equal, we have a race problem and "Difference detected!" will be printed.

If we run only a single instance of the RacyMember, we won't get any race problem since either the 2 numbers will be incremented, or the 2 numbers will be printed. There will no be an interleaving of these operations.

But if we start multiple instances of the RacyMember, we'll start to see the following:
\begin{lstlisting}
Datarace detected!
Datarace detected!
...
\end{lstlisting}
The reason this race problem is occurring is that the writing and the reading of the 2 numbers is not placed in a critical section. With the Hazelcast distributed lock we can easily solve this problem:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class RaceFreeMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        AtomicNumber number1 = hzInstance.getAtomicNumber("number1");
        AtomicNumber number2 = hzInstance.getAtomicNumber("number2");
        ILock lock = hazelcastInstance.getLock("lock");
        System.out.println("Started");
        for (int k = 0; k < 10000; k++) {
            if (k % 100 == 0) System.out.println("at: " + k);
            lock.lock();
            try {
                if (k % 2 == 0) {
                    long n1 = number1.get();
                    Thread.sleep(10);
                    long n2 = number2.get();
                    if (n1 - n2 != 0) System.out.println("Datarace detected!");
                } else {
                    number1.incrementAndGet();
                    number2.incrementAndGet();
                }
            } finally {
                lock.unlock();
            }
        }
        System.out.println("Finished");
    }
}
\end{lstlisting}
When this code is executed; you will not see "Datarace detected!". This is because the lock provides a critical section around writing and reading of the numbers. 

A few things worth knowing about the Hazelcast lock and locking in general:
\begin{enumerate}
\item No Condition support: With the java.util.concurrent.locks.Lock it also is possible to create a java.util.concurrent.locks.Condition object to wait for a certain condition to happen; e.g. a custom made queue just received an element. Unfortunately the Condition functionality is not supported by Hazelcast at the moment.
\item It is reentrant, so you can acquire it multiple times in a single thread without causing a deadlock, of course you need to release it as many times as you have acquired it, to make it available to other threads.
\item Just like with the other Lock implementations, it should always be acquired outside of a try/finally block. Else it can happen that the lock acquire failed, but an unlock is still executed. 
\item Fairness: TODO
\item Keep locks as short as possible. If locks are kept too long, it can lead to performance problems or worse: deadlock.
\item With locks it is easy to run into deadlocks if you don't know what you are doing; so make sure that you do. Having code you don't control running inside your locks is asking for problems. Make sure you understand exactly the scope of the lock. 
\item To reduce the length of a deadlock, the tryLock methods can be used that control the waiting period. The lock.lock() method luckily will not block indefinitely, but will timeout with a OperationTimeoutException after 300 seconds.  
\item Locks are automatically released when a member has acquired a lock and this member goes down. This prevents threads that are waiting for a lock to wait indefinitely and this is important for failover to work in a distributed system. The downside however is that if a member goes down that acquired the lock and started making changes, that other members could start to see partial changes. In these cases either the system could do some self repair or else a transaction potentially can solve the problem.
\item a lock must always be released by the same lock that acquired it, otherwise look at the ISemaphore.
\item there are no configuration options available for the lock
\item it isn't possible to partition a lock; so to have it close to your other partitioned data.
\end{enumerate}
\section{ISemaphore}
The semaphore is a classic synchronization aid that can be used to control the number of threads doing a certain activity concurrently, e.g. using a resource. Conceptually each semaphore has a number of permits, where each permit represents a single thread allowed to execute that activity concurrently. As soon as a thread want to start with the activity, it takes a permit (or waits until one becomes available) and once finished with the activity, the permit is returned.

If you initialize the semaphore with a single permit, it looks a lot like a lock. The big difference however is that the Semaphore no concept of ownership. So with a lock the thread that acquired the lock must release it, and with a semaphore any thread can release an acquired permit.

Hazelcast provides a distributed version of the java.util.concurrent.Semaphore named the com.hazelcast.core.ISemaphore. When a permit is acquired on the ISemaphore the following can happen:
\begin{enumerate}
\item a permit is available; the number of permits in the semaphore is decreased by one and the calling thread can continue. After that thread is finished with the activity, it should release the permit.
\item no permit is available; the calling thread will block until a permit comes available, a timeout happens, the thread is interrupted or when the semaphore is destroyed an InstanceDestroyedException will be thrown.
\end{enumerate}

I'll explain the semaphore with an example. To simulate a shared resource we have an AtomicNumber initialized with the value 0. We are going to use this resources 1000 times; a thread starts to use that resource it increments it and once completed it decrements it.
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class SemaphoreMember {
    public static void main(String[] args)throws Exception{
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        ISemaphore semaphore = hzInstance.getSemaphore("semaphore");
        AtomicNumber resource = hzInstance.getAtomicNumber("resource");
        for(int k=0;k<1000;k++){
            System.out.println("At iteration: "+k);
            System.out.println("Active Threads: " + resource.get());
            semaphore.acquire();
            try{
                resource.incrementAndGet();
                Thread.sleep(1000);
                resource.decrementAndGet();
            }finally{
                semaphore.release();
            }
        }
        System.out.println("Finished");
    }
}
\end{lstlisting}
We want to limit the concurrent access to the resource by allowing for at most 3 thread. We do this by configuring the initial-permits for the semaphore though the Hazelcast config file:
\begin{lstlisting}[language=xml]
<hazelcast xsi:schemaLocation="http://www.hazelcast.com/schema/config hazelcast-config-2.3.xsd"
           xmlns="http://www.hazelcast.com/schema/config"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
    <network>
        <join>
            <multicast enabled="true"/>
        </join>
    </network>
    <semaphore name="semaphore">
        <initial-permits>3</initial-permits>
    </semaphore>
</hazelcast>
\end{lstlisting}
When we start the SemaphoreMember 5 times you will see output like this:
\begin{lstlisting}
At iteration: 0
Active Threads: 1
At iteration: 1
Active Threads: 2
At iteration: 2
Active Threads: 3
At iteration: 3
Active Threads: 3
At iteration: 4
Active Threads: 3
\end{lstlisting}
As you can see the maximum number of concurrent threads of that resource always is equal or smaller than 3. As an experiment you can remove the semaphore acquire/release statements and see for yourself that there is no longer control on the number of concurrent usages of the resources.

A few things worth knowing about the ISemaphore:
\begin{enumerate}
\item fairness: the Semaphore acquire methods are fair and this is not configurable. So under contention, the longest waiting thread for a permit will acquire it before all other threads. This is done to prevent starvation (threads not being able to acquire a permit) at the expense of reduced throughput.
\item attach permits: one of the features added to the ISemaphore to make it more reliable in a distributed environment where failover is important, is the addition of attached permits. Normally when a permit is acquired, and the member that acquired the permit goes down, the permit is not released. The consequence is that the permit is lost and the maximum number of concurrent threads for a specific activity is reduced. It can even lead to a deadlock situation when the number of available permits reaches 0. With the attached permits, the permit is attached to a member, and when it goes down, the permit is automatically releases (similar as with the Hazelcast Lock).
\item the acquire() method doesn't timeout, unlike the Hazelcast Lock.lock() method. To prevent running into a deadlock, using one of timed acquire methods is a good solution.
\end

TODO: Semaphore factory.
TODO: Dynamic Semaphore creation.

\section{ICountDownLatch}
The CountDownLatch was introduced in Java 1.5 and is a synchronization aid that makes it possible for threads to wait until a set of operations being performed in another thread  complete. Very simplistically; a CountDownLatch could be seen as a gate containing a counter; behind this gate threads can wait till the counter reaches zero. In my experience CountDownLatches often are used when you have some kind of processing operation, and you one or more threads that wait till this operation is completed so they can do their logic.

In Hazelcast there also is a CountDownLatch; the org.hazelcast.core.ICountDownLatch. The following example is made up by 2 parts; 1 part is the 'Leader'. It will create a latch and do a 'countdown' to release it:

And we start a single leader, 
\begin{lstlisting}[language=java]
public class Leader{
    public static void main(String[] args)throws Exception{
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        ICountDownLatch latch = hzInstance.getCountDownLatch("countDownLatch");      
        System.out.println("Starting");
        //we init the latch with 1, since we only need to complete a single step.
        latch.setCount(1); 
        //do some sleeping to simulate doing something    
        Thread.sleep(5000);
        //now we do a countdown which notifies all followers
        latch.countDown();
        System.out.println("Leader finished");
        //we need to clean up the latch
        latch.destroy();
    }
}
\end{lstlisting}

TODO: What happens when the latch is destroyed before all the followers are notitifed?

And we also need followers; so Java applications that are going to wait for the latch to notify them:
\begin{lstlisting}[language=java]
public class Follower{
    public static void main(String[] args)throws Exception{
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null); 
        ICountDownLatch latch = hzInstance.getCountDownLatch("countDownLatch");
        System.out.println("Waiting");
        latch.await();
        System.out.println("Complete!");
    }
}
\end{lstlisting}
We can spawn as many of these waiters as you want, and all will be displaying:
\begin{lstlisting}
Waiting
\end{lstlisting}
Once the leader counts down to zero, all followers will be notified and show the following output:
\begin{lstlisting}
Waiting
Complete!
\end{lstlisting}
This example show a CountdownLatch with only a single 'step'. But if the process has n steps, initialize the CountdownLatch with n. In the following example a process that is made up of 2 steps is shown:
\begin{lstlisting}[language=java]
public class MultiStepLeader{
    public static void main(String[] args)throws Exception{
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        ICountDownLatch latch = hzInstance.getCountDownLatch("countDownLatch");
        latch.setCount(2);
        Thread.sleep(60000); 
        latch.countDown();
        System.out.println("Leader completed first step");      
        Thread.sleep(60000); 
        latch.countDown();
        System.out.println("Leader completed both steps");
        latch.destroy();
    }
}
\end{lstlisting}
Although the ICountdownLatch is a very useful synchronization aid, it probably isn't one you will use on a daily basis.

In practice you will probably not have all the countdown latches needed created up front. A way to deal with this is to let the 'Leader' create a ICountDownLatch when it starts with its steps, and send the id of this ICountdownLatch to all the followers. The followers can then retrieve the CountdownLatch by calling Hazelcast.getCountdownLatch(..).

Important: unlike Java's implementation, Hazelcast's ICountDownLatch count can be re-set after a countdown has finished but not during an active count. This allows the same proxy instance to be reused.

-- TODO: Text is copied.
The Hazelcast member that successfully invokes setCount(int) becomes the owner of the countdown and is responsible for staying connected to the cluster until the count reaches zero. If the owner  becomes disconnected prior to count reaching zero all awaiting threads will be notified. This provides a safety mechanism in the distributed environment.

\section{IdGenerator}
In the previous section we introduced the AtomicNumber and one of the things it can be used for is to generated unique id's within a cluster. Although it will work, it probably isn't the most scalable solution since all member will content on incrementing the value. If you are only interested in id's and not in the order the id's are generated and don't care when there is a risk of skipping id's, you can have a look at the com.hazelcast.core.IdGenerator.

Underneath is a small example of the id generator:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class IdGeneratorMember {
    public static void main(String[] args) {
        HazelcastInstance hazelcast = Hazelcast.newHazelcastInstance(null);
        IdGenerator idGenerator = hazelcast.getIdGenerator("idGenerator");
        for (int k = 0; k < 1000000; k++)
            System.out.printf("Id : %s\n", idGenerator.newId());
    }
}
\end{lstlisting}
If you start this multiple times, you will see in the console that there will not be any duplicate id's

The way the IdGenerator works is that each member claims a segment of id's to generate, e.g. 0..999999. This is done behind the scenes by incrementing a shared AtomicNumber. After it has claimed its segment, it can increment a local counter (so there is no network traffic). Only when the id's in its segments are used, then a new segment needs to be claimed and this is done by increasing the shared AtomicNumber. This means that members can generate id's much quicker (local increment vs a distributed one). And it will also scale a lot better because of reduced contention; instead of incrementing that AtomicNumber on every new id, only when all the id's in the segment are used, the AtomicNumber will be incremented.

Of course there are some downsides you need to be aware of:
\begin{enumerate}
\item the generated id's probably will be out of order
\item if a member goes down without fully using its range, there might be gaps.
\end{enumerate}
But for id generation, these limitations in most cases are not an issue. Apart from the IdGenerator, there are other options for creating cluster wide unique id's. One of them is the java.util.UUID, although it will take up more space than a long. 

\emph{If the cluster restarts} then id generation will start from 0.

\section{What is next?}
In this chapter we went over the various synchronization primitives that are directly exposes by Hazelcast. You can always build higher synchronization primitives on top of these lower ones.

TODO: Deadlocks, Livelocks, starvation etc. If you know how a hammer and a saw work, it doesn't mean that you immediately can build a house with it.
