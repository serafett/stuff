\chapter{Distributed Primitives}
If you have programmed in Java you have probably worked with concurrency primitives like the synchronized statement (the intrinsic lock) or perhaps even the concurrency library that was introduced in Java 5 under java.util.concurrent like the Executor, Lock and AtomicReference.

This concurrency functionality is useful if you want to write a Java application that use multiple threads, but the focus is to provide is synchronization in a single JVM and not distributed over multiple JVM's. Luckily Hazelcast provides support for various distributed synchronization primitives like the Lock, IAtomicLong etc. And apart from making synchronization between different JVM's possible, they also support high availably; so if one machine fails, the primitive remains usable for other JVM's.

\section{IAtomicLong}
The IAtomicLong, formally known as the AtomicNumber, is the distributed version of the java.util.concurrent.atomic.AtomicLong, so if you have used that before, working with the IAtomicLong should feel very similar. The IAtomicLong exposes most of the operations the AtomicLong provides like get, set, getAndSet, compareAndSet and incrementAndGet, but there is of course a big difference in performance since remote calls are involved.

I'll demonstrate the IAtomicLong by creating an instance and incrementing it one million times:
\begin{lstlisting}[language=java]
public class Member {
   public static void main(String[] args) {
      HazelcastInstance hz = Hazelcast.newHazelcastInstance();
      IAtomicLong counter = hz.getAtomicLong("counter");
      for (int k = 0; k < 1000 * 1000; k++) {
         if (k % 500000 == 0) 
            System.out.println("At: "+k);
         counter.incrementAndGet();
      }
      System.out.printf("Count is %s\n", counter.get());
   }
}
\end{lstlisting}
If you start this Member, you will see this:
\begin{lstlisting}
At: 0
At: 500000
Count is 1000000
\end{lstlisting}
If you run multiple instances of this member, then the total count should be equal to one million times the number of members you have started.

If the IAtomicLong becomes a contention point in your system, there are a few ways of dealing with it, depending on your requirements. One of the options it to create a stripe (essentially an array) of IAtomicLong instances to reduce pressure. Another options is to keep changes local and only publish them to the IAtomicLong once and a while. There are a few downsides here; you could loose information if a member goes down and is that the newest value is not always immediately visible to the outside world. If you want to generate unique id's, you can have a look at the IdGenerator.

Hazelcast only provides support for the long, but you can always simulate other types:
\begin{enumerate}
\item boolean: 0 for true and 1 for false.
\item double: a 64 bit double can be encoded into 64 bits, which can be stored in a long since is also 64 bits.
\end{enumerate}
There is no support for an atomic reference, but if you need it you can build it on top of the IMap. The name of the reference could be the key and the value could be the reference value. The IMap implements the ConcurrentMap interface, so you can lift on atomic operations like replace(K key, V oldValue, V newValue). 

\section{IdGenerator}
In previous section the IAtomicLong was introduced and one of the things it can be used for is to generated unique id's within a cluster. Although it will work, it probably isn't the most scalable solution since all member will content on incrementing the value. If you are only interested in unique id's you can have a look at the com.hazelcast.core.IdGenerator.

The way the IdGenerator works is that each member claims a segment of 1 million id's to generate. This is done behind the scenes by using an IAtomicLong and claiming a segment is done by incrementing that IAtomicLong by a million. After claiming the segment, the IdGenerator can increment a local counter. Once all id's in the segment are used, it needs to claim a new segment. The consequence of this approach is that only 1 in a million times you need to do network traffic; so 999.999 out of 1.000.000 the id generation can be done in memory and is extremely fast. Another consequence is that this approach scales a lot better than an IAtomicLong because there is a lot less contention: 1 out of 1.000.000 instead of 1 out of 1.

So lets see the IdGenerator in action:
\begin{lstlisting}[language=java]
public class IdGeneratorMember {
   public static void main(String[] args) throws Exception{
      HazelcastInstance hz = Hazelcast.newHazelcastInstance();
      IdGenerator idGenerator = hz.getIdGenerator("id");
      for (int k = 0; k < 1000; k++){
         Thread.sleep(1000);
         System.out.printf("Id : %s\n", idGenerator.newId());
      }
   }
}
\end{lstlisting}
If you start this multiple times, you will see in the console that there will not be any duplicate id's.

Of course there are some issues you need to be aware of:
\begin{enumerate}
\item id's generated by different members will be out of order
\item if a member goes down without fully using its segment, there might be gaps.
\end{enumerate}
For id generation, in most cases, this isn't not relevant. There are alternative solutions for creating cluster wide unique id's. One of them is the java.util.UUID, although it will take up more space than a long, it doesn't rely on access to a Hazelcast cluster.

Another important issue you need to know is that if the cluster restarts, then the IdGenerator is reset and starts from 0 because the IdGenerator doesn't write to storage, e.g a database. If you need this, you can create your own IdGenerator based on the same implementation mechanism the IdGenerator uses, but you persist the updates to the IAtomicLong.

\section{Distributed Lock}
A lock is a synchronization primitive that makes it possible that only a single thread is able to access to a critical section of code; if multiple threads at the same moment would access that critical section concurrently, you would get race problems. 

Hazelcast provides a distributed lock implementation and makes it possible to create a critical section within a cluster of JVM's; so only a single thread from one of the JVM's in the cluster is allowed to acquire that lock. Other threads that want to acquire the lock, no matter if they are on the same JVM's or not, will not be able to acquire it and depending on the locking method they called, they either block or fail. The com.hazelcast.core.ILock extends the java.util.concurrent.locks.Lock interface, so using the lock is quite simple.

The following example shows how a lock can be used to solve a race problem:
\begin{lstlisting}[language=java]
public class RaceFreeMember {
   public static void main(String[] args) throws Exception {
      HazelcastInstance hz = Hazelcast.newHazelcastInstance();
      IAtomicLong number1 = hz.getAtomicLong("number1");
      IAtomicLong number2 = hz.getAtomicLong("number2");
      ILock lock = hi.getLock("lock");
      System.out.println("Started");
      for (int k = 0; k < 10000; k++) {
         if (k % 100 == 0) 
            System.out.println("at: " + k);
         lock.lock();
         try {
            if (k % 2 == 0) {
               long n1 = number1.get();
               Thread.sleep(10);
               long n2 = number2.get();
               if (n1 - n2 != 0) 
                  System.out.println("Datarace detected!");
               } else {
                    number1.incrementAndGet();
                    number2.incrementAndGet();
               }
         } finally {lock.unlock();}
      }
      System.out.println("Finished");
   }
}
\end{lstlisting}
When this code is executed; you will not see "Datarace detected!". This is because the lock provides a critical section around writing and reading of the numbers. In the example code you will also find the version with a data race.

A few things worth knowing about the Hazelcast lock and locking in general:
\begin{enumerate}
\item Condition is not yet supported. With the java.util.concurrent.locks.Lock it is possible to create a java.util.concurrent.locks.Condition object to wait for a certain condition to happen; e.g. a queue just received an element another thread is waiting for.
\item It is reentrant, so you can acquire it multiple times in a single thread without causing a deadlock, of course you need to release it as many times as you have acquired it, to make it available to other threads.
\item Just like with the other Lock implementations, it should always be acquired outside of a try/finally block. Else it can happen that the lock acquire failed, but an unlock is still executed. 
\item Keep locks as short as possible. If locks are kept too long, it can lead to performance problems or worse: deadlock.
\item With locks it is easy to run into deadlocks if you don't know what you are doing; so make sure that you do. Having code you don't control running inside your locks is asking for problems. Make sure you understand exactly the scope of the lock. 
\item To reduce the chance of a deadlock, the tryLock methods can be used that control the waiting period. The lock.lock() method will not block indefinitely, but will timeout with a OperationTimeoutException after 300 seconds.  
\item Locks are automatically released when a member has acquired a lock and this member goes down. This prevents threads that are waiting for a lock to wait indefinitely and needed for failover to work in a distributed system. The downside however is that if a member goes down that acquired the lock and started making changes, that other members could start to see partial changes. In these cases either the system could do some self repair or else a transaction potentially can solve the problem.
\item a lock must always be released by the same lock that acquired it, otherwise look at the ISemaphore.
\item locks are fair, so locks will be granted in the order they are requested.
\item there are no configuration options available for the lock
\end{enumerate}

\section{ISemaphore}
The semaphore is a classic synchronization aid that can be used to control the number of threads doing a certain activity concurrently, e.g. using a resource. Conceptually each semaphore has a number of permits, where each permit represents a single thread allowed to execute that activity concurrently. As soon as a thread want to start with the activity, it takes a permit (or waits until one becomes available) and once finished with the activity, the permit is returned.

If you initialize the semaphore with a single permit, it looks a lot like a lock. One of the big difference is that the Semaphore has no concept of ownership. So with a lock the thread that acquired the lock must release it, but with a semaphore any thread can release an acquired permit. Another difference is that an exclusive lock only has 1 permit and a semaphore can have more than 1.

Hazelcast provides a distributed version of the java.util.concurrent.Semaphore named the com.hazelcast.core.ISemaphore. When a permit is acquired on the ISemaphore the following can happen:
\begin{enumerate}
\item a permit is available. The number of permits in the semaphore is decreased by one and the calling thread can continue. 
\item no permit is available. The calling thread will block until a permit comes available, a timeout happens, the thread is interrupted or when the semaphore is destroyed an InstanceDestroyedException will be thrown.
\end{enumerate}
I'll explain the semaphore with an example. To simulate a shared resource we have an IAtomicLong initialized with the value 0. This resource is going to used 1000 times, When a thread starts to use that resource it increments it and once completed it decrements it.
\begin{lstlisting}[language=java]
public class SemaphoreMember {
   public static void main(String[] args)throws Exception{
      HazelcastInstance hz = Hazelcast.newHazelcastInstance();
      ISemaphore semaphore = hz.getSemaphore("semaphore");
      IAtomicLong resource = hz.getAtomicLong("resource");
      for(int k=0;k<1000;k++){
         System.out.println("At iteration: "+k +
            ", Active Threads: " + resource.get());
         semaphore.acquire();
         try{
            resource.incrementAndGet();
            Thread.sleep(1000);
            resource.decrementAndGet();
         }finally{semaphore.release();}
      }
      System.out.println("Finished");
   }
}
\end{lstlisting}
We want to limit the concurrent access to the resource by allowing for at most 3 thread. This can be done by configuring the initial-permits for the semaphore in the Hazelcast config file:
\begin{lstlisting}[language=xml]
<semaphore name="semaphore">
   <initial-permits>3</initial-permits>
</semaphore>
\end{lstlisting}
When you start the SemaphoreMember 5 times you will see output like this:
\begin{lstlisting}
At iteration: 0, Active Threads: 1
At iteration: 1, Active Threads: 2
At iteration: 2, Active Threads: 3
At iteration: 3, Active Threads: 3
At iteration: 4, Active Threads: 3
\end{lstlisting}
As you can see the maximum number of concurrent threads using that resource always is equal or smaller than 3. As an experiment you can remove the semaphore acquire/release statements and see for yourself that there is no longer control on the number of concurrent usages of the resources.

A few things worth knowing about the ISemaphore:
\begin{enumerate}
\item fairness: the Semaphore acquire methods are fair and this is not configurable. So under contention, the longest waiting thread for a permit will acquire it before all other threads. This is done to prevent starvation, at the expense of reduced throughput.
\item attach permits: one of the features added to the ISemaphore to make it more reliable in a distributed environment where failover is important, is the addition of attached permits. Normally when a permit is acquired, and the member that acquired the permit goes down, the permit is not released. The consequence is that the permit is lost and the maximum number of concurrent threads for a specific activity is reduced. It can even lead to a deadlock situation when the number of available permits reaches 0. With the attached permits, the permit is attached to a member, and when it goes down, the permit is automatically released (similar as with the Hazelcast Lock).
\item the acquire() method doesn't timeout, unlike the Hazelcast Lock.lock() method. To prevent running into a deadlock, using one of timed acquire methods is a good solution.
\end{enumerate}

[TODO: Semaphore factory.]

\section{ICountDownLatch}
The java.util.concurrent.CountDownLatch was introduced in Java 1.5 and is a synchronization aid that makes it possible for threads to wait until a set of operations, being performed by one or more threads to complete. Very simplistically; a CountDownLatch could be seen as a gate containing a counter. Behind this gate, threads can wait till the counter reaches 0. In my experience CountDownLatches often are used when you have some kind of processing operation, and one or more threads need to wait till this operation completes so they can execute their logic. Hazelcast also contains a CountDownLatch; the org.hazelcast.core.ICountDownLatch.

To explain the ICountDownLatch, image there is a leader process that is executing some action and eventually completes. And imagine that there are one or more follower processes that need to do something after the leader has completed. We can implement the behavior of the Leader:
\begin{lstlisting}[language=java]
public class Leader{
   public static void main(String[] args)throws Exception{
      HazelcastInstance hz = Hazelcast.newHazelcastInstance();
      ICountDownLatch latch = hz.getCountDownLatch("latch");      
      System.out.println("Starting");
      latch.trySetCount(1); 
      Thread.sleep(5000);
      latch.countDown();
      System.out.println("Leader finished");
      latch.destroy();
   }
}
\end{lstlisting}
The Leader retrieves the CountDownLatch, calls trySetCount on it which makes him owner of that latch, does some waiting and then calls countdown; which notifies are listeners for that latch. Currently we ignore the boolean return value of trySetCount since there will only be a single Leader, but in practice you probably want deal with the return value. Although there will only be a single owner of the Latch, the countDown method can be called by other threads/processes.

The next part is the Follower:
\begin{lstlisting}[language=java]
public class Follower {
   public static void main(String[] args) throws Exception {
      HazelcastInstance hz = Hazelcast.newHazelcastInstance();
      ICountDownLatch latch = hz.getCountDownLatch("latch");
      System.out.println("Waiting");
      boolean success = latch.await(10, TimeUnit.SECONDS);
      System.out.println("Complete:"+success);
  }
}
\end{lstlisting}
As you can see we first retrieve the ICountDownLatch and then call await on it so the thread listens for the ICountDownLatch to reach 0. In practice it can happen than a process that should have called the ICountDownLatch.countDown method, fails and therefor the ICountDownLatch will never reach 0. To force you to deal with this situation, there is no await method without a timeout to prevent waiting indefinitely. 

If we first start a leader and then one or more followers, the followers will wait till the leader completes. It is important that the leader is started first, else the followers will immediately complete since the latch already is 0. The example show a ICountDownLatch with only a single step. But if a process has n steps, initialize the CountdownLatch with n instead of 1 and for each completed step call the countDown method.

One thing to watch out for is that a ICountDownLatch waiter can be notified prematurely. In a distributed environment the leader could go down before it has reached zero and this would result in the waiters to wait till the end of time. This behavior is undesirable, so Hazelcast will automatically notify all listeners if the ICountDownLatch owner gets disconnected. So it can be that listeners are notified before all steps of a certain process are completed. To deal with this situation the current state of the process needs to be verified and appropriate actions need to be undertaken. e.g. restart all operations, continue with the first failed operation, or throw an exception.

Although the ICountdownLatch is a very useful synchronization aid, it probably isn't one you will use on a daily basis. Unlike Java's implementation, Hazelcast's ICountDownLatch count can be re-set after a countdown has finished but not during an active count. 

\section{What is next?}
In this chapter we looked at various synchronization primitives that are supported by Hazelcast. If for whatever reason you need a different one you can try to build it on top of existing ones, or create a custom one using the Hazelcast SPI. One of the things I would like to see added the ability to control the partition the primitive is living on since this would improve locality of reference. 
