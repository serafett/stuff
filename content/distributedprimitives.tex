\chapter{Distributed Primitives}

If you have programmed in Java you have probably worked with concurrency primitives like the synchronized statement (the intrinsic lock) or perhaps even the concurrency library that was introduced in Java 5 under java.util.concurrent like the Executor, Lock and AtomicReference. And with every new release of Java all kinds of new functionality is added, like the Fork/Join framework in Java 7.

This concurrency functionality is useful if you want to write a Java application that use multiple threads. The focus of this functionality however is to provide concurrency abstractions to be used in a single JVM, although you can always connect multiple JVM's together either using raw sockets or a more higher level remoting library (RMI, Servlets etc)

Hazelcast provides direct support for synchronization primitives that are distributed. The basic distributed features that are provided are:
\begin{enumerate}
\item failover; so if one machine fails, the system will still be working
\item scalability; just add more machines and you will get more capacity. Although this doesn't mean that your system will automatically become better scalable when you introduce Hazelcast, 
\end{enumerate}
In this chapter the following distributed primitives will be explained:
\begin{enumerate}
\item AtomicNumber
\item Lock
\item ISemaphore
\item ICountdownLatch
\item IdGenerator
\end{enumerate}
\section{AtomicNumber}
The AtomicNumber is the distributed version of the java.util.concurrent.atomic.AtomicLong, so if you have used that before, working with the AtomicNumber should feel very similar. The main difference is the way it is created. The AtomicNumber exposes most of the operations the AtomicLong provides like get, set, getAndSet, compareAndSet and incrementAndGet, there is of course a big difference in performance since remote calls are involved.

We'll demonstrate the AtomicNumber by creating an instance and incrementing it one million times:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class AtomicNumberMember {
    public static void main(String[] args) {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
        AtomicNumber counter = hzInstance.getAtomicNumber("counter");
        for (int k = 0; k < 1000 * 1000; k++) {
            if (k % 200000 == 0) System.out.println("At: "+k);
            counter.incrementAndGet();
        }
        System.out.printf("Count is %s\n", counter.get());
    }
}
\end{lstlisting}
If you start the AtomicNumberMember, you will see this:
\begin{lstlisting}
At: 0
At: 200000
At: 400000
At: 600000
At: 800000
Count is 1000000
\end{lstlisting}
If you run multiple instances of this member, then the total count should be equal to one million times the number of members you have started.

The AtomicNumbers is very useful in lock free algorithms, although you need to take care that uncontrolled repeating can lead to live-locking; the system is appears to do something since no progress is made, but in reality it is only burning cpu cycles and network resources. In most cases it is best to limit the number of retries and throw an exception when it is exceeded.

If the AtomicNumber is a contention point in your system, there are a few ways of dealing with it depending on your requirements. One of the options it to create a stripe (essentially an array) of AtomicNumbers to reduce pressure; but this approach can't always be applied. Another options is to keep changes locally and only publish them to the AtomicNumber once and a while. There are a few downsides here; you could loose information if a member goes down and is that the newest value is not always immediately visible to the outside world. If you want to generate unique id's, you can should a look at the IdGenerator.

Hazelcast only provides support for the long, but you can always simulate for other types:
\begin{enumerate}
\item boolean: 0 for true and 1 for false.
\item double: a 64 bit double can be encoded into 64 bits, which can be stored in a long since is also 64 bits.
\end{enumerate}
There is no support for an Atomic reference, but if you need it you can build it on top of a Hazelcast map. The name of the reference could be the key and the value could be the reference. The Hazelcast IMap implements the ConcurrentMap interface, so you can lift on operations like replace(K key, V oldValue, V newValue) . 

\section{Distributed Lock}
A lock is a synchronization primitive that makes it possible that only a single thread is able to access to a critical section of code; if multiple threads at the same moment would access that critical section concurrently, you would get race problems. 

Hazelcast provides a distributed lock implementation and makes it possible to create a critical section within a cluster of JVM's; so only a single thread from one of the JVM's in the cluster is allowed to acquire that lock. Other threads that want to acquire the lock, no matter if they are on the same JVM's or not, will not be able to acquire it and depending on the locking method they called, they either block or fail. The com.hazelcast.core.ILock extends the java.util.concurrent.locks.Lock interface, so using the lock is quite simple.

To show you how a race problem can be solved with a Hazelcast lock, I have created a small example containing a race problem:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class RacyMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
        AtomicNumber number1 = hzInstance.getAtomicNumber("number1");
        AtomicNumber number2 = hzInstance.getAtomicNumber("number2");
        System.out.println("Started");
        for (int k = 0; k < 1000000; k++) {
            if (k % 10000 == 0) System.out.println("at: " + k);
            if (k % 2 == 0) {
                long n1 = number1.get();
                Thread.sleep(100);
                long n2 = number2.get();
                if (n1 - n2 != 0) System.out.println("Datarace detected!");
            } else {
                number1.incrementAndGet();
                number2.incrementAndGet();
            }
        }
        System.out.println("Finished");
    }
}
\end{lstlisting}
The idea is that a thread is going to increment 2 numbers half of the time and in the other half it will compare the 2 numbers and expects them to be equal. If they are not equal, we have a race problem and "Difference detected!" will be printed.

If we run only a single instance of the RacyMember, we won't get any race problem since either the 2 numbers will be incremented, or the 2 numbers will be printed. There will no be an interleaving of these operations.

But if we start multiple instances of the RacyMember, we'll start to see the following:
\begin{lstlisting}
Datarace detected!
Datarace detected!
...
\end{lstlisting}
The reason this race problem is occurring is that the writing and the reading of the 2 numbers is not an atomic operations and should be placed in a critical section. With the Hazelcast distributed lock we can easily solve this problem:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class RaceFreeMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
        AtomicNumber number1 = hzInstance.getAtomicNumber("number1");
        AtomicNumber number2 = hzInstance.getAtomicNumber("number2");
        ILock lock = hazelcastInstance.getLock("lock");
        System.out.println("Started");
        for (int k = 0; k < 10000; k++) {
            if (k % 100 == 0) System.out.println("at: " + k);
            lock.lock();
            try {
                if (k % 2 == 0) {
                    long n1 = number1.get();
                    Thread.sleep(10);
                    long n2 = number2.get();
                    if (n1 - n2 != 0) System.out.println("Datarace detected!");
                } else {
                    number1.incrementAndGet();
                    number2.incrementAndGet();
                }
            } finally {
                lock.unlock();
            }
        }
        System.out.println("Finished");
    }
}
\end{lstlisting}
When this code is executed; you will not see "Datarace detected!". This is because the lock provides a critical section around writing and reading of the numbers. 

A few things worth knowing about the Hazelcast lock and locking in general:
\begin{enumerate}
\item Condition is not supported. With the java.util.concurrent.locks.Lock it is possible to create a java.util.concurrent.locks.Condition object to wait for a certain condition to happen; e.g. a custom made queue just received an element. Unfortunately the Condition is not supported by Hazelcast at the moment.
\item It is reentrant, so you can acquire it multiple times in a single thread without causing a deadlock, of course you need to release it as many times as you have acquired it, to make it available to other threads.
\item Just like with the other Lock implementations, it should always be acquired outside of a try/finally block. Else it can happen that the lock acquire failed, but an unlock is still executed. 
\item Fairness: TODO
\item Keep locks as short as possible. If locks are kept too long, it can lead to performance problems or worse: deadlock.
\item With locks it is easy to run into deadlocks if you don't know what you are doing; so make sure that you do. Having code you don't control running inside your locks is asking for problems. Make sure you understand exactly the scope of the lock. 
\item To reduce the length of a deadlock, the tryLock methods can be used that control the waiting period. The lock.lock() method luckily will not block indefinitely, but will timeout with a OperationTimeoutException after 300 seconds.  
\item Locks are automatically released when a member has acquired a lock and this member goes down. This prevents threads that are waiting for a lock to wait indefinitely and this is important for failover to work in a distributed system. The downside however is that if a member goes down that acquired the lock and started making changes, that other members could start to see partial changes. In these cases either the system could do some self repair or else a transaction potentially can solve the problem.
\item a lock must always be released by the same lock that acquired it, otherwise look at the ISemaphore.
\item there are no configuration options available for the lock
\item it isn't possible to partition a lock; so to have it close to your other partitioned data.
\end{enumerate}

\section{ISemaphore}
The semaphore is a classic synchronization aid that can be used to control the number of threads doing a certain activity concurrently, e.g. using a resource. Conceptually each semaphore has a number of permits, where each permit represents a single thread allowed to execute that activity concurrently. As soon as a thread want to start with the activity, it takes a permit (or waits until one becomes available) and once finished with the activity, the permit is returned.

If you initialize the semaphore with a single permit, it looks a lot like a lock. The big difference however is that the Semaphore no concept of ownership. So with a lock the thread that acquired the lock must release it, and with a semaphore any thread can release an acquired permit.

Hazelcast provides a distributed version of the java.util.concurrent.Semaphore named the com.hazelcast.core.ISemaphore. When a permit is acquired on the ISemaphore the following can happen:
\begin{enumerate}
\item a permit is available. The number of permits in the semaphore is decreased by one and the calling thread can continue. 
\item no permit is available. The calling thread will block until a permit comes available, a timeout happens, the thread is interrupted or when the semaphore is destroyed an InstanceDestroyedException will be thrown.
\end{enumerate}

I'll explain the semaphore with an example. To simulate a shared resource we have an AtomicNumber initialized with the value 0. This resource is going to used 1000 times, When a thread starts to use that resource it increments it and once completed it decrements it.
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class SemaphoreMember {
    public static void main(String[] args)throws Exception{
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
        ISemaphore semaphore = hzInstance.getSemaphore("semaphore");
        AtomicNumber resource = hzInstance.getAtomicNumber("resource");
        for(int k=0;k<1000;k++){
            System.out.println("At iteration: "+k);
            System.out.println("Active Threads: " + resource.get());
            semaphore.acquire();
            try{
                resource.incrementAndGet();
                Thread.sleep(1000);
                resource.decrementAndGet();
            }finally{
                semaphore.release();
            }
        }
        System.out.println("Finished");
    }
}
\end{lstlisting}
We want to limit the concurrent access to the resource by allowing for at most 3 thread. This can be done by configuring the initial-permits for the semaphore in the Hazelcast config file:
\begin{lstlisting}[language=xml]
<hazelcast>
    <network>
        <join><multicast enabled="true"/></join>
    </network>
    <semaphore name="semaphore">
        <initial-permits>3</initial-permits>
    </semaphore>
</hazelcast>
\end{lstlisting}
When you start the SemaphoreMember 5 times you will see output like this:
\begin{lstlisting}
At iteration: 0
Active Threads: 1
At iteration: 1
Active Threads: 2
At iteration: 2
Active Threads: 3
At iteration: 3
Active Threads: 3
At iteration: 4
Active Threads: 3
\end{lstlisting}
As you can see the maximum number of concurrent threads using that resource always is equal or smaller than 3. As an experiment you can remove the semaphore acquire/release statements and see for yourself that there is no longer control on the number of concurrent usages of the resources.

A few things worth knowing about the ISemaphore:
\begin{enumerate}
\item fairness: the Semaphore acquire methods are fair and this is not configurable. So under contention, the longest waiting thread for a permit will acquire it before all other threads. This is done to prevent starvation, at the expense of reduced throughput.
\item attach permits: one of the features added to the ISemaphore to make it more reliable in a distributed environment where failover is important, is the addition of attached permits. Normally when a permit is acquired, and the member that acquired the permit goes down, the permit is not released. The consequence is that the permit is lost and the maximum number of concurrent threads for a specific activity is reduced. It can even lead to a deadlock situation when the number of available permits reaches 0. With the attached permits, the permit is attached to a member, and when it goes down, the permit is automatically released (similar as with the Hazelcast Lock).
\item the acquire() method doesn't timeout, unlike the Hazelcast Lock.lock() method. To prevent running into a deadlock, using one of timed acquire methods is a good solution.
\end{enumerate}

[TODO: Semaphore factory.]

\section{ICountDownLatch}
The java.util.concurrent.CountDownLatch was introduced in Java 1.5 and is a synchronization aid that makes it possible for threads to wait until a set of operations, being performed in another thread, complete. Very simplistically; a CountDownLatch could be seen as a gate containing a counter; behind this gate threads can wait till the counter reaches zero. In my experience CountDownLatches often are used when you have some kind of processing operation, and one or more threads need to wait till this operation completes so they can execute their logic. Hazelcast also contains a CountDownLatch; the org.hazelcast.core.ICountDownLatch.

The explain the ICountDownLatch, image there is a leader process that undertakes some action and eventually completes. And imagine that there are one or more follower processes that need to do something after the leader has completed. We can implement the behavior of the Leader and the Follower using the ICountDownLatch like this:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class Leader{
    public static void main(String[] args)throws Exception{
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
        ICountDownLatch latch = hzInstance.getCountDownLatch("countDownLatch");      
        System.out.println("Starting");
        latch.setCount(1); 
        Thread.sleep(5000);
        latch.countDown();
        System.out.println("Leader finished");
        latch.destroy();
    }
}

public class Follower{
    public static void main(String[] args)throws Exception{
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(); 
        ICountDownLatch latch = hzInstance.getCountDownLatch("countDownLatch");
        System.out.println("Waiting");
        latch.await();
        System.out.println("Complete!");
    }
}
\end{lstlisting}
First we start a leader and then one or more followers. The followers will wait till the leader completes, so their output will show:
\begin{lstlisting}
Waiting
\end{lstlisting}
Then we start the Leader. Once the leader counts down to zero, all followers will be notified and show the following output:
\begin{lstlisting}
Waiting
Complete!
\end{lstlisting}
This example show a ICountDownLatch with only a single 'step'. But if the process has n steps, initialize the CountdownLatch with n instead of 1 and for each completed step call the 'countDown' method.

In practice you will probably not have all the countdown latches needed created up front. A way to deal with this is to let the 'Leader' create a ICountDownLatch when it starts, and send the name of this ICountdownLatch to all the followers. The followers can then retrieve the CountdownLatch by calling hzInstance.getCountdownLatch(..).

Although the ICountdownLatch is a very useful synchronization aid, it probably isn't one you will use on a daily basis. Unlike Java's implementation, Hazelcast's ICountDownLatch count can be re-set after a countdown has finished but not during an active count. This allows the same proxy instance to be reused.

In a distributed environment, members can go down. If the Leader in the example would go down, before the ICountDownLatch has reached zero, the Followers would wait till the end of time since the event will never come. This behavior is undesirable, so Hazelcast will automatically notify all listeners if the member that owns the ICountDownLatch, so the one that called 'setCount(int)', gets disconnected. So it can be that a thread is notified before all steps of a certain process are completed. To deal with this situation the current state of the process needs to be verified, and appropriate actions need to be undertaken. e.g. restart all operations, continue with the first failed operation, or throw an exception.

\section{IdGenerator}
In the beginning of this chapter we introduced the AtomicNumber and one of the things it can be used for is to generated unique id's within a cluster. Although it will work, it probably isn't the most scalable solution since all member will content on incrementing the value. If you are only interested in unique id's you can have a look at the com.hazelcast.core.IdGenerator.

Underneath is a small example of the id generator:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class IdGeneratorMember {
    public static void main(String[] args) throws Exception{
        HazelcastInstance hazelcast = Hazelcast.newHazelcastInstance();
        IdGenerator idGenerator = hazelcast.getIdGenerator("idGenerator");
        for (int k = 0; k < 1000; k++){
            Thread.sleep(1000);
            System.out.printf("Id : %s\n", idGenerator.newId());
        }
    }
}
\end{lstlisting}
If you start this multiple times, you will see in the console that there will not be any duplicate id's.

The way the IdGenerator works is that each member claims a segment of 1 million id's to generate. This is done behind the scenes by using an AtomicNumber and claiming a segment is done by incrementing that AtomicNumber by a million. After the claiming the segment, the IdGenerator can increment a local counter. Once all id's in the segment are used, it needs to claim a new segment. The consequence of this approach is that only 1 in a million times you need to do network traffic; so 999.999 out of 1.000.000 the id generation can be done in memory and is extremely fast. Another consequence is that this approach scales a lot better than an AtomicNumber because there is a lot less contention on the AtomicNumber (1 out of 1.000.000 instead of 1 out of 1).

Of course there are some issues you need to be aware of:
\begin{enumerate}
\item id's generated by different members will be out of order
\item if a member goes down without fully using its segment, there might be gaps.
\end{enumerate}
For id generation these issues in most cases are not relevant. Apart from the IdGenerator, there are other options for creating cluster wide unique id's. One of them is the java.util.UUID, although it will take up more space than a long but it doesn't rely on access to a Hazelcast cluster.

Another important issue you need to know about is that if the cluster restarts, then id generation is reset and starts from 0 because the IdGenerator doesn't write to storage, e.g a database. If you need this, you can create your own id generator based on the same implementation mechanism the IdGenerator uses, but you persist the updates to the AtomicNumber.

\section{What is next?}
In this chapter we went over the various synchronization primitives that are directly exposes by Hazelcast. You can always build higher synchronization primitives on top of these lower ones.

TODO: Deadlocks, Livelocks, starvation etc. If you know how a hammer and a saw work, it doesn't mean that you immediately can build a house with it.
