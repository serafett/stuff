\chapter{Distributed Primitives}

This chapter explains the following distributed primitives:
\begin{enumerate}
\item Lock
\item ISemaphore
\item ICountdownLatch
\item INumber
\item IdGenerator
\end{enumerate}

\section{Distributed Lock}

A lock makes it possible to provide mutual exclusion, meaning that only a single
thread will have access to a certain resources. With the introduction of Java 5,
the java.util.concurrent.locks.Lock is introduced. Hazelcast makes it possible
to create a Lock that is distributed, so can be shared between multiple processes
running on different machines.

Example:

\begin{verbatim}
public class DistributedLock{
    public static void main(String[] args){
    }
}
\end{verbatim}

Important: when a node has acquired a lock and this node goes down, then that lock will
automatically be released. This prevents threads that are waiting for a lock to wait 
indefinitely and this is important for failover to work in a distributed system. The downside
however is that if a node that acquired the lock and started making changes, goes down,
that other nodes could start to see partial changes. In these cases either the system
could do some self repair or else a transaction potentially can solve the problem.

\section{ISemaphore}

The ISemaphore in Hazelcast is distributed version of the java.util.concurrent.Semaphore
implementation. a counting semaphore; it can be seen as a structure with 
a limit number of permits. When a permit needs to be obtained, a few things can happen:
\begin{enumerate}
\item a permit is available; the number of permits in the semaphore is decreased by one 
and the calling thread returns.
\item no permit is available; the calling thread will block until a permit comes available,
a timeout happens, the thread is interrupted or when the semaphore is destroyed a InstanceDestroyedException
will be thrown.
\end{enumerate}

A semaphore that is initialized with the value 1, will have similar behavior as a lock,
but with the big difference that with a semaphore a different thread is allowed to release
the permit than the thread that acquired the permit.

\begin{verbatim}
public class SemaphoreMain{
    public static void main(String[] args)throws Exception{
        SemaphoreConfig config = new SemaphoreConfig()
            .setName("semaphore")
            .setInitialPermits(2);
        //todo: how to create a semaphore based on some config
        ISemaphore semaphore = Hazelcast.getSemaphore("semaphore");
        AtomicNumber counter = Hazelcast.getAtomicNumber("counter");
        for(int k=0;k<1000000;k++){
            semaphore.acquire();
            try{
                Thread.sleep(1000);
            }finally{
                semaphore.release();
            }
        }
    }
}
\end{verbatim}

TODO: Explain attach

\section{ICountDownLatch}
The CountDownLatch was introduced in Java 1.5 and is a synchronization aid that makes it
possible for threads to wait until a set of operations being performed in another thread 
complete. Very simplistically; a CountDownLatch could be seen as a gate containing a counter;
behind this gate threads can wait till the counter reaches zero. 

In Hazelcast there also is a CountDownLatch; the org.hazelcast.core.ICountDownLatch. 
The following example is made up by 2 parts; 1 part is the 'Leader'. It will create a latch
and do a 'countdown' to release it:

And we start a single leader, 
\begin{verbatim}
public class Leader{
    public static void main(String[] args)throws Exception{
        ICountDownLatch latch = Hazelcast.getCountDownLatch("countDownLatch");
        //we init the latch with 1, since we only need to complete a single step.
        latch.setCount(1); 
        //do some sleeping to simulate doing something    
        Thread.sleep(5000);
        //now we do a countdown which opens the latch and all waiting
        //followers are notified.
        latch.countDown();
        System.out.println("Leader finished");
        //we need to clean up the latch
        latch.destroy();
    }
}
\end{verbatim}

TODO: What happens when the latch is destroyed before all the followers are notitifed?

And we also need followers; so Java applications that are going to wait for the 
latch to notify them.

\begin{verbatim}
public class Followers{
    public static void main(String[] args)throws Exception{
        ICountDownLatch latch = Hazelcast.getCountDownLatch("countDownLatch");
        System.out.println("Waiting");
        latch.await();
        System.out.println("Complete!")
    }
}
\end{verbatim}
We can spawn as many of these waiters as you want, and all will be displaying.

\begin{verbatim}
Waiting
\end{verbatim}

Once the leader counts down to zero, all followers will be notified and show the following output:

\begin{verbatim}
Waiting
Complete!
\end{verbatim}

This example show a CountdownSemaphore with only a single 'step'. But if the process has n steps,
the countdownlatch can be initialized with n. In the following example a process that is made up of 2 steps is shown.

\begin{verbatim}
public class MultiStepLeader{
    public static void main(String[] args)throws Exception{
        ICountDownLatch latch = Hazelcast.getCountDownLatch("countDownLatch");
        latch.setCount(2);

        Thread.sleep(60000); 
        latch.countDown();
        System.out.println("Leader completed first step");
        
        Thread.sleep(60000); 
        latch.countDown();
        System.out.println("Leader completed both steps");
        latch.destroy();
    }
}
\end{verbatim}

Although the ICountdownLatch is a very useful synchronization aid, it probably isn't one
you will use on a daily basis.

In practice you will probably not have all the countdown latches needed created up front.
A way to deal with this is to let the 'Leader' create a ICountDownLatch when it starts with
its steps, and send the id of this ICountdownLatch to all the followers. The followers can 
then retrieve the CountdownLatch by calling Hazelcast.getCountdownLatch(..).

Important: unlike Java's implementation, Hazelcast's ICountDownLatch count can be re-set
after a countdown has finished but not during an active count. This allows the same
proxy instance to be reused.

-- TODO: Text is copied.
The Hazelcast member that successfully invokes setCount(int) becomes the owner of the countdown 
and is responsible for staying connected to the cluster until the count reaches zero. If the owner 
becomes disconnected prior to count reaching zero all awaiting threads will be notified. This provides 
a safety mechanism in the distributed environment.

\section{AtomicNumber}

The AtomicNumber is the distributed version of the java.util.concurrent.atomic.AtomicLong.
The main reason I'm using the AtomicNumber for is for are counters. 

Below is a small example:
\begin{verbatim}
public class AtomicNumberMain{
    public static void main(String[] args){
        AtomicNumber counter = Hazelcast.getAtomicNumber("counter");
        int count = 1000 * 1000;
        for(int k=0;k<count;k++){
            counter.incrementAndGet();
        }
        System.out.printf("Count is %s\n",counter.get());
    }
}
\end{verbatim}
In this case the counter is incremented 1000*1000 times and if you run this application
in parallel, then the total count should be equal to 1000*1000 times the number of applications
you have started.

The AtomicNumber exposes most of the operations the AtomicLong provides like:
\begin{enumerate}
\item get
\item set
\item getAndSet
\item compareAndSet
\item incrementAndGet
\end{enumerate}
so if you have used the AtomicLong before, working with the AtomicNumber should feel similar.

With the AtomicNumbers can be used in lock free algorithms, although you need to 
take care that uncontrolled repeating can lead to live-locking; the system is appears to do
something, but in reality it is only burning cpu cycles. In case of a distributed version 
the system will also be consuming network resources. 

If the counter is going to be a contention point in your system, you can either create
a stripe (an array) of counters and pick one to increment. With n counters in place, you 
should have n times more scalability. Another option is to store increments locally and 
only once and a while increment the shared counter. The downside of this approach is that
you could loose counts if a node goes down for example. Another downside is that the newest
count is not always immediately visible. 

There currently is only support for the long. But you can always use this to simulate
\begin{enumerate}
\item boolean: 0 for true and 1 for false.
\item double: a 64 bit double can be encoded into 64 bits, which can be stored in a long 
      which is also 64 bits.
\end{enumerate}

\section{IdGenerator}

In the previous section we introduced the AtomicNumber and one of the things it
can be used for is to generated unique id's within a cluster. Although it will work,
it probably isn't the most scalable solution since all member will content on incrementing
the value. If you are only interested in id's and not in the order the id's are generated 
(or even when elements are skipped) you can have a look at the IdGenerator.

\begin{verbatim}
public class IdGeneratorMain{
    public static void main(String[] args){
        IdGenerator idGenerator = Hazelcast.getIdGenerator("idGenerator");
        int count = 1000 * 1000;
        for(int k=0;k<100;k++){
            long id = idGenerator.newId();
            System.out.printn("Id : %s\n",id);
        }
    }
}
\end{verbatim}

The way the org.hazelcast.core.IdGenerator works is that each node claims a segment of 
id's to generate, e.g. 0..999999. This is done behind the scenes by incrementing a shared 
AtomicNumber. After it has claimed its segment, it can increment a local counter (so there is 
no network traffic). Only when the id's in its segments are used, then a new segment
needs to be claimed and this is done by increasing the shared AtomicNumber.

This means that node's can generate id's much quicker (local increment vs a distributed
one). And it will also scale a lot better because of reduced contention; instead of incrementing
that AtomicNumber on every new id, only when all the id's in the segment are used, the
AtomicNumber will be incremented.

Of course there are some downsides you need to be aware of:
\begin{enumerate}
\item the generated id's probably will be out of order
\item if a node goes down without using its range, there might be gaps.
\end{enumerate}
But for id generation, these limitations in most cases are not an issue.

Apart from the IdGenerator, there are other options for creating cluster wide unique id's.
One of them is the java.util.UUID, although it will take up more space than a long. 

Important: If the cluster restarts then id generation will start from 0.

\section{What is next?}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi libero sem,
interdum eget varius vel, faucibus placerat purus. Sed vulputate diam sit amet
risus dapibus dignissim. Praesent lobortis eleifend augue. Cum sociis natoque
penatibus et magnis dis parturient montes, nascetur ridiculus mus. Morbi libero
turpis, viverra ac vulputate a, faucibus vel quam. Quisque interdum congue
lacus, in tempus nisl tincidunt at. Curabitur sed eros eu enim vehicula
fermentum quis nec justo. Vestibulum rutrum laoreet est, eget condimentum justo
feugiat at. Cras ac sem ac magna ornare tempor non nec nisl. Maecenas feugiat
fringilla nisl, vitae ullamcorper ante posuere a. Sed mollis lacinia interdum.
Vivamus vel urna metus. Nulla eget tellus sem. Praesent volutpat suscipit nulla,
nec dictum arcu iaculis id. Duis pharetra vestibulum sapien, quis pulvinar odio
pharetra id. Cras at erat velit, vel tincidunt elit. Curabitur vehicula leo eu
odio vulputate ac consequat nulla ultricies. Maecenas venenatis condimentum
urna ut ultrices. Aliquam blandit fermentum eros, ac lacinia sem scelerisque
at. Nullam vitae nisi at erat posuere cursus a non velit.
