\chapter{Distributed Executor Service}
Java 5 was perhaps the most fundamental upgrade since Java was released. On a language level we got generics, static imports, enumerations, varargs, enhanced for loop and annotations. Although less known, Java 5 also got fundamental fixes for the Java Memory Model (JSR-133) and we got a whole new concurrency library (JSR-166) found in java.util.concurrent.

This library contains a lot of goodies; some parts you probably don't use on a regular based, but other parts you perhaps do. One of the features that was added is the java.util.concurrent.Executor. The idea is that you wrap functionality in a Runnable if you don't need to return a value, or a in a Callable if you need to return a value, and it submitting it to the Executor. In case of the ThreadPoolExecutor, these tasks are stored in a work queue and there is a pool of worker threads that take work from this queue to process it.

A very basic example of the executor:
\begin{lstlisting}[language=java]
class EchoService{
   private final ExecutorService = Executors.newSingleThreadExecutor();
   public void echoAsynchronously(final String msg){
      executor.execute(new Runnable(){
         public void run() { System.out.println(msg); }
      });	
   }
}
\end{lstlisting}
So while a worker thread is processing the task, the thread that submitted the task is free to work asynchronously. There is virtually no limit in what you can do in a task; complex database operations, intensive cpu or IO operations, render images etc. The problem in a distributed system however is that the default implementation of the Executor, the ThreadPoolExecutor, is designed to run within a single JVM. In a distributed system you want that a task submitted in one JVM, can be processed in another. 

Luckily Hazelcast makes this possible by providing an ExecutorService implementation that is designed to be used in a distributed environment so that tasks can be send from one JVM to another. Lets start with a very simple example of ExecutorService in Hazelcast. We begin with a task that will do some waiting and echo a message:
\begin{lstlisting}[language=java]
import java.io.Serializable;
public class EchoTask implements Runnable, Serializable {
    private final String msg;
    public EchoTask(String msg) {
        this.msg = msg;
    }
    public void run() {
        try { Thread.sleep(5000);} 
        catch (InterruptedException e) {}
        System.out.println("echo:" + msg);
    }
}
\end{lstlisting}
This EchoTask implements the Runnable interface so that it can be submitted to the Executor. But it also implements the Serializable interface because it could be that it is send to a different JVM to be processed. Because of serialization you can't pass complex task dependencies like a database connection. The solution to this problem is adding enough 'context' so that when a task is executed, it is able to reconstruct its execution context. A simple singleton or a more complex one where a lookup based on the HazelcastInstance (see HazelcastInstanceAware) could be a solution.

The next part is the MasterMember that is responsible for submitting (and executing) 1000 echo messages:
\begin{lstlisting}[language=java]
import com.hazelcast.core.Hazelcast;
import java.util.concurrent.Executor;
public class MasterMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        ExecutorService executor = hzInstance.getExecutorService("executor");
        for (int k = 1; k <= 1000; k++) {
            Thread.sleep(1000);
            System.out.println("Producing echo task: " + k);
            executor.execute(new EchoTask("" + k));
        }
        System.out.println("EchoTaskMain finished!");
    }
}
\end{lstlisting}
First we retrieve the executor from the HazelcastInstance and then we slowly submit 1000 echo tasks. By default Hazelcast configures the executor with 40 threads in the pool. For our example we only need 1, so we configure it in the hazelcast.xml file like this:
\begin{lstlisting}[language=xml]
<hazelcast>
    <executor-service name="executor">
        <max-pool-size>1</max-pool-size>
        <core-pool-size>1</core-pool-size>
    </executor-service>
</hazelcast>
\end{lstlisting}
When the MasterMember is started you will get output like this:
\begin{lstlisting}
Producing echo task: 1
Producing echo task: 2
Producing echo task: 3
Producing echo task: 4
Producing echo task: 5
echo:1
Producing echo task: 6
Producing echo task: 7
Producing echo task: 8
Producing echo task: 9
Producing echo task: 10
echo:2
...
\end{lstlisting}
As you can see the production of messages is 1/second and the processing is 0.2/second (the echo task sleeps 5 seconds), this means that we produce work 5 times faster than we are able to process it. Apart from making the EchoTask faster, there are a 2 directions to scale:
\begin{enumerate}
\item scale up 
\item scale out
\end{enumerate}
Both are explained below and in practice they are often combined. 

\section{Scaling up}
Scaling up, also called vertical scaling, is done by increasing the processing capacity on a single JVM. Since each thread can process 0.2 messages/second and we get 1 message/second, if the Executor would have 5 threads it can process messages as fast as they are produced.

When you scale up you need to look carefully at the JVM if it can handle the additional load. If not; you need to increase its resources (either cpu, memory, disk etc). If you fail to do so the performance could degrade instead of improve. 

Scaling up the ExecutorService in Hazelcast is very simple, just increment the maxPoolSize and corePoolSize. Since we know that 5 threads is going to give maximum performance, lets set them to 5.
\begin{lstlisting}[language=xml]
<hazelcast>
    <network>
        <join><multicast enabled="true"/></join>
    </network>
    <executor-service name="executor">
        <max-pool-size>5</max-pool-size>
        <core-pool-size>5</core-pool-size>
    </executor-service>
</hazelcast>
\end{lstlisting}
When we run the MasterNode we'll see something like this:
\begin{lstlisting}
Producing echo task: 1
Producing echo task: 2
Producing echo task: 3
Producing echo task: 4
Producing echo task: 5
echo:1
Producing echo task: 6
echo:2
Producing echo task: 7
echo:3
Producing echo task: 8
echo:4
...
\end{lstlisting}
As you can see, the tasks are being processed as quickly as they are being produced. 

If you take a closer look at the previous hazelcast configuration, you see that the max-pool-size and core-pool-size are configured to be. The core-pool-size is the minimal size of the thread pool, even if there is nothing to do. But in some cases you want to have a thread pool with a dynamic size since threads (even if they are not running) consume memory. So if you want a pool that increases size when there is a lot of work to do and decreases size otherwise, you can configure the max-pool-size to be bigger than the core-pool-size. Example:
\begin{lstlisting}[language=xml]
<hazelcast>
     ...  
    <executor-service name="executor">
        <core-pool-size>1</core-pool-size> 
        <max-pool-size>5</max-pool-size>
        <keep-alive-seconds>60</keep-alive-seconds>
    </executor-service>
</hazelcast>
\end{lstlisting}
With this configuration the thread pool of the executor is configured with a minimum size of 1 and a maximum size of 5. You can also see that we configured the keep-alive-seconds; this is the number of seconds an idle thread is allowed to stay alive before it is destroyed. The default keep-alive-seconds is 300 (5 minutes), so if you are fine with that you don't need to configure it.

\section{Scaling out}
Scaling up was very simple since the cpu and memory have enough capacity. But often one or more of the resources will be the limiting factor. Of course we can always increase capacity, but there will be a point it will stop to be cost efficient since it expenses go up quicker than the capacity improvements.

Scaling out, also called horizontal scaling, is orthogonal to scaling up; instead of increasing the capacity of the system by increasing the capacity of a single machine, we just add more machines. In our case we can safely start up multiple Hazelcast members on the same machine since processing the task doesn't consume resources (it just waits a bit). But in real systems you probably want to add more machines (physical or virtualized) to the cluster.

To scale up our echo example, we can add the following very basic slave member:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class SlaveMember {
    public static void main(String[] args) {
        Hazelcast.newHazelcastInstance(null);
    }
}
\end{lstlisting}
We don't need to do anything else because this member will automatically start the executor that was started in the master node.

If one master and slave are started, you will see that the slave member is processing tasks as well:
\begin{lstlisting}
echo:31
echo:33
echo:35	
\end{lstlisting}
So in only a few lines of code, we are now able to scale out! If you want, you can start more slave members, but with tasks being created at 1 messages/second, adding more than 4 slave members (which make a total of 5 members running the executor) we have reached maximum performance. 

\section{Executors and HazelcastInstanceAware}
In a lot of cases when you execute a task, the task needs access to the HazelcastInstance to retrieve Hazelcast dependencies like an employeeMap. This can be by implementing HazelcastInstanceAware, e.g.

\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.io.Serializable;
public class SomeTask implements
        Runnable, Serializable, HazelcastInstanceAware {
    private transient HazelcastInstance hzInstance;
    public void setHazelcastInstance(HazelcastInstance hzInstance) {
        this.hzInstance = hzInstance;
    }
    public void run() {
        ....
    }
}
\end{lstlisting}
When this Task is processed (not when it is submitted), the Hazelcast executor checks if the task implements HazelcastInstanceAware. If so, it will inject the HazelcastInstance of the executor by calling the 'setHazelcastInstance' method. The hzInstance field can be transient since it will never be send over the line. 

\section{Routing}
Till so far we didn't care about which member did the actual processing of the task; as long as a member picks it up. But in some cases you want to have control on which member is going to execute. Luckily Hazelcast provides different ways to route messages:
\begin{enumerate}
\item any member. This is the default configuration.
\item specific member
\item the member hosting a specific partition
\item all or subset of the members.
\end{enumerate}
In the previous section we already covered routing to any member. In the following sections I'll explain the last 3 routing strategies.

\subsection{Executing on a specific member}
In some cases you want to execute a task on a specific member. In Hazelcast this can be done by wrapping the task in a org.hazelcast.core.DistributedTask and provide the member. When the Executor receives such a task, it will send the wrapped task to the provided member.

As an example we are going to start some members. The master member is going to echo a message to each member (including itself). This is done by retrieving all members using the Cluster object and iterating over the cluster members. To each of the members we are going to send an echo message containing their own address. 
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.util.concurrent.*;
public class MasterMember {
    public static void main(String[] args) {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        ExecutorService executorService = hzInstance.getExecutorService();
        for (Member member : hzInstance.getCluster().getMembers()) {
            EchoTask task = new EchoTask("echo" + member.getInetSocketAddress());
            executorService.execute(new DistributedTask(task, null, member));
        }
    }
}
\end{lstlisting}
When we start a few slaves and a master, we'll get output like:
\begin{lstlisting}
Members [2] {
	Member [192.168.1.100]:5702 this
	Member [192.168.1.100]:5703
}
...
echo/192.168.1.100:5702
\end{lstlisting}
As you can see, the EchoTasks are to the correct member.

\subsection{Executing on a member hosting a specific partition}
When an operation is executed in a distributed system, this operation can access distributed resources. If these resources are often other machine, scalability and performance suffer to do network overhead and limited capacity of the network. Luckily scalability and performance can be increased by improving locality of reference.

In Hazelcast this can be done by placing the resources for a task in the same partition and by sending the task to the member that owns that partition. When you start designing a distributed system, perhaps the most fundamental step is designing the partitioning schema.

Sending a task to member that owns the partition can be done in 2 ways; either let the task implement com.hazelcast.core.PartitionAware interface or make use of the com.hazelcast.core.DistributedTask.

As an example we are going to create a distributed system where there is some dummy data in a map and for every key in that map we are going to execute a verify task. This task will verify if it has been executed on the same member as where that key is residing:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.io.Serializable;
public class VerifyTask implements
        Runnable, PartitionAware, Serializable, HazelcastInstanceAware {
    private final String key;
    private transient HazelcastInstance hzInstance;
    public VerifyTask(String key) {
        this.key = key;
    }
    public void setHazelcastInstance(HazelcastInstance hzInstance) {
        this.hzInstance = hzInstance;
    }
    public void run() {
        IMap map = hzInstance.getMap("map");
        boolean localKey = map.localKeySet().contains(key);
        System.out.println("Key is local:" + localKey);
    }
    public Object getPartitionKey() {
        return key;
    }
}
\end{lstlisting}
The VerifyTask is implementing various interfaces. For this example the most important one is the PartitionAware interface that exposes the 'getPartitionKey' method. Adding this interface to a task indicates to Hazelcast that this task should be send to the member owning the partition the partitionKey belongs to. When the returned partitionKey is null, Hazelcast is free to select any member. If you look closer to the run method, you will see it accesses the map and checks if the key is hosted locally and prints the result.

The next step is the MasterMember which first creates a map with some entries; we only care about the key so the value is bogus. And then iterates over the keys in the map and send a VerifyTask for each key:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.util.*;
import java.util.concurrent.ExecutorService;
public class MasterMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        Map<String, String> map = hzInstance.getMap("map");
        for (int k = 0; k < 10; k++) 
            map.put(UUID.randomUUID().toString(), "");
        ExecutorService executor = hzInstance.getExecutorService();
        for (String key : map.keySet()) 
            executor.execute(new VerifyTask(key));
     }
}
\end{lstlisting}
First we start a few slave members and then we the master member and you will get output like:
\begin{lstlisting}
key is local:true
key is local:true
...
\end{lstlisting}
As you can see, the tasks are executed on the same member as where the data is living. A nice experiment is to let the VerifyTask getPartitionKey method return null. This will cause the task to be executed on a random member and therefor reduces locality of reference.

\subsection{Executing on all or subset of members}
In some cases you want to execute a task on multiple or even all members. This can be done using the com.hazelcast.core.MultiTask. You need to use this functionality wisely since it will cause load on multiple members , potentially all members, and therefor can reduce scalability.

In the following example there is a set of members and on these members there is a distributed map containing some entries. Each entry has some UUID as key and 1 as value. To demonstrate executing a task on all members, we are going to create a distributed sum operation that sums all values in the map:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.io.Serializable;
import java.util.concurrent.Callable;
public class SumTask implements
        Callable<Integer>, Serializable, HazelcastInstanceAware {
    private transient HazelcastInstance hzInstance;
    public void setHazelcastInstance(HazelcastInstance hzInstance) {
        this.hzInstance = hzInstance;
    }
    public Integer call() throws Exception {
        IMap<String, Integer> map = hzInstance.getMap("map");
        int result = 0;
        for (String key : map.localKeySet()) {
            System.out.println("Calculating for key: " + key);
            result += map.get(key);
        }
        System.out.println("Local Result: " + result);
        return result;
    }
}
\end{lstlisting}
When this SumTask is called it retrieves the map and then it iterates over all local keys, sums the values and returns the result.

The MasterMember will first create the map with some entries. Then it will create a MultiTask containing the SumTask and a set of all members. This MultiTask will be submitted to the executor where Hazelcast takes care of forking it to all members. After that the result are joined using the MultiTask.get operator which returns a Collection of Integers (the sum result for every member). And then all the integers in the set are added the result printed:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.util.*;
import java.util.concurrent.ExecutorService;
public class MasterMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        Map map = hzInstance.getMap("map");
        for (int k = 0; k < 5; k++)
            map.put(UUID.randomUUID().toString(), null);
        Set<Member> members = hzInstance.getCluster().getMembers();
        MultiTask<Integer> task = new MultiTask<Integer>(new SumTask(), members);
        hzInstance.getExecutorService().execute(task);
        Collection<Integer> results = task.get();
        int x = 0; for (Integer i : results) x += i;
        System.out.println("Result: " + x);
    }
}
\end{lstlisting}
When we start 1 slave and then a master member, we'll see something like this for the slave:
\begin{lstlisting}
Calculating for key: 6ed5fe89-b2f4-4644-95a3-19dffcc71a25
Calculating for key: 5c870a8c-e8d7-4a26-b17b-d94c71164f3f
Calculating for key: 024b1c5a-21d4-4f46-988b-67a567ae80c9
Local Result: 3
\end{lstlisting}
And master member:
\begin{lstlisting}
Calculating for key: 516bd5d3-8e47-48fb-8f87-bb647d7f3d1f
Calculating for key: 868b2f1e-e03d-4f1a-b5a8-47fb317f5a39
Local Result: 2
Result: 5
\end{lstlisting}
In this example we execute a task on all members, but if you only want to execute a task on a subset of members, you can apply some filtering on the members passed to the MultiTask.

\section{Futures}
The java Executor interface only exposes a single 'void execute(Runnable)' method that can be called to have a Runnable asynchronously executed. But in some cases you need to synchronize on results, e.g. when using a Callable or just want to wait till a runnable completes. And in some cases you want to cancel a task, e.g. when it runs too long. This can be done making use of the java.util.concurrent.Future in combination with one of the submit methods of the ExecutorService, which extends the Executor.

To demonstrate the future, we are going to calculate a Fibonacci number by wrapping calculation in a callable and synchronizing on the result:
\begin{lstlisting}[language=java]
import java.io.Serializable;
import java.util.concurrent.Callable;
public class FibonacciCallable implements Callable<Long>, Serializable {
    private final int input;
    public FibonacciCallable(int input) {
        this.input = input;
    }
    public Long call() {
        return calculate(input);
    }
    private long calculate(int n) {
        if (Thread.currentThread().isInterrupted()) {
            System.out.println("FibonacciCallable is interrupted");
            throw new RuntimeException("Fibonacci is interrupted");
        }
        if (n <= 1) return n;
        else return calculate(n - 1) + calculate(n - 2);
    }
}
\end{lstlisting}
If you look closely, you can see there is a check 'Thread.currentThread().isInterrupted()' that is run frequently while calculating. When the thread is interrupted, the Interrupted flag is set and the calculation will be aborted. In our case we are going to throw an exception.

The next step is submitting the task and using a Future to synchronize on results:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.util.concurrent.*;
public class MasterMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        ExecutorService executorService = hzInstance.getExecutorService();
        int n = Integer.parseInt(args[0]);
        Future<Long> future = executorService.submit(new FibonacciCallable(n));
        try {
            long result = future.get(10, TimeUnit.SECONDS);
            System.out.println("Result: "+result);
        } catch (TimeoutException ex) {
            System.out.println("A timeout happened, the future is cancelled");
            future.cancel(true);
        }
    }
}
\end{lstlisting}
As you can see when we call the executorService.submit(Callable) method, we get back a Future as result. This future allows us to synchronize on completion or cancel the computation. 

When we can run this application with 5 as argument, the output will be: 
\begin{lstlisting}
Result: 5
\end{lstlisting}
But when you run this application with 500 as argument, it will take more than 10 seconds to complete and therefor the future.get will timeout. The output will be:
\begin{lstlisting}
A timeout happened, the future is cancelled
FibonacciCallable is interrupted
\end{lstlisting}
This is because the future.get operation will timeout. If it doesn't on your machine, it could be that your machine is very quick and you need to use a smaller timeout. When the timeout happens, a TimeoutException is thrown and we cancel the future by calling 'future.cancel(true)'. This cancel operation on the future is send back to the thread that is potentially running on a different member and will set the interrupted status of that Thread. It is up to the task to check the interrupted status of the thread periodically to be able to be responsive to interrupts.

\section{Execution Callback}
With a future it is possible to synchronize on task completion. In some cases you want to synchronize on the completion of the task before executing some logic, in the same thread that submitted the task. But in other cases you want this post completion logic to be executed asynchronously, so that the submitting thread doesn't need to block. Hazelcast provides a solution for this using the execution callback.

In the 'Future' topic an example is shown where a Fibonacci number is calculated and waiting on the completion of that operation is done using a Future. In the following example we are also going to calculate a Fibonacci number, but instead of waiting for that task to complete, we register an ExecutionCallback where we print the result asynchronously. We do this by wrapping our Fibonacci task in a DistributedTask and register an ExecutionCallback that contains the printing logic:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.util.concurrent.*;
public class MasterMember {
    public static void main(String[] args){
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        ExecutorService es = hzInstance.getExecutorService();
        DistributedTask<Long> task = new DistributedTask<Long>(new FibonacciCallable(10));
        ExecutionCallback<Long> executionCallback = new ExecutionCallback<Long>() {
            public void done(Future<Long> future) {
                try {
                    if (!future.isCancelled()) 
                        System.out.println("Result: " + future.get());
                } catch (Exception e) {
                    e.printStackTrace();
                }
            }
        };
        task.setExecutionCallback(executionCallback);
        es.execute(task);
        System.out.println("Fibonacci task submitted");
    }
}
\end{lstlisting}
As you can see, the ExecutionCallback has a single method 'done' that needs to be implemented and receives the Future pointing to the FibonacciCallable. As soon as the task has completed (either normally, exception or cancelled) the 'done' method will be called and the status of the task, including its result, can be retrieved using this Future. In our case we are going to print the result.

If you run this example you will see the following output:
\begin{lstlisting}
Fibonacci task submitted
Result: 55
\end{lstlisting}
As you can see, the thread that submitted the tasks to be executed, was not blocked. You can also see that eventually the result of the Fibonacci calculation will be printed. 

\section{What is next}
In this chapter we explored the distributed execution of tasks using the Hazelcast ExecutorService. When you have worked with the ExecutorService before, you know that there are some gotcha's that can cause spending a lot of time on small things that are easy to solve. Hazelcast also introduces some gotcha's and good to knows:

\emph{Executors doesn't log exceptions:} when a task fails with an exception (or an error), this exception will not be logged by Hazelcast. This is in line with the ThreadPoolExecutorService from Java and it can be really annoying when you are spending a lot of time on why something doesn't work. It can easily be fixed; either add a try/catch in your runnable and log the exception. Or wrap the runnable/callable in a proxy that does the logging; the last option will keep your code a bit cleaner. 

\emph{Lite Members also execute tasks:} when you create a lite member, this member will also execute tasks and this can be really unexpected. When a task is executed on a lite member, it can access all the Hazelcast data structures without a problem since they are accessible on all members. If this really is ann issue, a native client could be a solution; see chapter 'Clients'.

\emph{No work queue capacity control:} with the normal ThreadPoolExecutor you can limit the amount of unprocessed work on the work queue and you also can control what happens with a surplus of work. With the Hazelcast distributed Executor there is no control. So it could be that you swamp a system with unprocessed work and this can lead to an java.lang.OutOfMemoryError.

\emph{No high availability for the workqueue}. Each member will create one or more local ThreadPoolExecutors with ordinary workqueues that do the real work. When a task is submitted, it will be put on the workqueue of that ThreadPoolExecutor and will not be backed up by Hazelcast. If something would happen with that member, all unprocessed work will be lost. A project that might be interesting to investigate is: https://github.com/jclawson/hazelcast-work.

\emph{Routing works on members and not on partitions}. If a member M1 would be owner of some key: K and a task with uses K is partition key, then Hazelcast finds member M1 based on PartitionService.getPartition(K).getOwner() and the task routed to M1 and  stored in the local ThreadPoolExecutor of M1. But if before the task is executed, a repartitioning happens, and member M2 would become owner of key K, then the task is still executed on M1. For normal map operations this is not an issue, since the requests will be routed to the correct machine. But if this would happen to the SumTask for example, the result can be a bogus value since map entries can be skipped or duplicates found. 

\emph{Accessing the Default Executor:} Each HazelcastInstance has a default ExecutorService that can be accessed with the method 'HazelcastInstance.getExecutorService()'

\emph{Destroying Executors}: todo: executors can't be destroyed by calling shutdown because it only disables the local proxy, and the ExecutorService returned by Hazelcast doesn't implement the Instance interface, so no destroy can be called.  

[TODO: Load balancer]
