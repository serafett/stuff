\chapter{Distributed Executor Service}

Java 5 was perhaps the most fundamental upgrade since Java was released. On a language level we got generics, static imports, enumerations, enhanced for loop and annotations. From a correctness point of view we get the new Java Memory Model. And we also got the executor framework in the java.util.concurrent library.

With the executor you can create task object which need to be implementations of one of the following
2 interfaces:
\begin{enumerate}
\item java.util.Runnable: if your tasks doesn't need to return a value.
\item java.util.concurrent.Callable: if your task does need to return a value.
\end{enumerate}
These tasks are created by one thread, and this thread puts these tasks in the executor where they are stored in a queue to be picked up by other threads; the worker threads. So while these worker threads are processing the tasks, the task that submitted the tasks is free to do other work. So tasks are being executed asynchronously, although if you use a future you can sync on task completion. Often these worker threaded are pooled threads since threads can be quite expensive to create. 

A very short example:
\begin{lstlisting}[language=java]
class EchoTest{
   private final Executor = Executors.newSingleThreadExecutor();

   public void echoAsynchronously(){
      executor.execute(new Runnable(){
         public void run(){
            ....
         }
      });	
   }
}
\end{lstlisting}

There is virtually no limit in what you can place in such a task; you could execute complex database operations, intensive cpu or IO operations. The problem in a distributed system however, is that the default implementations of the Executor are designed to be run within a single machine. So the thread responsible for creating a tasks will be running in the same JVM as the thread that executed the task.

Hazelcast extends the functionality provided by the Executor framework to make it fit to run in a distributed environment. This means that tasks placed by one JVM can executed on a different JVM. This makes it very easy to scale; just add more machines and the total capacity of processing tasks will increase.

Let start with a very simple example of a distributed executor in Hazelcast. We begin with creating a basic task that will do some waiting and echo a message:

\begin{lstlisting}[language=java]
import java.io.Serializable;
public class EchoTask implements Runnable, Serializable {
    private final String msg;
    public EchoTask(String msg) {
        this.msg = msg;
    }

    @Override
    public void run() {
        try {
            Thread.sleep(5000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        System.out.println("echo:" + msg);
    }
}
\end{lstlisting}

This EchoTask implements the Runnable interface so that it can be submitted to the Executor. But it also implements the Serializable interface because it is potentially going to be serialized to a different JVM. 

The next structure we are going to create is the EchoTaskMain:
\begin{lstlisting}[language=java]
import com.hazelcast.core.Hazelcast;
import java.util.concurrent.Executor;
public class MasterNode {
    public static void main(String[] args) throws Exception {
        Executor executor = Hazelcast.getExecutorService("executor");
        for (int k = 0; k < 1000; k++) {
            Thread.sleep(1000);
            executor.execute(new EchoTask("" + k));
        }
        System.out.println("EchoTaskMain finished!");
    }
}
\end{lstlisting}
As you can see we can retrieve the executor using Hazelcast.getExecutor and then we slowly submit 1000 echo tasks. When we execute this EchoTaskMain, we are going to see something like this in the output:

By default Hazelcast configures the executor to have 40 threads in the pool. For this example we only want a single thread per executor, so we configure it like this:

\begin{lstlisting}[language=xml]
<hazelcast xsi:schemaLocation="http://www.hazelcast.com/schema/config
            hazelcast-config-2.0.xsd"
           xmlns="http://www.hazelcast.com/schema/config"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
    <executor-service name="executor">
        <max-pool-size>1</max-pool-size>
        <core-pool-size>1</core-pool-size>
    </executor-service>
</hazelcast>
\end{lstlisting}

When we run running the MasterNode you will get output like this:
\begin{verbatim}
Producing echo task: 1
Producing echo task: 2
Producing echo task: 3
Producing echo task: 4
Producing echo task: 5
echo:1
Producing echo task: 6
Producing echo task: 7
Producing echo task: 8
Producing echo task: 9
Producing echo task: 10
echo:2
Producing echo task: 11
Producing echo task: 12	
\end{verbatim}

As you can see the production of messages is 1/second and the consumption is 0.2 per second (each echo task sleeps 5 seconds). There are few ways to speed things up (apart from making the echo task faster):
\begin{enumerate}
\item scale up 
\item scale out .
\end{enumerate}
Both of the strategies to scale are explained below and in practice they are often combined. 

\section{Scaling up}
(also called vertical scaling): to increase the capacity of the system we could launch more threads by setting the corePoolSize/maxPoolSize, since each thread can do 0.2 messages/second, if you run 5 threads in parallel, you can process 0.2 messages/second * 5 = 1.0 messages/second. When you scale up you need to look carefully at the machine if it can handle the additional load. If not; you need to increase its resources (either cpu, disk, memory etc); if you fail to do so, the performance could go down instead of up. But scaling up quickly becomes very expensive and scaling out stops to be cost efficient. 

Scaling up the ExecutorService in Hazelcast is very simple, just increment the max-pool-size and core-pool-size. Since we know that having 5 threads is going to give maximum performance, lets set it to five.
\begin{lstlisting}[language=xml]
<hazelcast xsi:schemaLocation="http://www.hazelcast.com/schema/config
            hazelcast-config-2.0.xsd"
           xmlns="http://www.hazelcast.com/schema/config"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
    <network>
        <join><multicast enabled="true"/></join>
    </network>

    <executor-service name="executor">
        <max-pool-size>5</max-pool-size>
        <core-pool-size>5</core-pool-size>
    </executor-service>
</hazelcast>
\end{lstlisting}

When we run the EchoTaskMain:
\begin{verbatim}
Producing echo task: 2
Producing echo task: 3
Producing echo task: 4
Producing echo task: 5
echo:1
Producing echo task: 6
echo:2
Producing echo task: 7
echo:3
Producing echo task: 8
echo:4
Producing echo task: 9
echo:5
Producing echo task: 10
echo:6
Producing echo task: 11	
\end{verbatim}
As you can see, the tasks are being processed as quickly as they are being produced. So we now have scale the system to its maximum capacity.

TODO: explain poolsize growing and time to live stuff of the threads

\section{Scaling out}
(also called horizontal scaling): we can increase the capacity of the system we just launch more boxes. In our case we are going to scale up be launching multiple jvm's since processing the echo tasks doesn't consume any resources because it does a simple wait. But in production system you probably want to add more machines to a cluster

To scale out we just need to launch one of these:

\begin{lstlisting}[language=java]
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
public class SlaveNode {
    public static void main(String[] args) {
        Hazelcast.getDefaultInstance();
    }
}
\end{lstlisting}
We don't need to do anything else. This EchoTaskWorker loads the hazelcast.xml, and since the executor is declared there, it will automatically start the executor and the thread in the thread pool. 

When you spawn one of these EchoTaskWorkers, you will see that it start picking up work:
\begin{verbatim}
echo:31
echo:33
echo:35	
\end{verbatim}
So in only a few lines of code, we are now able to scale out! If you want you can spawn more EchoTaskWorkers, but with 1 messages/second being processed and 0.2 messages/second being consumed per node (including the EchoTaskProducer), having more than 5 nodes will not speed things up.

\section{Executors and HazelcastInstanceAware}
In a lot of cases when you create a task, the task needs to get access to the HazelcastInstance to retrieve all kinds of datastructures. This can be done very easily by letting the Runnable/Callable implement the HazelcastInstanceAware interface, e.g.

\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.io.Serializable;
public class SomeTask implements
        Runnable, Serializable, HazelcastInstanceAware {
    private transient HazelcastInstance hazelcastInstance;
    @Override
    public void setHazelcastInstance(HazelcastInstance hazelcastInstance) {
        this.hazelcastInstance = hazelcastInstance;
    }
    @Override
    public void run() {
        ....
    }
}
\end{lstlisting}
When this Task is executed (not when it is submitted), the Hazelcast executor checks if the task implements HazelcastInstanceAware. And if so, it will inject the HazelcastInstance by calling the 'setHazelcastInstance' method. The hazelcastInstance field can be transitive since it will never be send over the line. 

\section{Callable}
The ExecutorService not only allows instances of the Runnable interface to be submitted, also the callable instances can be send. Callable's are useful if you want to return a value.

Lets start with a very basic Callable implementation that adds 2 int's and returns the result:
\begin{lstlisting}[language=java]
import java.util.concurrent.Callable;
import java.io.Serializable;
public class SumTask implements Callable<Integer>, Serializable {
    final int a;
    final int b;
    public SumTask(int a, int b) {
        this.a = a;
        this.b = b;
    }
    @Override
    public Integer call() {
        try {
            Thread.sleep(5000); 
        } catch (InterruptedException e) {
            e.printStackTrace();
        }	    
        return a+b;
    }
}

\end{lstlisting}

\section{Futures}
The normal java.util.concurrent.Executor only exposes a single 'void execute(Runnable)' method that can be called to have a task asynchronously executed. But in some cases you want to synchronize on completion or cancel the task (for example when it is running for too long). This can be done making use of the java.util.concurrent.Future and one of the submit methods of the ExecutorService.

In the first example we are going to wait for the result on a future and in the second example we are going to cancel a future.

\begin{lstlisting}[language=java]
import java.io.Serializable;
import java.util.concurrent.Callable;

public class FibonacciCallable implements Callable<Long>, Serializable {
    private final int input;

    public FibonacciCallable(int input) {
        this.input = input;
    }
    @Override
    public Long call() {
        return calculate(input);
    }
    private long calculate(int n) {
        if (Thread.currentThread().isInterrupted()) {
            System.out.println("FibonacciCallable is interrupted");
            throw new RuntimeException("Fibonacci is interrupted");
        }
        if (n <= 1) return n;
        else return calculate(n - 1) + calculate(n - 2);
    }
}
\end{lstlisting}

As you can see there is a check 'Thread.currentThread().isInterrupted()'. When the thread is interrupted, the calculation will be aborted. In our case we are going to throw an exception to abort the calculation.

\begin{lstlisting}[language=java]
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;

import java.util.concurrent.ExecutorService;
import java.util.concurrent.Future;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.TimeoutException;

public class MasterNode {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hazelcastInstance = Hazelcast.getDefaultInstance();
        ExecutorService executorService = hazelcastInstance.getExecutorService();
        int n = Integer.parseInt(args[0]);
        Future<Long> future = executorService.submit(new FibonacciCallable(n));
        try {
            long result = future.get(10, TimeUnit.SECONDS);
            System.out.println("result: "+result);
        } catch (TimeoutException ex) {
            System.out.println("A timeout happened, the future is cancelled");
            future.cancel(true);
        }
        System.out.println("Finished");
    }
}
\end{lstlisting}
As you can see when we call the executorService.submit, we get back a Future as result. This future allows us to synchronize on completion or cancel the computation (in hazelcast this cancellation will get back to the node that actually is running )

When we call run this application with 5 as argument, the output will be: 
\begin{verbatim}
result: 5
Finished
\end{verbatim}

But when you run this application with 500 as argument, it will take more than 10 seconds to complete and therefor the future.get will timeout. The output will be:
\begin{verbatim}
A timeout happened, the future is cancelled
FibonacciCallable is interrupted
Finished
\end{verbatim}
This is because the future.get operation will timeout (if it doesn't instead of using 10 seconds as timeout, use a lower value; it could be that you have a very fast machine). When the timeout happens, a TimeoutException is thrown. And when the TimeoutException is thrown, we cancel the future. This cancel operation on the future is send back to the thread (that is potentially running on a different jvm) and will set the interrrupted status of that Thread. It is up to the task (runnable/callable) that is being run by executor to check the interrupted status of the thread periodically to be able to be responsive to interrupts.

\section{Execution Callback}

\section{Routing}

Till so far we didn't care about which node did the actual processing of the task; as long as any member picked it up. But in some cases you want to have control on which node is going to execute. Luckily Hazelcast provides different ways to route messages:
\begin{enumerate}
\item executing on any member. This is the default configuration.
\item executing on a specific cluster member
\item executing on a node hosting a specific partition
\item executing on all or subset of the cluster members.
\end{enumerate}
In the sections below I will explain the other routing mechanisms.

\subsection{Executing on a specific cluster member}
In some cases you want to execute a task on a specific member. In Hazelcast this can be done by wrapping the task you want to execute in a org.hazelcast.core.DistributedTask and provide the member node you want the task to be executed on.

As an example we are going to start a bunch of nodes and we are going to echo a different message to each node. Lets begin with defining the echo task:

\begin{lstlisting}[language=java]
import java.io.Serializable;
public class EchoTask implements Runnable, Serializable{
    private final String msg;
    public EchoTask(String msg) {
        this.msg = msg;
    }
    @Override
    public void run() {
        System.out.println(msg);
    }
}
\end{lstlisting}

And a very basic slave node:
\begin{lstlisting}[language=java]
import com.hazelcast.core.Hazelcast;
public class SlaveNode {
    public static void main(String[] args){
        Hazelcast.getDefaultInstance();
    }
}
\end{lstlisting}

And the MasterNode that sends a echo task to every member:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.util.Set;
import java.util.concurrent.ExecutorService;
public class MasterNode {
    public static void main(String[] args){
        HazelcastInstance hazelcastInstance = Hazelcast.getDefaultInstance();
        ExecutorService executorService = hazelcastInstance.getExecutorService();

        Set<Member> members = hazelcastInstance.getCluster().getMembers();
        int k=0;
        for(Member member:members){
            EchoTask task = new EchoTask("echo-"+k);
            DistributedTask distributedTask = new DistributedTask(task, member);
            executorService.execute(distributedTask);
            k++;
        }
    }
}
\end{lstlisting}

When we start a one SlaveNodes and then start a single master node, we'll get output like:

slave node:
\begin{verbatim}
echo-0
\end{verbatim}

master node:
\begin{verbatim}
echo-1
\end{verbatim}

So as you can see, it is very easy to execute a task on a specific node.

TODO: What happens when the node goes down after the task has been submitted, but before it is executed. No other node is allowed to pick up that task since it was made specifically for that node. Is there some callback mechanism?

\subsection{Executing on a node hosting a specific partition}
Often you want to have locality of reference; so the data being used for a specific process should be close to the process. Locality of reference is important for a scalable system since the amount of network communication (which is also a shared resource) will be reduced.

In hazelcast this can be done by sending a task to the member that owns the data in a specific partition. This has the advantage that you don't need to go over the network to retrieve data, as long as it all is in the same partition. When you start writing a distributed system, perhaps the most fundamental step to do right in the beginning is to get the partitioning schema right.

In Hazelcast sending a task to node that owns the partition can be done in 2 ways; either let the task implement com.hazelcast.core.PartitionAware interface or make use of the com.hazelcast.core.DistributedTask or let the task implement the PartitionAware interface.

As an example we are going to create a clustered system where there is some dummy data in a map and for every key in that map we are going to execute a task. This task will verify if it has been executed on the same node as where that key is residing.

Lets begin with the task:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.io.Serializable;
public class LocalTask implements
        Runnable, PartitionAware, Serializable, HazelcastInstanceAware {
    private final String partitionId;
    private HazelcastInstance hazelcastInstance;
    public PartitionedTask(String partitionId) {
        this.partitionId = partitionId;
    }
    @Override
    public void setHazelcastInstance(HazelcastInstance hazelcastInstance) {
        this.hazelcastInstance = hazelcastInstance;
    }
    @Override
    public void run() {
        IMap map = hazelcastInstance.getMap("map");
        boolean localKey = map.localKeySet().contains(partitionId);
        System.out.println("key is local:" + localKey);
    }
    @Override
    public Object getPartitionKey() {
        return partitionId;
    }
}
\end{lstlisting}
The LocalTask is implementing various interfaces. The most important one in this case is the PartitionAware interface that exposes the 'getPartitionKey' method. Adding this interface to a task indicates to Hazelcast that when this task is executed, that the getPartitionKey method should be called to determine the actual node the tasks is going to be executed. If null is returned as partitionKey, Hazelcast has the freedom to execute the task on any node.

Then we create some slave nodes:
\begin{lstlisting}[language=java]
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
public class SlaveNode {
    public static void main(String[] args){
        Hazelcast.getDefaultInstance();
    }
}
\end{lstlisting}
Start up a single instance of this slave and keep it running.

And then we create the master node:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.util.*;
import java.util.concurrent.*;
public class MasterNode {
    public static void main(String[] args) throws Exception {
        HazelcastInstance defaultInstance = Hazelcast.getDefaultInstance();

        Map<String, String> map = defaultInstance.getMap("map");
        for (int k = 0; k < 10; k++) {
            map.put(UUID.randomUUID().toString(), "");
        }

        ExecutorService executor = defaultInstance.getExecutorService();
        for (String key : map.keySet()) {
            executor.execute(new LocalTask(key));
        }
    }
}
\end{lstlisting}
In the master node we first create some dummy data in a map. And then we are executing a partition task on every key in that map. 

After you run the MasterNode, you will get the following output:
\begin{verbatim}
key is local:true
key is local:true
key is local:true
key is local:true
key is local:true
\end{verbatim}
As you can see, the tasks are now executed on the same node as where the data is living. So with Hazelcast it is quite easy to group data and functions operating on this data. 

Another nice experiment is to let the PartitionedTask.getPartitionKey method return null. This will cause the task to be executed on a random node and therefor there will not be a locality of reference.

TODO: Tell about cluster change between task creation and task execution.

TODO: Tell about the PartitionService.getPartition(Object o)

\subsection{Executing on all or subset of members}
In some cases you want to execute a task on multiple machines; in my experience it will mostly be on all machines. You need to use this task wisely since it will cause load on multiple machines (potentially all machines).

We are going to create an example where there are a bunch of nodes. And on this nodes there is a shared map containing 100 entries. Each entry will have a key 1..100 and the value will always be null. To show the functionality of executing on all members, we are going to do a distributed count operation.

Lets begin with the count task:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.io.Serializable;
import java.util.concurrent.Callable;
public class CountTask implements Callable<Integer>, Serializable, HazelcastInstanceAware {
    private transient HazelcastInstance hazelcastInstance;
    @Override
    public void setHazelcastInstance(HazelcastInstance hazelcastInstance) {
        this.hazelcastInstance = hazelcastInstance;
    }
    @Override
    public Integer call() throws Exception {
        IMap<String, Integer> map = hazelcastInstance.getMap("map");
        int result = 0;
        for (String key : map.localKeySet()) {
            System.out.println("Calculating for key: " + key);
            result += map.get(key);
        }
        System.out.println("Local Result: " + result);
        return result;
    }
}

\end{lstlisting}
When this CountTask is called it retrieved the map and then it iterates over all keys that are local within the node and adds the mapped value and after it has finished iterating it returns the result.

The next thing are the slave node's:
\begin{lstlisting}[language=java]
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
public class SlaveNode {
    public static void main(String[] args){
        Hazelcast.getDefaultInstance();
    }
}
\end{lstlisting}
Lets starts 2 of these slaves and keep them running.

And the last part is the master node. This node will be responsible for inserting the test data, and doing the count operation:

\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.util.*;
import java.util.concurrent.*;
public class MasterNode {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hazelcastInstance = Hazelcast.getDefaultInstance();
        insertDummyData(hazelcastInstance);
        System.out.println("Result: " + count(hazelcastInstance));
    }
    private static int count(HazelcastInstance hazelcastInstance) throws Exception {
        //fork the tasks
        Set<Member> members = hazelcastInstance.getCluster().getMembers();
        MultiTask<Integer> task = new MultiTask<Integer>(new CountTask(), members);
        ExecutorService executorService = hazelcastInstance.getExecutorService();
        executorService.execute(task);

        //join the results.
        Collection<Integer> results = task.get();
        int x = 0;
        for (Integer i : results) x += i;
        return x;
    }
    private static void insertDummyData(HazelcastInstance hazelcastInstance) {
        Map map = hazelcastInstance.getMap("map");
        for (int k = 0; k < 15; k++) {
            map.put(""+k, 1);
        }
    }
}
\end{lstlisting}
The most important part here is the count method. First we are going to retrieve all members in the custer using "hazelcastInstance.getCluster().getMembers()". Then we are going to create a Hazelcast MultiTask; which receives our CountTask (the operation to be executed on a member) and a set of members to execute this count task on (so the task will be forked to all members). After that we call we join on task.get() and aggregate the results.

When you execute this, you will see something like this for slave node 1
\begin{verbatim}
Calculating for key: 11
Calculating for key: 0
Calculating for key: 4
Calculating for key: 5
Local Result: 4
\end{verbatim}

And slavenode 2
\begin{verbatim}
Calculating for key: 10
Calculating for key: 1
Calculating for key: 13
Calculating for key: 3
Local Result: 4
\end{verbatim}

And masternode:
\begin{verbatim}
Calculating for key: 12
Calculating for key: 8
Calculating for key: 6
Calculating for key: 2
Calculating for key: 7
Calculating for key: 14
Calculating for key: 9
Local Result: 7
Result: 15
\end{verbatim}

As you can see the load is 'equally' spread among all the members. 

TODO: Map/Reduce

TODO: Tell about cluster change between task creation and task execution.

\section{What is next}
In this chapter we explored the distributed execution of tasks using the Hazelcast ExecutorService. When you have worked with the ExecutorService before, you know that there are some gotcha's that can cause spending a lot of time on small things that are easy to solve. Hazelcast also introduces some gotcha's that are important to know up front.

\emph{Executors doesn't log exceptions:} when a task fails with an exception (or an error), this exception will not be logged by Hazelcast. This is in line with the ThreadPoolExecutorService from Java and it can be really annoying when you are spending a lot of time on why something doesn't work. It can easily be fixed; either add a try/catch in your runnable and log the exception. Or wrap the runnable/callable in a proxy that does the logging; the last option will keep your code a bit cleaner. 

\emph{Lite Members also execute tasks:} when you create a lite member, this member will also execute tasks and this can be really unexpected. When a task is executed on a lite member, it can access all the hazelcast datastructures without a problem since they will be accessible on all members; but it could be that either it hasn't the capabilities for processing or that it is undesired behavior. If this really in is issue, you could add some queue in front of the executor where each node actively moves work from the queue into the executor.

\emph{No processing guarantee:} although Hazelcast will make sure that the workqueue provides failover, as soon as a task is taken from the workqueue, no guarantees will be given that the task is processed. It could be that the machine that took the task crashed, it could also be that the task itself fails. 

\emph{No workqueue capacity control:} with the normal ThreadPoolExecutor you can limit the amount of unprocessed work on the workqueue and you also can control what happens with surplus of work. With the Hazelcast Distributed Executor there is no control. So it could be that you swamp a system with unprocessed work. And this can lead to an java.lang.OutOfMemoryError.

TODO: Finding back context

