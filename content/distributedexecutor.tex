\chapter{Distributed Executor Service}
Java 5 was perhaps the most fundamental upgrade since Java was released. On a language level we got generics, static imports, enumerations, varargs, enhanced for loop and annotations. Although less known, Java 5 also got fundamental fixes for the Java Memory Model (JSR-133) and last but certainly not least; we got a whole new concurrency library (JSR-166) found in java.util.concurrent.

The concurrency library contains a lot of goodies; some parts you probably don't use on a regular based, but other parts you perhaps do. One of the features that was added is the java.util.concurrent.Executor. The idea is that you wrap functionality in a Runnable (if you don't need to return a value) or a in a Callable (if you need to return a value) and hand it over to the Executor. In case of the ThreadPoolExecutor, these tasks are stored in a queue (the work queue) and there is a pool of worker threads that take work from this queue to process it.

A very basic example of the executor:
\begin{lstlisting}[language=java]
class EchoTest{
   private final ExecutorService = Executors.newSingleThreadExecutor();
   public void echoAsynchronously(final String msg){
      executor.execute(new Runnable(){
         public void run() { System.out.println(msg); }
      });	
   }
}
\end{lstlisting}
So while a worker thread is processing the task, the thread that submitted the task is free to work asynchronously. Although when you make use of the submit method in combination with a Future, the submitting thread can synchronize on completion of the worker thread, example:
\begin{lstlisting}[language=java]
class EchoTest{
   private final ExecutorService = Executors.newSingleThreadExecutor();
   public void echoAndWait(final String msg) throws Exception{
      Futurue future = executor.submit(new Runnable(){
         public void run() { System.out.println(msg); }
      });	
      //wait for completion of the echo operation
	  future.get();
   }
}
\end{lstlisting}
There is virtually no limit in what you can do in a task; you could execute complex database operations, intensive cpu or IO operations, render images etc. The problem in a distributed system however is that the default implementation of the Executor, the ThreadPoolExecutor, is designed to run within a single JVM. So the thread responsible for producing the task will be running in the same JVM as the worker thread that is processing that task. In a distributed system you want that a task submitted in one JVM, can be processed in another. In some cases you don't care which JVM processes it, in other cases you want to have it run one (or more) specific JVM.

Luckily Hazelcast makes this possible by providing an ExecutorService implementation that is designed to be used in a distributed environment; tasks will be send to one or more members depending on the configuration used. Lets start with a very simple example of ExecutorService in Hazelcast. We begin with a basic task that will do some waiting and echo a message:
\begin{lstlisting}[language=java]
import java.io.Serializable;
public class EchoTask implements Runnable, Serializable {
    private final String msg;
    public EchoTask(String msg) {
        this.msg = msg;
    }
    public void run() {
        try { Thread.sleep(5000);} 
        catch (InterruptedException e) {}
        System.out.println("echo:" + msg);
    }
}
\end{lstlisting}
This EchoTask implements the Runnable interface so that it can be submitted to the Executor. But it also implements the Serializable interface because it could be that it send to a different JVM.

The next part is the MasterMember that is responsible for submitting (and executing) 1000 echo messages:
\begin{lstlisting}[language=java]
import com.hazelcast.core.Hazelcast;
import java.util.concurrent.Executor;
public class MasterMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        ExecutorService executor = hzInstance.getExecutorService("executor");
        for (int k = 1; k <= 1000; k++) {
            Thread.sleep(1000);
            System.out.println("Producing echo task: " + k);
            executor.execute(new EchoTask("" + k));
        }
        System.out.println("EchoTaskMain finished!");
    }
}
\end{lstlisting}
First we retrieve the executor from the HazelcastInstance and then we slowly submit 1000 echo tasks. By default Hazelcast configures the executor to have 40 threads in the pool per member. For our example we only want a single thread per executor per member, so we configure it in the hazelcast.xml like this:
\begin{lstlisting}[language=xml]
<hazelcast>
    <executor-service name="executor">
        <max-pool-size>1</max-pool-size>
        <core-pool-size>1</core-pool-size>
    </executor-service>
</hazelcast>
\end{lstlisting}

When we run the master member you will get output like this:
\begin{lstlisting}
Producing echo task: 1
Producing echo task: 2
Producing echo task: 3
Producing echo task: 4
Producing echo task: 5
echo:1
Producing echo task: 6
Producing echo task: 7
Producing echo task: 8
Producing echo task: 9
Producing echo task: 10
echo:2
Producing echo task: 11
Producing echo task: 12	
\end{lstlisting}
As you can see the production of messages is 1/second and the processing is 0.2/second (each echo task sleeps 5 seconds), this means that we produce work 5 times faster than we are able to process it. There are a few ways to speed things up (apart from making the echo task faster of course):
\begin{enumerate}
\item scale up 
\item scale out
\end{enumerate}
Both of the strategies are explained below and in practice they are often combined. 

\emph{Task Serialization.} You need be aware that you need to think with a different hat than you do with the traditional ThreadPoolExecutor because the JVM where the tasks is executed potentially is different one than the JVM that submitted the task. This means that you can't pass complex dependencies like a database connection pool, through fields of that task. The solution to this problem is that you need to add enough 'context' so that when a task is executed it is able to reconstruct its execution context. A singleton can be a help in this case.

\section{Scaling up}
Scaling up, also called vertical scaling, is done by increasing the processing capacity on a single JVM. In this case we can launch more threads by setting the corePoolSize/maxPoolSize of the Executor. Since each thread can process 0.2 messages/second and we get 1 message/second, with 5 threads we can process messages as fast as they are produced.

When you scale up you need to look carefully at the JVM if it can handle the additional load. If not; you need to increase its resources (either cpu, disk, memory etc); if you fail to do so, the performance could go down instead of up. 

Scaling up the ExecutorService in Hazelcast is very simple, just increment the maxPoolSize and corePoolSize. Since we know that having 5 threads is going to give maximum performance, lets set them to 5.
\begin{lstlisting}[language=xml]
<hazelcast>
    <network>
        <join><multicast enabled="true"/></join>
    </network>
    <executor-service name="executor">
        <max-pool-size>5</max-pool-size>
        <core-pool-size>5</core-pool-size>
    </executor-service>
</hazelcast>
\end{lstlisting}
When we run the master node we'll see something like this:
\begin{lstlisting}
Producing echo task: 1
Producing echo task: 2
Producing echo task: 3
Producing echo task: 4
Producing echo task: 5
echo:1
Producing echo task: 6
echo:2
Producing echo task: 7
echo:3
Producing echo task: 8
echo:4
Producing echo task: 9
echo:5
Producing echo task: 10
echo:6
Producing echo task: 11	
\end{lstlisting}
As you can see, the tasks are being processed as quickly as they are being produced. So we now reached maximum scalability.

If you take a closer look at the previous hazelcast configuration, you saw that we configured the max-pool-size and the core-pool-size to be 5. The core-pool-size is the minimal size of the thread pool, even if there is nothing to do. But in some cases you want to have a thread pool with a dynamic size since threads (even if they are not running) consume memory. So if you want a pool that increases size when there is a lot of work to do and decreases size when there is not much work to do, you can configure the max-pool-size to be bigger than the core-pool-size. Example:
\begin{lstlisting}[language=xml]
<hazelcast>
     ...  
    <executor-service name="executor">
        <core-pool-size>1</core-pool-size> 
        <max-pool-size>5</max-pool-size>
        <keep-alive-seconds>60</keep-alive-seconds>
    </executor-service>
</hazelcast>
\end{lstlisting}
With this configuration the thread pool of the executor is configured with a minimum size of 1 thread and a maximum size of 5 threads. You can also see that we configured the keep-alive-seconds; this is the number of seconds an idle thread is allowed to stay alive before it is destroyed. The default keep-alive-seconds is 300 (5 minutes), so if you are fine with that you don't need to configure it.

In this case scaling up was very simple since the amount of resources (cpu, memory, IO bandwidth, disk-space etc) is more than enough. But often one or more of these resources will be the limiting factor. Of course we can always add more of these resources, but there will be a point that adding resources will not be cost efficient since they will cost more and more but provide less increments in capacity.

\section{Scaling out}
Scaling out, also called horizontal scaling, is orthogonal to scaling up; instead of increasing the capacity of the system by increasing the capacity of a single machine, we add more machines. In our case we can safely start up multiple Hazelcast members on the same machine since processing the task doesn't consume resources (it just waits a bit). But in real system you probably want to add more machines (physical or virtualized) to the cluster.

To scale up our echo example, we can add the following very basic slave member:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class SlaveMember {
    public static void main(String[] args) {
        Hazelcast.newHazelcastInstance(null);
    }
}
\end{lstlisting}
We don't need to do anything else because this member will automatically start the executor that was started in the master node.

When you start one master and slave member, you will see that the slave member is processing tasks as well:
\begin{lstlisting}
echo:31
echo:33
echo:35	
\end{lstlisting}
So in only a few lines of code, we are now able to scale out! If you want, you can start more slave members, but with tasks being created at 1 messages/second, adding more than 4 slave members (which make a total of 5 members running the executor) we have reached maximum scalability. 

\section{Executors and HazelcastInstanceAware}
In a lot of cases when you create a task, the task needs to get access to the HazelcastInstance to retrieve all kinds of data structures. This can be done very easily by letting the Runnable/Callable implement the HazelcastInstanceAware interface, e.g.

\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.io.Serializable;
public class SomeTask implements
        Runnable, Serializable, HazelcastInstanceAware {
    private transient HazelcastInstance hzInstance;
    public void setHazelcastInstance(HazelcastInstance hzInstance) {
        this.hzInstance = hzInstance;
    }
    public void run() {
        ....
    }
}
\end{lstlisting}
When this Task is processed (not when it is submitted), the Hazelcast executor checks if the task implements HazelcastInstanceAware. If so, it will inject the HazelcastInstance of the executor, by calling the 'setHazelcastInstance' method. The hzInstance field can be transient since it will never be send over the line. 

\section{Routing}
Till so far we didn't care about which member did the actual processing of the task; as long as any member picked it up. But in some cases you want to have control on which member is going to execute. Luckily Hazelcast provides different ways to route messages:
\begin{enumerate}
\item executing on any member. This is the default configuration.
\item executing on a specific member
\item executing on the member hosting a specific partition
\item executing on all or subset of the members.
\end{enumerate}
In the previous section we already covered routing to any member. In the following sections I'll explain the last 3 routing strategies.

\subsection{Executing on a specific cluster member}
In some cases you want to execute a task on a specific member. In Hazelcast this can be done by wrapping the task in a org.hazelcast.core.DistributedTask and provide the member.

As an example we are going to start a some members. The master member is going to echo a message to each member (including itself). This is done by retrieving all members using the Cluster object and iterating over it:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.util.Set;
import java.util.concurrent.ExecutorService;
public class MasterMember {
    public static void main(String[] args){
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        ExecutorService executorService = hzInstance.getExecutorService();
        Set<Member> members = hzInstance.getCluster().getMembers();
        int k=0;
        for(Member member: members){
            EchoTask task = new EchoTask("echo-"+k);
            DistributedTask distributedTask = new DistributedTask(task, member);
            executorService.execute(distributedTask);
            k++;
        }
    }
}
\end{lstlisting}
When we start one slave and master member, we'll get output like:

slave member:
\begin{lstlisting}
echo-0
\end{lstlisting}
master member:
\begin{lstlisting}
echo-1
\end{lstlisting}
So as you can see, it is very easy to execute a task on a specific member.

TODO: What happens when the member goes down after the task has been submitted, but before it is executed. No other member is allowed to pick up that task since it was made specifically for that member. Is there some callback mechanism?

\subsection{Executing on a member hosting a specific partition}
Often you want to have locality of reference; so the data being used for a specific process should be close to the process. Locality of reference is important for a scalable system since the amount of network communication (which is also a shared resource) will be reduced.

In hazelcast this can be done by sending a task to the member that owns the data in a specific partition. This has the advantage that you don't need to go over the network to retrieve data, as long as it all is in the same partition. When you start writing a distributed system, perhaps the most fundamental step to do right in the beginning is to get the partitioning schema right.

In Hazelcast sending a task to member that owns the partition can be done in 2 ways; either let the task implement com.hazelcast.core.PartitionAware interface or make use of the com.hazelcast.core.DistributedTask or let the task implement the PartitionAware interface.

As an example we are going to create a clustered system where there is some dummy data in a map and for every key in that map we are going to execute a verify task. This task will verify if it has been executed on the same member as where that key is residing.

Lets begin with the task:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.io.Serializable;
public class VerifyTask implements
        Runnable, PartitionAware, Serializable, HazelcastInstanceAware {
    private final String key;
    private transient HazelcastInstance hzInstance;
    public VerifyTask(String key) {
        this.key = key;
    }
    public void setHazelcastInstance(HazelcastInstance hzInstance) {
        this.hzInstance = hzInstance;
    }
    public void run() {
        IMap map = hzInstance.getMap("map");
        boolean localKey = map.localKeySet().contains(key);
        System.out.println("Key is local:" + localKey);
    }
    public Object getPartitionKey() {
        return key;
    }
}
\end{lstlisting}
The VerifyTask is implementing various interfaces. The most important one in this case is the PartitionAware interface that exposes the 'getPartitionKey' method. Adding this interface to a task indicates to Hazelcast that this task should be send to the member hosting the partition the partitionKey points to. When the partitionKey is null, Hazelcast is free to select a member.

If you look closer to the run method, you will see it accessed the map and checks if the key is hosted locally and prints the result.

The master member will be first creating a map with 10 entries (we only care about the key). And then will iterate over the keys in the map and send a VerifyTask for each key. 
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.util.*;
import java.util.concurrent.ExecutorService;
public class MasterMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        Map<String, String> map = hzInstance.getMap("map");
        for (int k = 0; k < 10; k++) 
            map.put(UUID.randomUUID().toString(), "");
        ExecutorService executor = hzInstance.getExecutorService();
        for (String key : map.keySet()) 
            executor.execute(new VerifyTask(key));
     }
}
\end{lstlisting}
First we start a few slave members and then we the master member and you will get output like:
\begin{lstlisting}
key is local:true
key is local:true
key is local:true
...
\end{lstlisting}
As you can see, the tasks are executed on the same member as where the data is living. So with Hazelcast it is very easy to group data and functions operating on this data. This  and this is extremely important for since it provides locality of reference and this allows us to create very efficient systems because you don't need to go over the network that much.

A nice experiment is to let the VerifyTask getPartitionKey method return null. This will cause the task to be executed on a random member and therefor there will not be a locality of reference.

TODO: Tell about cluster change between task creation and task execution.
TODO: Tell about the PartitionService.getPartition(Object o)

\subsection{Executing on all or subset of members}
In some cases you want to execute a task on multiple member (often it will all members). You need to use this functionality wisely since it will cause load on multiple members (potentially all members). In Hazelcast sending a task to one or more members can be done using the com.hazelcast.core.MultiTask.

We are going to create an example where there are a set of members. And on these members there is a distributed map containing 100 entries. Each entry will have a key 1..100 and the value will always be 1. To show the functionality of executing on all members, we are going to do a distributed sum operation.

Lets begin with the sum task:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.io.Serializable;
import java.util.concurrent.Callable;
public class SumTask implements
        Callable<Integer>, Serializable, HazelcastInstanceAware {
    private transient HazelcastInstance hzInstance;
    public void setHazelcastInstance(HazelcastInstance hzInstance) {
        this.hzInstance = hzInstance;
    }
    public Integer call() throws Exception {
        IMap<String, Integer> map = hzInstance.getMap("map");
        int result = 0;
        for (String key : map.localKeySet()) {
            System.out.println("Calculating for key: " + key);
            result += map.get(key);
        }
        System.out.println("Local Result: " + result);
        return result;
    }
}
\end{lstlisting}
When this SumTask is called it retrieves the map and then it iterates over all local keys sums the mapped value and after it has finished iterating it returns the result.

The master member will first create the map of 100 entries. Then it will create a MultiTask containing the SumTask and a set of all members. This MultiTask will be submitted to the executor where Hazelcast takes care of sending (fork) it to all members. After that the result can be retrieved (join) using the MultiTask.get operator which returns a Collection of Integers (the sum result for every member). The only thing that remains is to add all the integers in the set and to print it:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.util.*;
import java.util.concurrent.ExecutorService;
public class MasterMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        Map map = hzInstance.getMap("map");
        //insert dummy data
        for (int k = 0; k < 15; k++)
            map.put(UUID.randomUUID().toString(), null);
        //fork the tasks.
        Set<Member> members = hzInstance.getCluster().getMembers();
        MultiTask<Integer> task = new MultiTask<Integer>(new SumTask(), members);
        ExecutorService executorService = hzInstance.getExecutorService();
        executorService.execute(task);
        //join the results.
        Collection<Integer> results = task.get();
        int x = 0;
        for (Integer i : results) x += i;
        System.out.println("Result: " + x);
    }
}
\end{lstlisting}
When we start 2 slave members and then a master member, we'll see something like this, for slave member 1:
\begin{lstlisting}
Calculating for key: 11
Calculating for key: 0
Calculating for key: 4
Calculating for key: 5
Local Result: 4
\end{lstlisting}
And slave member 2
\begin{lstlisting}
Calculating for key: 10
Calculating for key: 1
Calculating for key: 13
Calculating for key: 3
Local Result: 4
\end{lstlisting}
And master member:
\begin{lstlisting}
Calculating for key: 12
Calculating for key: 8
Calculating for key: 6
Calculating for key: 2
Calculating for key: 7
Calculating for key: 14
Calculating for key: 9
Local Result: 7
Result: 15
\end{lstlisting}
As you can see the load is 'equally' spread among all the members. 

In this example we executed a task on all members, but if you only want to execute a task on a subset of members, you can apply some filtering on the members. 

TODO: Map/Reduce

TODO: Tell about cluster change between task creation and task execution.

\section{Futures}
The java Executor interface only exposes a single 'void execute(Runnable)' method that can be called to have a task asynchronously executed. But in some cases you want to synchronize on task completion or cancel the task (for example when it takes too much time to run). This can be done making use of the java.util.concurrent.Future and one of the submit methods of the ExecutorService (the ExecutorService extends the Executor). TODO: When working with a callable, you probably want to use a future.

In the first example we are going to wait for the result on a future and in the second example we are going to cancel a future:
\begin{lstlisting}[language=java]
import java.io.Serializable;
import java.util.concurrent.Callable;
public class FibonacciCallable implements Callable<Long>, Serializable {
    private final int input;
    public FibonacciCallable(int input) {
        this.input = input;
    }
    public Long call() {
        return calculate(input);
    }
    private long calculate(int n) {
        if (Thread.currentThread().isInterrupted()) {
            System.out.println("FibonacciCallable is interrupted");
            throw new RuntimeException("Fibonacci is interrupted");
        }
        if (n <= 1) return n;
        else return calculate(n - 1) + calculate(n - 2);
    }
}
\end{lstlisting}
As you can see there is a check 'Thread.currentThread().isInterrupted()' that is run frequently while calculating. When the thread is interrupted, the calculation will be aborted. In our case we are going to throw an exception to abort the calculation. 

The future can be used like this:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.util.concurrent.*;
public class MasterMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        ExecutorService executorService = hzInstance.getExecutorService();
        int n = Integer.parseInt(args[0]);
        Future<Long> future = executorService.submit(new FibonacciCallable(n));
        try {
            long result = future.get(10, TimeUnit.SECONDS);
            System.out.println("result: "+result);
        } catch (TimeoutException ex) {
            System.out.println("A timeout happened, the future is cancelled");
            future.cancel(true);
        }
        System.out.println("Finished");
    }
}
\end{lstlisting}
As you can see when we call the executorService.submit, we get back a Future as result. This future allows us to synchronize on completion or cancel the computation. 

When we can run this application with 5 as argument, the output will be: 
\begin{lstlisting}
result: 5
Finished
\end{lstlisting}
But when you run this application with 500 as argument, it will take more than 10 seconds to complete and therefor the future.get will timeout. The output will be:
\begin{lstlisting}
A timeout happened, the future is cancelled
FibonacciCallable is interrupted
Finished
\end{lstlisting}
This is because the future.get operation will timeout (if it doesn't instead of using 10 seconds as timeout, use a lower value; it could be that you have a very fast machine). When the timeout happens, a TimeoutException is thrown and we cancel the future. This cancel operation on the future is send back to the thread that is potentially running on a different member and will set the interrupted status of that Thread. It is up to the task (runnable/callable) that is being run by executor to check the interrupted status of the thread periodically to be able to be responsive to interrupts.

\section{Execution Callback}
With a future it is possible to synchronize on task completion. In some cases you want to wait on tasks completion in the same thread that submitted the task. But in other you want to have some logic executed when the task completes, but you don't want to block the submitting thread. Hazelcast provides a solution for this using the execution callback.

In the futures topic we created an example where we calculated fibonacci and wait on the completion of that calculation using the future.get() method. In the following example we are also going to calculate Fibonacci, but instead of waiting for that task to complete, we register an execution callback where we print the result asynchronously. We do this by wrapping our Fibonacci task in a com.hazelcast.core.DistributedTask and register an ExecutionCallback, that contains the printing logic, on this DistributedTask like this:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.util.concurrent.*;
public class MasterMember {
    public static void main(String[] args){
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance(null);
        ExecutorService es = hzInstance.getExecutorService();
        DistributedTask<Long> task = new DistributedTask<Long>(new FibonacciCallable(10));
        ExecutionCallback<Long> executionCallback = new ExecutionCallback<Long>() {
            public void done(Future<Long> future) {
                try {
                    if (!future.isCancelled()) 
                        System.out.println("Fibonacci calculation result = " + future.get());
                } catch (Exception e) {
                    e.printStackTrace();
                }
            }
        };
        task.setExecutionCallback(executionCallback);
        es.execute(task);
        System.out.println("Finished");
    }
}
\end{lstlisting}
As you can see, the ExecutionCallback has a single method 'done' that needs to be implemented and receives future .... As soon as the task has completed (either normally, exception or cancelled) the 'done' method will be called and the status of the task, including its result, can be retrieved using the future. In our case we are going to print the result.

If you run this example you will see the following output:
\begin{lstlisting}
Finished
Fibonacci calculation result = 55
\end{lstlisting}
As you can see, the thread that submitted the tasks to be executed, was not blocked. You can also see that eventually the result of the Fibonacci calculation will be printed. 

\section{What is next}
In this chapter we explored the distributed execution of tasks using the Hazelcast ExecutorService. When you have worked with the ExecutorService before, you know that there are some gotcha's that can cause spending a lot of time on small things that are easy to solve. Hazelcast also introduces some gotcha's that are important to know up front.

\emph{Executors doesn't log exceptions:} when a task fails with an exception (or an error), this exception will not be logged by Hazelcast. This is in line with the ThreadPoolExecutorService from Java and it can be really annoying when you are spending a lot of time on why something doesn't work. It can easily be fixed; either add a try/catch in your runnable and log the exception. Or wrap the runnable/callable in a proxy that does the logging; the last option will keep your code a bit cleaner. 

\emph{Lite Members also execute tasks:} when you create a lite member, this member will also execute tasks and this can be really unexpected. When a task is executed on a lite member, it can access all the Hazelcast data structures without a problem since they are accessible on all members. If this really is an issue, a native client could be a solution; see chapter 'Clients'.

\emph{No work queue capacity control:} with the normal ThreadPoolExecutor you can limit the amount of unprocessed work on the work queue and you also can control what happens with surplus of work. With the Hazelcast distributed Executor there is no control. So it could be that you swamp a system with unprocessed work. And this can lead to an java.lang.OutOfMemoryError.

\emph{No high availability for the workqueue}. Each member will create one or more local ThreadPoolExecutors with ordinary workqueues, that do the real work. When a task is submitted, it will be put on the workqueue of that ThreadPoolExecutor. If something would happen with that member, all unprocessed work will be lost. A project that might be interesting to investigate is: https://github.com/jclawson/hazelcast-work.

\emph{Destroying Executors}: todo: executors can't be destroyed by calling shutdown because it only disables the local proxy, and the ExecutorService returned by Hazelcast doesn't implement the Instance interface, so no destroy can be called.  

[TODO: Finding back context]
[TODO: Load balancer]
[TODO: Default Executor]