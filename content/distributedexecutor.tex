\chapter{Distributed Executor Service}
Java 5 was perhaps the most fundamental upgrade since Java was released. On a language level we got generics, static imports, enumerations, varargs, enhanced for loop and annotations. Although less known, Java 5 also got fundamental fixes for the Java Memory Model (JSR-133) and we got a whole new concurrency library (JSR-166) found in java.util.concurrent.

This library contains a lot of goodies; some parts you probably don't use on a regular based, but other parts you perhaps do. One of the features that was added is the java.util.concurrent.Executor. The idea is that you wrap functionality in a Runnable if you don't need to return a value, or in a Callable if you need to return a value, and submitting it to the Executor. A very basic example of the executor:
\begin{lstlisting}[language=java]
class EchoService{
   private final ExecutorService = 
      Executors.newSingleThreadExecutor();

   public void echoAsynchronously(final String msg){
      executor.execute(new Runnable(){
         public void run() { 
            System.out.println(msg); 
         }
      });	
   }
}
\end{lstlisting}
So while a worker thread is processing the task, the thread that submitted the task is free to work asynchronously. There is virtually no limit in what you can do in a task; complex database operations, intensive cpu or IO operations, render images etc. 

However the problem in a distributed system is that the default implementation of the Executor, the ThreadPoolExecutor, is designed to run within a single JVM. In a distributed system you want that a task submitted in one JVM, can be processed in another. Luckily Hazelcast provides the IExecutorService, that extends the java.util.concurrent.ExecutorService, but is designed to be used in a distributed environment. The IExecutorService is a new in Hazelcast 3.x.

Lets start with a very simple example of IExecutorService where a task is executed that does some waiting and echoes a message:
\begin{lstlisting}[language=java]
public class EchoTask implements Runnable, Serializable {
   private final String msg;
   public EchoTask(String msg) {
      this.msg = msg;}
   public void run() {
      try { 
         Thread.sleep(5000);
      } catch (InterruptedException e) {}
      System.out.println("echo:" + msg);
   }
}
\end{lstlisting}
This EchoTask implements the Runnable interface so that it can be submitted to the Executor. It also implements the Serializable interface because it could be that it is send to a different JVM to be processed. Instead of making the class Serializable, you could also rely on other serialization mechanisms; see [chapter serialization]. 

The next part is the MasterMember that is responsible for submitting (and executing) 1000 echo messages:
\begin{lstlisting}[language=java]
public class MasterMember {
   public static void main(String[] args) throws Exception {
      HazelcastInstance hz = Hazelcast.newHazelcastInstance();
      IExecutorService executor = hz.getExecutorService("exec");
      for (int k = 1; k <= 1000; k++) {
         Thread.sleep(1000);
         System.out.println("Producing echo task: " + k);
         executor.execute(new EchoTask("" + k));
      }
      System.out.println("EchoTaskMain finished!");
   }
}
\end{lstlisting}
First we retrieve the executor from the HazelcastInstance and then we slowly submit 1000 echo tasks. One of the differences between Hazelcast 2.x and 3.x is that the HazelcastInstance.getExecutorService() method has disappeared; you now always need to provide a name instead of relying on the default one. By default Hazelcast configures the executor with 8 threads in the pool. For our example we only need 1, so we configure it in the hazelcast.xml file like this:
\begin{lstlisting}[language=xml]
<executor-service name="exec">
   <pool-size>1</pool-size>
</executor-service>
\end{lstlisting}
Another difference is that the core-pool-size and keep-alive-seconds properties from Hazelcast 2.x have disappeared, so the pool will have a fixed size.

When the MasterMember is started you will get output like this:
\begin{lstlisting}
Producing echo task: 1
Producing echo task: 2
Producing echo task: 3
Producing echo task: 4
Producing echo task: 5
echo:1
....
\end{lstlisting}
As you can see the production of messages is 1/second and the processing is 0.2/second (the echo task sleeps 5 seconds), this means that we produce work 5 times faster than we are able to process it. Apart from making the EchoTask faster, there are 2 dimension for scaling:
\begin{enumerate}
\item scale up 
\item scale out
\end{enumerate}
Both are explained below and in practice they are often combined. 

\section{Scaling up}
Scaling up, also called vertical scaling, is done by increasing the processing capacity on a single JVM. Since each thread can process 0.2 messages/second and we produce 1 message/second, if the Executor would have 5 threads it can process messages as fast as they are produced.

When you scale up you need to look carefully at the JVM if it can handle the additional load. If not; you may need to increase its resources (either cpu, memory, disk etc). If you fail to do so the performance could degrade instead of improving. 

Scaling up the ExecutorService in Hazelcast is very simple, just increment the maxPoolSize. Since we know that 5 threads is going to give maximum performance, lets set them to 5.
\begin{lstlisting}[language=xml]
<executor-service name="exec">
   <pool-size>5</pool-size>
</executor-service>
\end{lstlisting}
When we run the MasterNode we'll see something like this:
\begin{lstlisting}
Producing echo task: 1
Producing echo task: 2
Producing echo task: 3
Producing echo task: 4
Producing echo task: 5
echo:1
Producing echo task: 6
echo:2
Producing echo task: 7
echo:3
Producing echo task: 8
echo:4
...
\end{lstlisting}
As you can see, the tasks are being processed as quickly as they are being produced. 

\section{Scaling out}
Scaling up was very simple since there is enough cpu and memory capacity. But often one or more of the resources will be the limiting factor. In practice increasing the computing capacity within a single machine will reach a point where it isn't cost efficient since the expenses go up quicker than the capacity improvements.

Scaling out, also called horizontal scaling, is orthogonal to scaling up; instead of increasing the capacity of the system by increasing the capacity of a single machine, we just add more machines. In our case we can safely start multiple Hazelcast members on the same machine since processing the task doesn't consume resources while the task waits. But in real systems you probably want to add more machines (physical or virtualized) to the cluster.

To scale up our echo example, we can add the following very basic slave member:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class SlaveMember {
   public static void main(String[] args) {
      Hazelcast.newHazelcastInstance();
   }
}
\end{lstlisting}
We don't need to do anything else because this member will automatically participate in the executor that was started in the master node and start processing tasks. 

If one master and slave are started, you will see that the slave member is processing tasks as well:
\begin{lstlisting}
echo:31
echo:33
echo:35	
\end{lstlisting}
So in only a few lines of code, we are now able to scale out! If you want, you can start more slave members, but with tasks being created at 1 task/second, maximum performance is reached with 4 slaves.

\section{Routing}
Till so far we didn't care about which member did the actual processing of the task; as long as a member picks it up. But in some cases you do want to have that control. Luckily the IExecutorService provides different ways to route tasks:
\begin{enumerate}
\item any member. This is the default configuration.
\item specific member
\item the member hosting a specific key
\item all or subset of members.
\end{enumerate}
In the previous section we already covered routing to any member. In the following sections I'll explain the last 3 routing strategies. This is also where a big difference is visible between Hazelcast 2.x and 3.x, while 2.x relied on the DistributedTask, 3.x relies on explicit routing methods on the IExecutorService. 

\subsection{Executing on a specific member}
In some cases you may want to execute a task on a specific member. As an example we are going to send an echo task to each member in the cluster. This is done by retrieving all members using the Cluster object and iterating over the cluster members. To each of the members we are going to send an echo message containing their own address. 
\begin{lstlisting}[language=java]
public class MasterMember {
   public static void main(String[] args) {
      HazelcastInstance hz = Hazelcast.newHazelcastInstance();
      IExecutorService executorService = hz.getExecutorService("ex");
      for (Member member : hz.getCluster().getMembers()) {
         EchoTask task = new EchoTask("echo" + 
            member.getInetSocketAddress());
         executorService.executeOnMember(task, member);
      }
   }
}
\end{lstlisting}
When we start a few slaves and a master, we'll get output like:
\begin{lstlisting}
Members [2] {
   Member [192.168.1.100]:5702 this
   Member [192.168.1.100]:5703
}
...
echo/192.168.1.100:5702
\end{lstlisting}
As you can see, the EchoTasks are executed on the correct member.

\subsection{Executing on key owner}
[Talip: again refer and encourage the use of IMap.executeOnKey API]
When an operation is executed in a distributed system, this operation often needs to access distributed resources. If these resources are hosted on a different member then where the task is running, scalability and performance may suffer due to remoting overhead. Luckily this problem can be solved by improving locality of reference.

In Hazelcast this can be done by placing the resources for a task in some partition and sending the task to the member that owns that partition. When you start designing a distributed system, perhaps the most fundamental step is designing the partitioning scheme.

As an example we are going to create a distributed system where there is some dummy data in a map and for every key in that map we are going to execute a verify task. This task will verify if it has been executed on the same member as where that partition for that key is residing:
\begin{lstlisting}[language=java]
public class VerifyTask implements
       Runnable, Serializable, HazelcastInstanceAware {
   private final String key;
   private transient HazelcastInstance hz;
   public VerifyTask(String key) { 
      this.key = key;
   }
   public void setHazelcastInstance(HazelcastInstance hz) {
      this.hz = hz;
   }
   public void run() {
      IMap map = hz.getMap("map");
      boolean localKey = map.localKeySet().contains(key);
      System.out.println("Key is local:" + localKey);
   }
}
\end{lstlisting}
If you look at the run method, you can see it accesses the map, retrieves all the keys that are owned by this member using the IMap.localKeySet() method, checks if the key is contained in that key set and prints the result. Another thing that is interesting is that this task implements HazelcastInstanceAware. This signals to Hazelcast that when this class is deserialized for execution, it will inject the HazelcastInstance executing that task. For more information see [reference to serialization/HazelcastInstanceAware section]

The next step is the MasterMember which first creates a map with some entries; we only care about the key so the value is bogus. And then it iterates over the keys in the map and send a VerifyTask for each key:
\begin{lstlisting}[language=java]
public class MasterMember {
   public static void main(String[] args) throws Exception {
      HazelcastInstance hz = Hazelcast.newHazelcastInstance();
      Map<String, String> map = hz.getMap("map");
      for (int k = 0; k < 10; k++)
         map.put(UUID.randomUUID().toString(), "");
      IExecutorService executor = hz.getExecutorService("exec");
      for (String key : map.keySet())
         executor.executeOnKeyOwner(new VerifyTask(key), key);
   }
}
\end{lstlisting}
As you can see we are now relying on the 'executeOnKeyOwner' to execute a task on the member owning a specific key. To verify the routing, we start a few slaves first and then start a master and we'll see output like this:
\begin{lstlisting}
key is local:true
key is local:true
...
\end{lstlisting}
As you can see, the tasks are executed on the same member as where the data is living.

An alternative way of executing a request on a specific member, that originates from Hazelcast 2.x, is to let the task implement the HazelcastPartitionAware interface and use the  'execute' or 'submit' method on the IExecutorService. The HazelcastPartitionAware exposes the 'getPartitionKey' method that is used by the executor to figure out the key of the partition to route to. If a null value is returned, any partition will do.

\subsection{Executing on all or subset of members}
In some cases you may want to execute a task on multiple or even on all members. You should use this functionality wisely since it will create load on multiple members, potentially all members, and therefore can reduce scalability.

In the following example there is a set of members and on these members there is a distributed map containing some entries. Each entry has some UUID as key and 1 as value. To demonstrate executing a task on all members, we are going to create a distributed sum operation that sums all values in the map:
\begin{lstlisting}[language=java]
public class SumTask implements
   Callable<Integer>, Serializable, HazelcastInstanceAware {
   private transient HazelcastInstance hz;
   public void setHazelcastInstance(HazelcastInstance hz) {
      this.hz = hz;}
   public Integer call() throws Exception {
      IMap<String, Integer> map = hz.getMap("map");
      int result = 0;
      for (String key : map.localKeySet()) {
         System.out.println("Calculating for key: " + key);
         result += map.get(key);
      }
      System.out.println("Local Result: " + result);
      return result;
   }
}
\end{lstlisting}
When this SumTask is called it retrieves the map and then iterates over all local keys, sums the values and returns the result.

The MasterMember will first create the map with some entries. Then it will submit the SumTask to each member and the result will be a map of Future instances. And finally we'll join all the futures and sum the result and print it:
\begin{lstlisting}[language=java]
public class MasterMember {
   public static void main(String[] args) throws Exception {
      HazelcastInstance hz = Hazelcast.newHazelcastInstance();
      Map<String,Integer> map = hz.getMap("map");
      for (int k = 0; k < 5; k++)
         map.put(UUID.randomUUID().toString(), 1);
      IExecutorService executor = hz.getExecutorService("exec");
      Map<Member,Future<Integer>> result = 
         executor.submitToAllMembers (new SumTask());
      int sum = 0;
      for(Future<Integer> future: result.values())
         sum+=future.get();
      System.out.println("Result: " + sum);
   }
}
\end{lstlisting}
When we start 1 slave and then a master member, we'll see something like this for the slave:
\begin{lstlisting}
Calculating for key: 6ed5fe89-b2f4-4644-95a3-19dffcc71a25
Calculating for key: 5c870a8c-e8d7-4a26-b17b-d94c71164f3f
Calculating for key: 024b1c5a-21d4-4f46-988b-67a567ae80c9
Local Result: 3
\end{lstlisting}
And master member:
\begin{lstlisting}
Calculating for key: 516bd5d3-8e47-48fb-8f87-bb647d7f3d1f
Calculating for key: 868b2f1e-e03d-4f1a-b5a8-47fb317f5a39
Local Result: 2
Result: 5
\end{lstlisting}
In this example we execute a task on all members, but if you only want to execute a task on a subset of members, you can call the submitToMembers method and pass the subset of members.

\emph{Not possible to send Runnable to every partition:} there is no direct support to send a runnable to every partition. If this is an issue, the SPI could be a solution since Operations can be routed to specific partitions. So you could build such an executor on top of the SPI.

\section{Futures}
The  Executor interface only exposes a single 'void execute(Runnable)' method that can be called to have a Runnable asynchronously executed. But in some cases you need to synchronize on results, e.g. when using a Callable or just want to wait till a task completes. This can be done making use of the java.util.concurrent.Future in combination with one of the submit methods of the IExecutorService.

To demonstrate the future, we are going to calculate a Fibonacci number by wrapping the calculation in a callable and synchronizing on the result:
\begin{lstlisting}[language=java]
public class FibonacciCallable 
   implements Callable<Long>, Serializable {
   private final int input;
   public FibonacciCallable(int input) {
      this.input = input;}
   public Long call() {return calculate(input);}
   private long calculate(int n) {
      if (n <= 1) return n;
      else return calculate(n - 1) + calculate(n - 2);
   }
}
\end{lstlisting}
The next step is submitting the task and using a Future to synchronize on results:
\begin{lstlisting}[language=java]
public class MasterMember {
   public static void main(String[] args) throws Exception {
      HazelcastInstance hz = Hazelcast.newHazelcastInstance();
      IExecutorService executor = hz.getExecutorService("exec");
      int n = Integer.parseInt(args[0]);
      Future<Long> future = 
         executor.submit(new FibonacciCallable(n));
      try {
         long result = future.get(10, TimeUnit.SECONDS);
         System.out.println("Result: "+result);
      } catch (TimeoutException ex) {
         System.out.println("A timeout happened");
      }
   }
}
\end{lstlisting}
As you can see when we call the executorService.submit(Callable) method, we get back a Future as result. This future allows us to synchronize on completion or cancel the computation. 

When we run this application with 5 as argument, the output will be: 
\begin{lstlisting}
Result: 5
\end{lstlisting}
when you run this application with 500 as argument, it will probably take more than 10 seconds to complete and therefore the future.get will timeout. When the timeout happens, a TimeoutException is thrown. If it doesn't on your machine, it could be that your machine is very quick and you need to use a smaller timeout. Unlike Hazelcast 2.x, unfortunately it isn't possible in Hazelcast 3.0 to cancel a future. [todo: add reason] One possible solution is to let the task periodically check if certain key in a distributed map exist. A task can then be cancelled by writing some value for that key. You need to take care of removing keys to prevent this map from growing; this could be done using the time to live setting.

\section{Execution Callback}
With a future it is possible to synchronize on task completion. In some cases you want to synchronize on the completion of the task before executing some logic, in the same thread that submitted the task. In some other cases you want this post completion logic to be executed asynchronously, so that the submitting thread doesn't block. Hazelcast provides a solution for this using the ExecutionCallback.

In the 'Future' topic an example is shown where a Fibonacci number is calculated and waiting on the completion of that operation is done using a Future. In the following example we are also going to calculate a Fibonacci number, but instead of waiting for that task to complete, we register an ExecutionCallback where we print the result asynchronously:
\begin{lstlisting}[language=java]
public class MasterMember {
   public static void main(String[] args){
      HazelcastInstance hz = Hazelcast.newHazelcastInstance();
      IExecutorService executor = hz.getExecutorService("exec");
      ExecutionCallback<Long> callback = 
           new ExecutionCallback<Long>() {
         public void onFailure(Throwable t) {
            t.printStackTrace();}
         public void onResponse(Long response) {
            System.out.println("Result: " + response);}
      };
      executor.submit(new FibonacciCallable(10), callback);
      System.out.println("Fibonacci task submitted");
    }
}
\end{lstlisting}
As you can see, the ExecutionCallback has 2 methods; one method that is called on a valid response and we print it. And the other method is called on failure and we print the stacktrace: 

If you run this example you will see the following output:
\begin{lstlisting}
Fibonacci task submitted
Result: 55
\end{lstlisting}
As you can see, the thread that submitted the tasks to be executed, was not blocked. You can also see that eventually the result of the Fibonacci calculation will be printed. 

\section{Good to know}

\emph{Work-queue has no high availability:} Each member will create one or more local ThreadPoolExecutors with ordinary work-queues that do the real work. When a task is submitted, it will be put on the work-queue of that ThreadPoolExecutor and will not be backed up by Hazelcast. If something would happen with that member, all unprocessed work will be lost. 

\emph{Work-queue is not partitioned:} Each member specific executor will have its own private work-queue, once an item is placed on that queue, it will not be taken by a different member. So it could be that one member has a lot of unprocessed work, and another is idle.

\emph{Work-queue by default has unbound capacity:} This can lead to OutOfMemoryErrors because the number of queued tasks can grow without being limited. This can be solved by setting the <queue-capacity> property on the executor-service.

\emph{No Load Balancing:} is currently available for tasks that can run on any member. In the future there probably is going to be a customizable load balancer interface where for example load balancing could be done on the number of unprocessed tasks, cpu load, memory load etc. If load-balancing is needed, it could be done by creating an IExecutorService proxy that wraps the one returned by Hazelcast. Using the members from the ClusterService or  member information from SPI:MembershipAwareService, it could route 'free' tasks to a specific member based on load.

\emph{Destroying Executors:} you need to be careful when shutting down an IExecutorService because it will shutdown all corresponding executor in every member and subsequent calls to proxy will result in a RejectedExecutionException. When the executor is destroyed and later a HazelcastInstance.getExecutorService is done with the id of the destroyed executor a new executor will be created as if the old one never existed.

\emph{Executors doesn't log exceptions:} when a task fails with an exception (or an error), this exception will not be logged by Hazelcast. This is in line with the ThreadPoolExecutorService from Java and it can be really annoying when you are spending a lot of time on why something doesn't work. It can easily be fixed; either add a try/catch in your runnable and log the exception. Or wrap the runnable/callable in a proxy that does the logging; the last option will keep your code a bit cleaner. 

\emph{HazelcastInstanceAware:} When a task is deserialized, in a lot of cases you need to get access to the HazelcastInstance. This can be done by letting the task implement HazelcastInstanceAware. For more information see [chapter Serialization:HazelcastInstanceAware]

\section{What is next}
In this chapter we explored the distributed execution of tasks using the Hazelcast ExecutorService.....