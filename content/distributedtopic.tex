\chapter{Distributed Topic}
In the 'Distributed Collections' chapter we talked about the IQueue, which can be used to create point to point message solutions. In such a solution each message will be processed by a single consumer. An alternative approach is the publish/subscribe mechanism, where a single message can be processed by an arbitrary number of subscribed consumers.

Hazelcast provides a publish/subscribe mechanism in the form of the com.hazelcast.core.ITopic. It is a distributed mechanism for publishing messages to multiple subscribers. Any number of members can publish messages to a topic and any number of members can receive messages from the topics they are subscribed to. The message can be an ordinary POJO, although it must be able to serialize [see serialization chapter] since it needs to go over the network.

I'll show you how the distributed topic works based on a very simple example; there is a single topic that is shared between a publisher and a subscriber. The publisher publishes the current date on the topic:
\begin{lstlisting}[language=java]
public class PublisherMember {
    public static void main(String[] args){
        HazelcastInstance hz = Hazelcast.newHazelcastInstance();
        ITopic<Date> topic = hz.getTopic("topic");
        topic.publish(new Date());
        System.out.println("Published");
        System.exit(0);
    }
}
\end{lstlisting}
And the subscriber acquires the same topic and adds a MessageListener to subscribe itself to the topic:
\begin{lstlisting}[language=java]
public class SubscribedMember {
    public static void main(String[] args){
        HazelcastInstance hz = Hazelcast.newHazelcastInstance();
        ITopic<Date> topic = hz.getTopic("topic");
        topic.addMessageListener(new MessageListenerImpl());
        System.out.println("Subscribed");
    }
    private static class MessageListenerImpl implements MessageListener<Date> {
        public void onMessage(Message<Date> m) {
            System.out.println("Received: "+m.getMessageObject());
        }
    }
}
\end{lstlisting}
We first start up the subscriber, and we'll see: "Subscribed" in the console. Then we start the publisher, and after it published the message "Published", the subscriber will output something like:
\begin{lstlisting}
Received: Sat Feb 15 13:05:24 EEST 2013
\end{lstlisting}
To make it a bit more interesting, you can start multiple subscribers. If you run the publisher again, all subscribers will receiving the same published message.

In the 'SubscribedMember' example we programmatically subscribe to a topic. But you can move this responsibility to the  Hazelcast configuration file:
\begin{lstlisting}[language=xml]
<topic name="topic">
    <message-listeners>
        <message-listener>MessageListenerImpl</message-listener>
    </message-listeners>
</topic>
\end{lstlisting}

\section{Scaling up the MessageListener}
Although the API for the ITopic has not changed with the introduction of Hazelcast 3, the threading model certainly has changed. With 2.x each member had a single 'event' ThreadPoolExecutor that executes the MessageListener(s). Each worker-thread will have its own work-queue and when a message is send, it is wrapped in a runnable and send to the correct work-queue using a hash based on the topic name and the member that published the message.

With Hazelcast 3 there is no executor anymore; there is only 1 queue and a single thread responsible for taking messages from that queue and calling the listeners. The consequence of this approach because of the single threaded nature, it will not scale. A solution for this problem 

This guarantees that messages published by some member on some topic are delivered in order because they are processed by the same thread. The ordering guarantee ensures that message listeners will receive the messages in the order they are published. If cluster member M publishes messages m1, m2, m3...mn to a topic T, then Hazelcast makes sure that all of the subscribers of topic T will receive m1, m2, m3...mn in order. 

In practice it could be you don't need this strict ordering and it could become a bottleneck. If message ordering isn't important you can offload the processing to an ordinary executor that is configured with multiple threads e.g.:
\begin{lstlisting}[language=java]
private final static Executor executor = Executors.newFixedThreadPool(10);       
private static class MessageListenerImpl implements MessageListener<Date> {
   public void onMessage(final Message<Date> m) {
      Runnable task = new Runnable() {
         public void run() {
            System.out.println("Received: " + m.getMessageObject());
         }
      };
      executor.execute(task);
   }
} 
\end{lstlisting}
This can greatly boost the processing capacity since messages from some member to some topic can now be processed in parallel.

\section{Good to know}

\emph{Not transactional}. The ITopic is not transactional, so be careful when it is used inside a transaction. If the transaction fails after a message is send or consumed and the transaction is rolled back, the message sending or consumption will not be rolled back.

\emph{No garbage collection} for the topics. So as long as the topics are not destroyed, they will be managed by Hazelcast and this can eventually lead to memory issues since the topics are stored in memory. There are a few ways to deal with this issue. One way is to send a message to the subscribers that they 

One way to solve it is by creating a garbage collection mechanism for the ITopic. Create a topic statistics IMap with the topic name as key and a pair containing the last processed message count and timestamp as value. When a topic is retrieved from the HazelcastInstance, an entry can be placed in the topic statics map if it is missing. Periodically iterate over all topics from the topics statics map e..g using the IMap.localKeySet(). Then retrieve the local statistics from the ITopic using the ITopic.getLocalTopicStats() method and check if there is a the number of processed messages has changed, using the information from the topic statistics map. If there is a difference, update the topic statics in the map; the new timestamp can be determined using the Cluster.getClusterTime() method. If there is no change, and period between the current timestamp and the last timestamp exceeds a certain threshold, the topic and the entry in the topic statistics map can be destroyed. It is important to understand that this solution isn't perfect, since is that it could happen that a message is send to a topic that has been destroyed. The topic will be recreated, but the subscribers are gone.

\emph{No durable subscriptions}. If a subscriber goes offline, it will not receives messages that were send while it was offline.

\emph{No metadata}. The message is an ordinary POJO and therefor doesn't contain any metadata like a timestamp or an address to reply to. Luckily this can be solved by wrapping the message in an envelop - a new POJO - and setting the metadata on that envelop.

\emph{Unexpected contention}. The executor that executes the MessageListeners uses a queue per worker thread, but it can happen that unrelated topic end up in the same queue. If the thread for that queue is busy with processing messages for other topics, message will not be processed even though there are processing threads available. Perhaps in the future the normal ThreadPoolExecutor will be replaced by the ForkJoinPool to solve this problem. [todo: needs to be updated]

\emph{No delivery guarantee:} When messages are send to a subscriber, they are put on a queue first and then the message will be taken from that queue to be processed. If items are on the queue and member fails, all messages on that queue will be lost.

\emph{Overflow:} When messages are send faster than they are consumed, it can lead to problems. The send will block until space becomes available [todo: is this true since an operation has a timeout]. 

\section{What is next}
In this chapter we have seen the ITopic. From a high level there is some overlap with JMS, but the provided functionality is limited. On the other side, the ITopic is extremely easy to use, scalable and doesn't require message brokers to be running.
