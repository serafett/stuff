\chapter{Hazelcast Clients}
Till so far the examples showed members that were full participants in the cluster; so they will known about others and they will host partitions. But in some cases you only want to connect to the cluster to read/write data or execute operations, but you don't want to participate as a full member in the cluster; in other words you want to have a client.

We are going to implement client example where a message will be put on a queue by a client and taken from the queue by a full member:
\begin{lstlisting}[language=java]
public class FullMember {
   public static void main(String[] args)throws Exception{
      HazelcastInstance hz = Hazelcast.newHazelcastInstance();
      BlockingQueue<String> queue = hz.getQueue("queue");
      for(;;) 
         System.out.println(queue.take());
   }
}
\end{lstlisting}

With the client one can connect to the cluster purely as a client and not have any of the responsibilities a normal cluster member has. When a Hazelcast operation is performed by a client, it is forwarded to a cluster member where it will be processed. A client only needs to have the hazelcast-client.jar on the classpath (+/- 150 kB depending on the version) [todo: verify], the normal Hazelcast jar is not needed. Underneath you can see the client example:
\begin{lstlisting}[language=java]
public class Client {
   public static void main(String[] args) throws Exception {
      ClientConfig cfg = new ClientConfig()
         .addAddress("127.0.0.1");
      HazelcastInstance client = HazelcastClient.newHazelcastClient(cfg);
      BlockingQueue<String> queue = client.getQueue("queue");
      queue.put("Hello");
      System.out.println("Message send by Client!");
   }
}
\end{lstlisting}
The client HazelcastInstance is created based on the com.hazelcast.client.ClientConfig. This config is configured with 127.0.0.1 as address since the full member will be running on the same machine as the client.

First start the full member and then start the client, and we can see that "Hello!" is printed by the full member. We'll also see that the client never appears as member in the member listing.	
 
\emph{Failover} for the client can be realized by explicitly configuring multiple member addresses in the ClientConfig to increase the chance that one of these members will be up and running. Once the client has connected to the cluster, it will keep up to date with all the member addresses of the cluster, not just the provided members.

\section{Configuration Options}
In the client example, we did a minimal configuration of the ClientConfig and relied on defaults, but there is a lot that can be configured:
\begin{enumerate}
\item addresses: the known addresses of the cluster. They don't need to include all addresses, only enough to make sure that some will always be online.
\item connection timeout: the amount of time in milliseconds the client waits for one of the members, configured with the addresses property, of the cluster to come online before giving up. Defaults to 5 minutes.
\item credentials: can be used to configure username/password to connect to the cluster. See UsernamePasswordCredentials. [todo: this works in combination with the interceptor?]
\item group config: configures the group name and the password to access the group.
\item connection timeout defaults to 30000
\item initial connection attempt limit:the  number of connection attempts the client makes the initial connection to the cluster. Defaults to 1.
\item reconnect attempt limit: the number of reconnection attempts the client makes when a connection fails. Defaults to 1.
\item reconnect timeout: [todo]. Defaults to 5000 ms.
\item update automatic:[TODO] Defaults to true.
\item socketInterceptor:.. see [security]
\item listeners: EventListener
\item router: see Routing for more information. Defaults to RoundRobinRouter.
\end{enumerate}
[todo: the client is still in progress, so properties might change]

\section{Routing}.
When a client connects to the cluster, it will have access to the full list of members and it will be kept in sync, even if the ClientConfig only has a subset of members. If an operation needs to be send to a specific member, it will directly be send to that member. If a operation can be executed on any member, Hazelcast does automatic load balancing over all members in the cluster. 

One of the very new cool features of Hazelcast 3.0 is that the load balancing mechanism is pulled out into an interface:
\begin{lstlisting}[language=java]
public interface Router {
   void init(HazelcastInstance hz);
   Member next();
}
\end{lstlisting}
This means that if you have specific load balancing requirements, e.g. load balance on cpu load, memory load, queue sizes etc, these requirements can be met by creating a custom Router implementation. I'm sure that in the next releases of Hazelcast some of these implementations will be provided out of the box. If you are going to implement a custom router, you can listen to member changes using:
\begin{lstlisting}[language=java]
Cluster cluster = hz.getCluster();
cluster.addMembershipListener(thelistener);
\end{lstlisting}
Router instances should not be shared between clients; every client should gets its own instance and the router can be configured from the ClientConfig. The clients are thread-safe, and therefor can be called concurrently, so you don't need to pool them.

\section{Security}
Often security is one of the requirements for production environments. 


[todo: security is fully client side; so a security risk since user developer doesn't need to apply security]
kerberos


LoginModuleConfig
PermissionConfig


\subsection{Authentication}

Authentication is done on first request only I hope.

\subsection{Authorization}

\subsection{Encryption}
In Hazelcast 3.0 it isn't possible to encrypt communication between client and cluster. This means that all network traffic, which not only includes normal operations like a map.put but also passwords in credentials and GroupConfig, can be read and potentially modified. If this is an issue, it is best not expose the client but manually expose the Hazelcast operations using a more secure remoting technology. 

\section{Group Configuration}
To prevent clients from joining a cluster, it is possible to configure the cluster group the client is able to connect to. On this cluster group the group name and the password can be set:
\begin{lstlisting}[language=java]
ClientConfig config = new ClientConfig()
    .addAddress("127.0.0.1");
config.getGroupConfig()
    .setName("group1")
    .setPassword("thepassword");
HazelcastInstance client = HazelcastClient.newHazelcastClient(config);
\end{lstlisting}
The groupname defaults to 'dev' and the password defaults to 'dev-pass'. For more information see [network configuration: cluster groups].


\section{Sharing classes}
In some cases you need to share classes between the client and the server. Of course you can give all the classes from the server to the client, but in some cases this is undesirable; perhaps it would increase tight coupling, security/copyrights issues, increased client size etc. If you don't want to share all the classes of the server with the client, create a separate project (in Maven terms this could be a module) and share this project between client and server. I normally call this project the api project since it clearly defines the API of the system. 

One advice; watch out with sharing domain objects between client and server because this can cause a tight coupling because the client starts to see the internals of your domain objects. A recommended practice is to introduce special objects that are optimized for client/server exchange; Data Transfer Objects (DTO's). They cause some duplication, but having some duplication is better to deal with than tight coupling, which can make a system very fragile.

\section{What happened to the lite member?}
If you have been using Hazelcast 2.x you might remember the lite member. The lite member participates as a cluster member, but isn't responsible for hosting any partitions. 

One advantage of a lite member compared to a native client, was that the latter didn't know about routing requests to the correct members and therefor was less performant. But with Hazelcast 3.x this limitation is removed and therefor the lite member doesn't serve a purpose anymore.

One disadvantage of the lite member is that because they are seen as member in the cluster and therefor send heartbeats/pings to each other and check each others statuses continuously. So if the number of lite members compared to the number of real members, is high, and clients join/leave frequently, it can influence the health of the cluster. 

Another disadvantage is that although a lite member doesn't host any partitions, it will run tasks from the Hazelcast executors. Personally I always found this undesirable since you don't want to have a client run server tasks.

\section{What is next}
In this short chapter we explained a few different ways to connect to a Hazelcast cluster using a client. But there are more client solutions available: like the C\# client, C++ client, Memcache Client and the Rest Client. For more information check the Client chapter of the Hazelcast reference manual.