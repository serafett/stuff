\chapter{Hazelcast Clients}

Till so far our examples showed members that were full participants in the cluster; so they will known about others and they will take their share in the load. But in some cases you only want to connect to the cluster to read/write data or execute operations, but you don't want to have a member of the cluster; in other words you want to have a client.

Hazelcast provides 2 client solutions:
\begin{enumerate}
\item native client
\item lite member
\end{enumerate}
These solutions will be explained in this chapter. 
\section{Native client}
With the native client one can connect to the cluster purely as a client and one will not become a member of the cluster. So since the native client is not a member of the cluster, it will not host any data or execute tasks from the distributed executor. When an operation is executed on the native client, this operation is forwarded to the cluster where this operation is processed.

To show you how the native client works, you first need to include the hazelcast-client.jar. It isn't needed to include the normal hazelcast jar. As an example we are going to going to create a very basic native client that puts a message on a queue which is going to be printed by a full member:
\begin{lstlisting}[language=java]
import com.hazelcast.client.*;
import com.hazelcast.core.HazelcastInstance;
import java.util.concurrent.BlockingQueue;
public class NativeClient {
    public static void main(String[] args) throws Exception {
        ClientConfig clientConfig = new ClientConfig();
        clientConfig.addAddress("127.0.0.1");
        HazelcastInstance client = HazelcastClient.newHazelcastClient(clientConfig);
        BlockingQueue<String> queue = client.getQueue("queue");
        queue.put("hello");
        System.out.println("Message send by lite member!");
    }
}
\end{lstlisting}
The client HazelcastInstance is created based on the com.hazelcast.client.ClientConfig. This config is configured with 127.0.0.1 as address since the full member will be running on the same machine as the client.

And we also have a full member:
\begin{lstlisting}[language=java]
import com.hazelcast.config.Config;
import com.hazelcast.core.Hazelcast;
import com.hazelcast.core.HazelcastInstance;
import java.util.concurrent.BlockingQueue;
public class FullMember {
    public static void main(String[] args)throws Exception{
        Config config = new Config();
        HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance(config);
        BlockingQueue<String> queue = hazelcastInstance.getQueue("queue");
        System.out.println("Received: "+queue.take());       
    }
}
\end{lstlisting}
If we first start the full member and then start the native client we can see that message is printed by the full member. We'll also see that the native client never appears as member in the member listing of the full member.	
 
\emph{Failover} is automatically provided by the native client by configuring multiple member addresses, if one member fails, the native client will automatically switch to another member.

There can be hundreds, even thousands of clients connected to the cluster. But by default there are 40 threads, configurable using the hazelcast.executor.client.thread.count property, on each member that will handle all the requests. 

In the native client example, we did a minimal configuration of the ClientConfig and relied on defaults, but there is a lot that can be configured:
\begin{enumerate}
\item addresses: the known addresses of the cluster. They don't need to include all addresses, only enough to make sure that some will always be online.
\item connection timeout: the amount of time in milliseconds the native client waits for one of the members, configured with the addresses property, of the cluster to come online before giving up.
\item credentials: can be used to configure username/password to connect to the cluster. See UsernamePasswordCredentials.
\item group config: configures the group the native client connects to. The name of the group and the password to access the group can be configured.
\item initial connection attempt limit:TODO
\item reconnect attempt limit:TODO
\item reconnect timeout:TODO
\item shuffle:TODO
\item update automatic:TODO
\end{enumerate}

\section{Lite Member}
Although the native client is a very simple mechanism to access the cluster, it has some drawbacks. The most important one is performance, since it knows nothing about the cluster and always needs to go through one of the configured members. That is why a another client solution is available: the lite member. In the example underneath you can see that we are also going to place a message on the queue just as with the native client:
\begin{lstlisting}[language=java]
import com.hazelcast.config.Config;
import com.hazelcast.core.*;
import java.util.concurrent.BlockingQueue;
public class LiteMember {
    public static void main(String[] args) throws Exception {
        Config config = new Config();
        config.setLiteMember(true);
        HazelcastInstance hazelcastInstance = Hazelcast.newHazelcastInstance(config);
        BlockingQueue<String> queue = hazelcastInstance.getQueue("queue");
        queue.put("echo");
        System.out.println("Message send from lite member!");
    }
}
\end{lstlisting}
As you can see there is not a big difference compared to creating a normal node. There only difference is the call 'setLiteMember(true)'. 

First we start the full member and then we start the lite member. In the logging of both of these members we can see that the members have found each other. In case of the lite member, we'll see something like this:
\begin{verbatim}
Members [2] {
    Member [10.37.129.2]:5701
    Member [10.37.129.2]:5702 this lite
}	
\end{verbatim}	
As you can see the cluster contains 2 members and the 'this' member shows that it is the lite member. If you look in logging of the full member, you will see that the message send by the lite member has been received by the full member.

\emph{same configuration:} A lite member, just like all other member, needs to have the same configuration, with the only difference is that the lite member field is set.

\emph{executors:} the lite member will execute tasks from the Hazelcast distributed executor. This is explained in TODO: reference to last chapter of the executors.

\section{Lite member vs Native client}
The lite member is a member of the cluster, it has a socket connection to all other members and knows where data is so it will be able to access it faster. On the downside there will be a lot more clustering overhead and "be on the same data center even on the same RAC". Native Clients can be anywhere in the LAN or WAN. It scales much better and overhead is quite less. So if your clients are less than Hazelcast nodes then LiteMember can be an option; otherwise definitely try Native Client. As a rule of thumb: Try Native client first, if it doesn't perform well enough for you, then consider LiteMember.

\section{What is next}
In this short chapter we explained the different ways to connect to a Hazelcast cluster using a client.