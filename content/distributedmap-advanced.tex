\chapter{Distributed Map Advanced}

TODO: This chapter will be completely rewritten.

Listing of subjects in this chapter
\begin{enumerate}
\item High Availability
\item Data Locality
\item Near Cache
\item Persistence
\item EntryListeners
\end{enumerate}

\section{High availability}
By default all data that is written will automatically be written synchronously to another JVM. This prevents data from getting lost if a JVM crashes, because it will be available on another JVM. By customizing the Hazelcast configuration file:
\begin{verbatim}
<hazelcast> 
<map name="customers"> 
<backup-count>1</backup-count>
 	</map> 
</hazelcast>
\end{verbatim}

This configuration specifies that the customers map should have a single backup. This is also the default, so if you are fine with that, you do not need to specify it. But if you 

This configuration 
Consistency
todo explain default
Explain asynchronous
Persistence

If the partitions in a map are backed up by another node, no data will be lost if a node fails. But sometimes you need more. And sometimes you need to integrate with existing system e.g. databases, to store and retrieve information from. If you only need to load information into the system, you can implement the com.hazelcast.core.MapLoader interface and if you also need to store you need to implement the com.hazelcast.core.MapStore interface (which extends the MapLoader interface).

We are going to provide a very basic in memory version of the MapStore interface:

\begin{verbatim}
public class CustomerStore implements MapStore<String,Customer>{
   private final Map<String,Customer> customers = new HashMap<String,Customer>();
   public CustomerStore(){
      store(new Customer("Homer"));
      store(new Customer("Bart"));
      store(new Customer("Marge"));
   }
   private void store(Customer customer){
      customers.put(custer.getId(),customer);
   }
   public synchronized Customer load(String key){
      return customers.get(key);
   }
   public synchronized Map<String,Customer>	loadAll(Collection<String> keys){
      return new HashMap<String,Customer>(customers);
   }
   public synchronized void delete(String key){
      customers.remove(key);
   } 
   public synchronized void deleteAll(Collection<String> keys){
      for(String key: keys) customers.remove(key);
   } 
   public synchronized void store(String key, Customer value){
      customers.put(key, value);
   } 
   public synchronized void storeAll(Map<K,V> map){
      customers.putAll(map);
   }   
}
\end{verbatim}

And it can be configured like this:

\begin{verbatim}
<hazelcast>
    ...
    <map name="customers">
        ...
        <map-store enabled="true">
           <class-name>com.hazelcast.examples.CustomerStore</class-name>
           <write-delay-seconds>0</write-delay-seconds>
        </map-store>
    </map>
</hazelcast>
\end{verbatim}

The write-delay-seconds defaults to 0 and this means that if a change is made in the map, a write through to the CustomerStore is going to happen. Although this example doesn't use a database, but if one would be used, the database would immediately be updated if the map is updated. If the write-delay-seconds is higher than zero, the entry will be marked as dirty and at some point in time it will be written to the database, this is called a write behind.


\section{Data locality}

In some cases you want data that logically belongs to each other, is stored on the same partition to have locality of reference (an important feature for performance). 

Imagine that a customer also can have orders, we could code it like this:

\begin{verbatim}
import java.io.Serializable;
import java.util.UUID;

public final class Order implements Serializable {
   private final String orderId = UUID.randomUUID().toString();
   private final String customerId;
   private final String articleId;
   private final int quantity;
   public Order(String customerId, String articleId, int quantity){
      this.customerId = customerId;
      this.articleId = articleId;
      this.quantity = quantity;
    }
    public String getOrderId(){return orderId;}
    public String getCustomerId(){return customerId;}
    public String getArticleId(){return articleId;}
    public int getQuantity(){return quantity;}
}

public class OrderService{
   private final Map<String,Order> orderMap = Hazelcast.getMap("orders");
   public void placeOrder(String customerId, String articleId, int count){
      Order order = new Order(customerId, articleId, count);
      orderMap.put(order.getOrderId(),order);
   }
}
\end{verbatim}

The problem is that the Orders are very likely to be stored on a different partition than the customers since the customer will be partitioned with the customer id, and the order with the order id. 

This can be solved by making use of the PartitionAware interface. If a key for the IMap implements this interface, instead of using the hashcode of the object to determine the correct partition, the PartitionAware.getPartitionKey() method will be called and the result of this method will be used to determine the right partition.

In our case we introduce an intermediate object: the OrderKey that implements this PartitionAware interface and returns the customerId as the partitionKey.

\begin{verbatim}
import com.hazelcast.core.PartitionAware;

import java.io.Serializable;

public final class OrderKey implements PartitionAware, Serializable {
   private final String orderId;
   private final String customerId;
   public OrderKey(String orderId, String customerId){
      this.orderId = orderId;
      this.customerId = customerId;
   }
   public Object getPartitionKey(){return customerId;}
   public int hashCode(){return orderId.hashCode();}
   public boolean equals(Object thatObject){
      if(thatObject == this)return true;
      if(!(thatObject instanceof OrderKey))return false;
      OrderKey that = (OrderKey)thatObject;
      return that.orderId.equals(this.orderId);
   }
}
\end{verbatim}

And we can integrate it in our code like this:

\begin{verbatim}
import com.hazelcast.core.Hazelcast;

import java.util.Map;

public class OrderService {
   private final Map<OrderKey, Order> orderMap = Hazelcast.getMap("orders");

   public String placeOrder(String customerId, String articleId, int quantity) {
      Order order = new Order(customerId, articleId, quantity);
      OrderKey key = new OrderKey(order.getOrderId(),customerId);
      orderMap.put(key,order);
      System.out.printf(
		"Order with id %s created: customer: %s, articleId %s, quantity %s\n",
         	order.getOrderId(),customerId,articleId,quantity);
      return order.getOrderId();
   }
}
\end{verbatim}

And we call it like this:

\begin{verbatim}
public class Main {
   public static void main(String[] args){
      CustomerService cs = new CustomerService();
      OrderService os = new OrderService();
      String customerId = cs.create("Homer");
      String orderId = ps.placeOrder(customerId,"123",1);
      PartitionService ps = Hazelcast.getPartitionService();
      Partition cp = ps.getPartition(customerId);
      Partition op = ps.getPartition(new OrderKey(orderId,customerId));
      System.out.printf("CustomerPartition: %s\n",cp.getPartitionId());
      System.out.printf("OrderPartition: %s\n",op.getPartitionId());
   }
}
\end{verbatim}

The Output would look something like this:

\begin{verbatim}
Customer Homer with id 9e556af1-6d63-4205-a70f-661ad7e9be72 and name Homer created
Order with id cb54f7ec-19f8-46b6-995a-a25b465c57fa created: customer: 9e556af1-6d63-4205-a70f-661ad7e9be72, articleId 123, quantity 1
CustomerPartition: 206
OrderPartition: 206
\end{verbatim}

As you can see, the partition where the Customer is stored, is the same as the partition where the Order of that same customer is stored. 

Being able to control what data is placed in the same partition, is a very powerful feature and needs to be taking into consideration from the beginning. Once this is done correctly, it will be a lot easier to write a high performance and scalable system. 

\section{Near Cache}

Till so far all our data is bound to a specific partition, e.g. the orders, the customer etc. But in some cases  data needs to be available on all machines, an example of such data is reference data. Hazelcast luckily supports  a feature called the near cache that makes data available on all partitions instead of a single one.

To explain the near cache, we are going to introduce the article domain object. We could define it like this:

\begin{verbatim}
public final class Article implements Serializable{
   private final String articleId = UUID.randomUUID().toString();
   private final String name;
   public Article(String name){this.name = name;}
   public String getArticleId(){return articleId;}
   public String getName(){return name;}
}
\end{verbatim}

If we want to verify that an article exist when an order is placed, we could do it like this:

\begin{verbatim}
public class OrderService{
   private final Map<String,Order> orderMap = Hazelcast.getMap("orders");
   private final Map<String,Article> articleMap = Hazelcast.getMap("articles");

   public void placeOrder(String customerId, String articleId){
      Article article = articleMap.get(articleId);
      if(article == null)throw new IllegalArgumentException();
      Order order = new Order(customerid, articleId);
      orderMap.put(order.getOrderId(),order);
   }
}
\end{verbatim}

Imagine that some articles, e.g. an IPad are very hot and will be ordered by many customer all the time. The problem is that because the IPad article object only lives at a single partition, that all other partitions need to make a potentially remote call to verify the existence of the IPad article. The consequence of doing remote calls is going to slow the system down (remote calls cause network traffic so prevent scalability and introduce latency).

Luckily Hazelcast provides another cool feature called the Near Cache. Items that are mostly read only, in this case an Article, will automatically be copied to all nodes that use it and not only to a single node that owns it. So if we have 5 nodes, and on all the 5 nodes the IPad article is ordered often, we are going to have 5 instances of the Ipad article because each node will have its own instance.

The Near Cache can be configured like this:
\begin{verbatim}
<hazelcast>
  ...
    <map name="articles">
      ...
      <near-cache>
        <time-to-live-seconds>0</time-to-live-seconds>
        <max-idle-seconds>60</max-idle-seconds>
        <eviction-policy>LRU</eviction-policy>
        <max-size>5000</max-size>
        <invalidate-on-change>true</invalidate-on-change>
      </near-cache>
    </map>
</hazelcast>
\end{verbatim}

todo: explain the different arguments
todo: give performance example with near cache enabled/disabled


\section{EntryListeners}

todo: explain entry listeners.

\section{What is next}
