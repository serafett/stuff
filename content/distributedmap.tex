\chapter{Distributed Map}
In this chapter, you'll learn how to use one of the most versatile data structures in Hazelcast; the com.hazelcast.core.IMap. The IMap extends the java.util.concurrent.ConcurrentMap interface, and therefor it also extends the java.util.Map but its implementation is designed to be used in a distributed environment. So changes made in one member are visible in another and when a member fails, backups will be restored an the cluster can continue as if nothing happened.

Internally Hazelcast divides the map in partitions (by default 271) and distributes the partitions evenly among the members in the cluster. The partition is determined based on the keyBased, so each key belongs to a single partition. 

Scaling up is simple; just add more members and they will take their share of the load. The oldest member decides which partitions need to be moved to the new members and chooses partitions that contain the least amount of data to reduce network traffic. Hazelcast doesn't do any runtime rebalancing of partitions e.g. based on partition size or usage. 

Scaling down also is simple; just shutdown the HazelcastInstances to release their share of the load. And the partitions will automatically be reassigned to other members. Of course a hard kill of the JVM is possible, but then you run a risk of loosing something.

[TODO: Give some idea about capacity in production.. number of entries... total size.

The commercial offering of Hazelcast includes Elastic Memory where the map entries are not stored in the Java Heap to drastically reduce gc times. On Youtube there is demo: http://www.youtube.com/watch?v=TOhbhKqJpvw where 4 Terabyte of data from 1 billion entries is stored on 100 Amazon EC2 instances, supporting to 1.3 million of operations/second.

\section{Reading/Writing}
The Hazelcast Map implements the ConcurrentMap/Map interface, so reading/writing key/values is very simple since you can use familiar methods like get/put etc. 

To demonstrate this basic behavior a distributed is created in a member and some entries are written to this map. In another member these entries are going to be read and printed:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.util.Map;
public class FillMapMember {
    public static void main(String[] args) {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
        Map<String, String> employees = hzInstance.getMap("employees");
        employees.put("1", "Tokyo");
        employees.put("2", "Paris");
        employees.put("3", "New York");
    }
}
\end{lstlisting}
As you can see the Map is retrieved using the hzInstance.getMap(mapName) and then some entries are stored in that map. Reading the entries can be done like this:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.util.Map;
public class PrintAllMember {
    public static void main(String[] args) {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
        Map<String, String> employees = hzInstance.getMap("employees");
        for(Map.Entry<String,String> entry : employees.entrySet())
            System.out.println(entry.getKey()+" "+entry.getValue());
    }
}
\end{lstlisting}
If the FillMapMember is runs before the PrintAllMember, we'll get output like this:
\begin{lstlisting}
1 Tokyo
2 Paris
3 New York
\end{lstlisting}
As you can see, the map updates from the FillMapMember are visible in the PrintAllMember.

Internally Hazelcast will serialize the key/values (see chapter Custom Serialization) to byte arrays and store this in the underlying storage mechanism. This means changes made to a key/value after they are stored in the map, will not reflect on the stored state. Therefor the following idiom is broken:
\begin{lstlisting}[language=java]
Employee e  = employees.get(123);
e.setFired(true);
\end{lstlisting}
If you want this change to be stored in the map, you need to put the updated value back:
\begin{lstlisting}[language=java]
Employee e  = employees.get(123);
e.setFired(true);
employees.put(123,e);
\end{lstlisting}

\subsection*{cache-value}
Apart from writing the update back to the map, there is another very important setting called 'cache-value'. When the value is read from a map, the value is deserialized since the map contains the serialized value. The problem is that this can lead to a lot of deserialization overhead if the value is read frequently. To reduce this overhead, if the member owns the key, the value instance can be cached. So multiple requests for some key that is owned by that member, will result in the same instance being returned instead of a copy. 

This works very fine if the value is an immutable object, but if the value is a mutable object, it can lead to unexpected sharing. This can cause all kinds of problems like race problems and JMM problems. By default 'cache-value' is set to true, so if you store mutable objects in your map, you need to explicitly set 'cache-value' to false, e.g:
\begin{lstlisting}[language=xml]
<hazelcast>
    <map name="employees">
       <cache-value>false</cache-value>
    </map>
</hazelcast>
\end{lstlisting}
This only need to be done with the map, the other data structures that are build on top of the map like the Queue do not cache the value. The cashed value is stored directly in local map entry, so the size of the cache will always be equal or smaller than the number of local map entries. This means that there will be extra memory usage, but it will not grow uncontrollable.

One thing to watch out for is that when you put an item in a cache-value enabled map, it will not return that instance on a subsequent read, but it will return a copy (due to deserialization). This copy will be cached, not the original instance.

\section{Hashcode and equals}
In most cases you probably will make use of some basic type like a Long, Integer or String as key. But in some cases you will need to create custom keys. But to do it correctly in Hazelcast, you need to understand how this mechanism works because it works differently compared to traditional map implementations. When you store a key/value in a Hazelcast map, instead of storing the Object, the object are serialized to byte arrays and these are stored. To use the hash/equals in Hazelcast you need to know the following rules:
\begin{enumerate}
\item for keys: the hash/equals is determined based on the content of the byte array, so equal keys need to result in equal byte arrays.
\item for values: the hash/equals is determined based on the deserialized object, and not the hash/equals of the byte array content. 
\end{enumerate}
As you can see the difference is subtile, but it is crucial to understand.

Below is an example of a key implementation that is valid for a map implementation like the HashMap, but invalid when it is used in a Hazelcast map:
\begin{lstlisting}[language=java]
public final class BrokenKey implements Serializable {
    private final String significant;
    private final String insignificant;
    public BrokenKey(String significant, String insignificant) {
        this.significant = significant;
        this.insignificant = insignificant;
    }
    public boolean equals(Object o) {
        if (this == o) return true;
        if (!(o instanceof BrokenKey)) return false;
        BrokenKey that = (BrokenKey) o;
        return that.significant.equals(this.significant);
    }
    public int hashCode() {
       return significant.hashCode();
   }
}
\end{lstlisting}
This BrokenKey has 2 fields; the significant field is used in the hash/equals implementation and the insignificant field is not. If we would make 2 keys:
\begin{lstlisting}
BrokenKey key1 = new BrokenKey("a","b");
BrokenKey key2 = new BrokenKey("a","c");
\end{lstlisting} 
then 'key1.equals(key2)' and 'key1.hashCode()==key2.hashCode()'. So if a value would be put with key1, it can be retrieved using key2. But because the byte array of key1 (which will contains 'a' and 'b') is different than the byte array of key2 (which will contain 'a' and 'c'), the hash code and equals be different. 

[TODO:Note that the distributed Set and List stores its entries as the keys in a distributed Map. So the notes above apply to the objects you store in Set and List.]

\section{Distributed Queries}
Imagine that we have a Hazelcast IMap where the key is some id and the value is Person object and we want to retrieve all persons with a given name. We could create the following very naive implementation:
\begin{lstlisting}[language=java]
   public Set<Person> getWithNameNaive(String name){
        Set<Person> result = new HashSet<Person>();
        for(Person person: personMap.values()){
            if(person.name.equals(name))
                result.add(person);
        }
        return result;
    }
\end{lstlisting}
This is what you probably would write if the map would be an ordinary map. But when the map is distributed map, there are some performance and scalability problems with this approach:
\begin{enumerate}
\item It is not parallelizable. One member will iterate over all persons instead of spreading the load over multiple members. Because the search isn't parallelizable, the system can't scale; you can add more members to the cluster to increase performance.
\item It is inefficient because all persons need to be pulled over the line before being deserialized into the memory of the executing member. So there is a huge amount of network traffic because all data go over the line.
\end{enumerate}

Luckily Hazelcast solves these problems by supporting predicates that are executed on top of a fork/join mechanism:
\begin{enumerate}
\item when the predicate is requested to be evaluated by the caller, it is forked to each member in the cluster
\item each member will filter all local map entries using the predicate. Before a predicate evaluates a map entry, the key/value of that entry are deserialized and passed to the predicate. 
\item the caller joins on the completion of all members and merges the results into a single set
\end{enumerate}
The fork/join approach is highly scalable because it parallelizable. By adding additional cluster members, the number of partitions per member is reduced and therefor the time a member needs to iterate over all its data, is reduced as well. Also the local filtering is parallelizable because a pool of 'partition threads' will evaluate segments of elements concurrently. And last but not least, the amount of network traffic is reduced drastically, since only filtered data is send instead of all data.

Hazelcast provides 2 API's for distributed queries:
\begin{enumerate}
\item Criteria API
\item Distributed SQL Query
\end{enumerate}

\subsection*{Criteria API}
To implement the Person search using the criteria API, it could be as simple as this:
\begin{lstlisting}[language=java]
    public Set<Person> getWithName(String name) {
        Expression getNameExpression = Predicates.get("name");
        Predicate predicate = Predicates.equal(getNameExpression, name);
        return (Set<Person>) personMap.values(predicate);
    }
\end{lstlisting}
First we need to get the name of the Person. This is done by the 'get' expression. Then we need to create an equal predicate that has as input the expression that gets the name and the other input is the name we are looking for. After we have created the predicate, we apply it to the personMap by calling the 'IMap.values(Predicate)' method which takes care of sending it to all members in the cluster, evaluating it, and merging the result. The Predicate is not limited to values only. It can also apply be applied to the keySet, the entrySet and the localKeySet of the IMap. 

\subsubsection*{Get expression}
In the previous example we already saw the get expression in action where it gets the name of the person object. When it is evaluated, it first tries to lookup an accessor method, so in case of 'name', the accessor methods it will try are 'isName()' and 'getName()'. If one found, it is called and the evaluation has completed. Of course an accessor method doesn't need to return a field, it could also be a synthetic accessor where some value is determined on the fly. If no accessor is found, a field with the given name is looked up. If that exists, it is returned and otherwise a RuntimeException is thrown. Hazelcast doesn't care about the accessibility of a field or an accessor method, so you are not forced to make them public.

In some cases you need to traverse over an object structure, e.g. we want the street of the address the person lives at. With the get expression this can be done like this: 'address.street'. This expression is evaluated from left to right and there is no limit on the number of steps involved. Also accessor methods can be used here. Another thing important to know is how the get expression deals with null, especially with object traversal. As soon null is found, null is returned instead of a NullPointerException being thrown. So if address would be null, the evaluation of the get expression 'address.street' will return null.

If you find the get expression too limited, you can create your own expression by extending the com.hazelcast.query.Expression interface:
\begin{lstlisting}[language=java]
public interface Expression<T> extends Serializable {
    T getValue(Object obj);
}
\end{lstlisting}

\subsubsection*{And, Or and Not predicates}
Predicates can be joined using the 'and' and 'or' predicate:
\begin{lstlisting}[language=java]
import static com.hazelcast.query.Predicates.*;
   ...
   public Set<Person> getWithNameAndAge(String name, int age) {
      Predicate namePredicate = equal(get("name"), name);
      Predicate agePredicate = equal(get("age"), age);
      Predicate predicate = and(namePredicate, agePredicate);
      return (Set<Person>) personMap.values(predicate);
   }
   public Set<Person> getWithNameOrAge(String name, int age) {
       Predicate namePredicate = equal(get("name"), name);
       Predicate agePredicate = equal(get("age"), age);
       Predicate Person = or(namePredicate, agePredicate);
       return (Set<Person>) personMap.values(predicate);
   }
\end{lstlisting}
And of course we can't forget the 'not' predicate:
\begin{lstlisting}[language=java]
    public Set<Person> getNotWithName(String name) {
        Predicate namePredicate = equal(get("name"), name);
        Predicate predicate = not(namePredicate);
        return (Set<Person>) personMap.values(predicate);
    }
\end{lstlisting}

\subsubsection*{Other predicates}
In the Predicates class you can find a whole collections of useful predicates:
\begin{enumerate}
\item notEqual: checks if the result of an expression is not equal to a certain value.
\item instanceOf: checks if the result of an expression has a certain type
\item like: checks if the result of an expression matches some string pattern. \% (percentage sign) is placeholder for many characters, \_ (underscore) is placeholder for only one character.
\item greaterThan: checks if the result of an expression is greater than a certain value.
\item greaterEqual: checks if the result of an expression is greater or equal than a certain value.
\item lessThan: checks if the result of an expression is less than a certain value
\item lessEqual: checks if the result of an expression is than than or equal to a certain value.
\item between: checks if the result of an expression is between 2 values (this is inclusive).
\item in: checks if the result of an expression is an element of a certain collection.
\item isNot: checks if the result of an expression is false.
\item regular expression: checks if the result of an expression matches some regular expression. Although there is no static convenience function for it on Predicates, the Predicates.RegexPredicate is publicly available.
\end{enumerate}
If the predicates provided by Hazelcast are not enough, you can always write your own predicate by implementing the Predicate interface:
\begin{lstlisting}[language=java]
public interface Predicate<K, V> extends Serializable {
    boolean apply(MapEntry<K, V> mapEntry);
}
\end{lstlisting}
The MapEntry not only contains the key/value, but also contains all kinds of metadata like the time it was created/expires/last-accessed etc. 

\subsubsection*{PredicateBuilder}
The syntax we used so far to create Predicates is clear but can be simplified by making use of the PredicateBuilder. It provides a fluent interface that can make building predicates simpler. But underwater the same functionality is being used. Here is an example where a predicate is build that selects all persons with a certain name and age using this PredicateBuilder:
\begin{lstlisting}[language=java]
    public Set<Person> getWithNameAndAgeSimplified(String name, int age) {
        EntryObject e = new PredicateBuilder().getEntryObject();
        Predicate predicate = e.get("name").equal(name).and(e.get("age").equal(age));
        return (Set<Person>) personMap.values(predicate);
    }
\end{lstlisting}
As you can see, it can simplify things, especially if you have complex predicates. But it is a matter of taste which approach you prefer.

\subsection*{Distributed SQL Query}
In the previous section the Criteria API was explained where expression/predicate objects are manually created. The pain can be reduced a bit by making use of the PredicateBuilder, but it still isn't perfect. That is why a DSL was added: the Distributed SQL Query, that is based on a SQL like language. But underwater the Criteria API is used. 

The 'get with name' function we already implemented using the Criteria API, can be implementing using the Distributed SQL Query like this:
\begin{lstlisting}
public Set<Person> getWithName(String name){
    Predicate predicate = new SqlPredicate(String.format("name = %s",name));
    return (Set<Person>) personMap.values(predicate);
}
\end{lstlisting}
As you can see, the SqlPredicate is a Predicate itself and therefor can be combined with the Criteria API. The language itself isn't case sensitive, but 'columns' used in the query are. Underneath you can see an overview of the DSL:
\begin{enumerate}
	\item logical operators
	\begin{lstlisting}
man and age>30
man=false or age = 45 or name = 'Joe'
man and (age >20 OR age < 30)
not man
	\end{lstlisting}

	\item relational operators
	\begin{lstlisting}
age <= 30
name ="Joe"
age != 30
	\end{lstlisting}

	\item between
	\begin{lstlisting}
age between 20 and 33
age not between 30 and 40
	\end{lstlisting}

	\item like
	\begin{lstlisting}
name like 'Jo%' (true for 'Joe', 'Josh', 'Joseph' etc.)
name like 'Jo_' (true for 'Joe'; false for 'Josh')
name not like 'Jo_' (true for 'Josh'; false for 'Joe')
name like 'J_s%' (true for 'Josh', 'Joseph'; false 'John', 'Joe')
	\end{lstlisting}
	\item in
	\begin{lstlisting}
age in (20, 30, 40)
age not in (60, 70)
	\end{lstlisting}
\end{enumerate}

\section{Indices}
To speed up queries, just like in databases, the Hazelcast map supports indices. Using an index prevents from iterating over all values (in database terms this is called a full table scan), but directly jump to the interesting ones. There are 2 types of indexes:
\begin{enumerate}
\item Ordered: e.g. a numeric field where you want to do searches like bigger than.
\item Unordered: e.g. a name field.
\end{enumerate}
In the previous chapter we talked a Person class which has a name, age etc. To speed up searching on these fields, we can place an unordered index on name and ordered index on age. 

To retrieve the index field of an Object, first an accessor method will be tried and if that doesn't exist, direct field access will be used. With the index accessor method you are not limited to returning a field, you could have a synthetic accessor method where some value is calculated on the fly. The index field also supports object traversal, so you could create an index on the street of the address of a person using 'address.street'. There is no limitation on the depth of the traversal. Hazelcast doesn't care about the accessibility of the index field or accessor method, so you are not forced to make them public. An index field or an object containing an field, for the 'x.y' notation, is allowed to be null.

The indices on the personMap can be configured like this:
\begin{lstlisting}[language=xml]
<hazelcast>
   <map name="persons">
      <indexes>
         <index ordered="false">name</index>
         <index ordered="true">age</index>
      </indexes>
   </map>
</hazelcast>
\end{lstlisting}
The ordered attribute defaults to 'false'. It also is possible to call the 'IMap.addIndex(String name,boolean ordered)' method directly, but adding the index should be done before an item is placed in the map otherwise the index is ignored. Sometimes by design adding an index to a map may be impossible before any value is added. For example if a map has a MapLoader that loads entries during map creation. Another disadvantage of doing it in code is that it can become hard to figure out if a field has an index, because it could be spread over the code.

The performance impact of using one or more indices depends on a lot of factors; among them are the size of the map, the chance of finding the element with a full table scan etc. Also adding one or more indices make mutations to the map more expensive since the index needs to be updated as well. Therefor you need to test in a production like environment and a representative size/quality of the dataset, which configuration is best for you. In the source code you can 2 very rudimentary index benchmarks, one for updating and one for searching. 

In Hazelcast versions prior to 3.0, indexing for String fields was done only for the first the 4 characters. With Hazelcast version 3.0+ indexing is done on the entire String.

[todo: how is the ordering implemented, for numeric values is can be simple, but what about strings?]

\section{Data locality}
In some cases you want data to be stored in the same partition to have improved locality of reference; an important feature for performance. This prevents going over the network, since the data can be accessed locally since it is stored in the same partition.

To explain data locality, imagine that you have a Customer:
\begin{lstlisting}[language=java]
import java.io.Serializable;
public class Customer implements Serializable {
    public final long id;
    public Customer(long id) {
        this.id = id;
    }
}
\end{lstlisting}
And imagine that each customer can have orders:
\begin{lstlisting}[language=java]
import java.io.Serializable;
public final class Order implements Serializable {
    public final long orderId, customerId, articleId;
    public Order(long orderId, long customerId, long articleId) {
        this.orderId = orderId;
        this.customerId = customerId;
        this.articleId = articleId;
    }
}
\end{lstlisting}
And imagine that customers and orders are stored in a map like this:
\begin{lstlisting}[language=java]
    long customerId = 100;
    long orderId = 200;
    long articleId = 300;
    Customer customer = new Customer(customerId);
    customerMap.put(customer.id, customer);
    Order order = new Order(orderId,customer.id,articleId)
	orderMap.put(order.id, order)
\end{lstlisting}
The problem with this approach is that it is very likely that the customer is going to be stored in a different partition than its orders because the customer is partitioned based on the customer id, and the orders are partitioned on the order id.

Luckily Hazelcast provides a way of storing the Customer and the Order in the same partition, using of the PartitionAware interface. If a key implements this interface, instead of using the key to determine the correct partition, the PartitionAware.getPartitionKey() will be used. In our case we introduce an intermediate object: the OrderKey that implements this PartitionAware interface and returns the customerId as the partitionKey:
\begin{lstlisting}[language=java]
import com.hazelcast.core.PartitionAware;
import java.io.Serializable;
public final class OrderKey implements PartitionAware, Serializable {
    public final long orderId, customerId;
    public OrderKey(long orderId, long customerId) {
        this.orderId = orderId;
        this.customerId = customerId;
    }
    public Object getPartitionKey() {
        return customerId;
    }
}
\end{lstlisting}
The equals/hashcode don't need to be implemented, since the equals/hash are determined based on the byte-array representation of the OrderKey. It can be integrate it in code and verified that the partition of the customer is the same as the partition of the order like this:
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import com.hazelcast.partition.*;
import java.util.Map;
public class DataLocalityMember {
    public static void main(String[] args) {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
        Map<Long, Customer> customerMap = hzInstance.getMap("customers");
        Map<OrderKey, Order> orderMap = hzInstance.getMap("orders");
        long customerId = 100;
        long orderId = 200;
        long articleId = 300;
        Customer customer = new Customer(customerId);
        customerMap.put(customer.id, customer);
        OrderKey orderKey = new OrderKey(orderId,customer.id);
        Order order = new Order(orderKey.orderId,customer.id,articleId);
        orderMap.put(orderKey, order);
        PartitionService pService = hzInstance.getPartitionService();
        Partition cPartition = pService.getPartition(customerId);
        Partition kPartition = pService.getPartition(new OrderKey(orderId, customerId));
        Partition oPartition = pService.getPartition(orderId);
        System.out.printf("Partition for customer: %s\n", cPartition.getPartitionId());
        System.out.printf("Partition for order with OrderKey: %s\n", kPartition.getPartitionId());
        System.out.printf("Partition for order without OrderKey: %s\n", oPartition.getPartitionId());
    }
}
\end{lstlisting}
The Output looks something like this:
\begin{lstlisting}
Partition for customer: 124
Partition for order with OrderKey: 124
Partition for order without OrderKey: 175
\end{lstlisting}
As you can see, the partition where the Customer is stored, is the same as the partition where the Order of that same customer is stored. 

Being able to collocate data in the same partition, is a very powerful feature and needs to be taken into consideration from the beginning and therefor is a very important architectural concern. Once this is done correctly, it will be a lot easier to write a high performance and scalable system. The technique of collocating data in a single partition, can be combined with sending functionality to the data instead of the data to the functionality. So if you need to do some operation, send that operation to the right partition where all the data for that operation can be accessed locally. For more information see "chapter: Executor and Routing" and "chapter: Distributed Services and routing".

\section{High availability}
In production a environment all kinds of things can go wrong. A machine could break down due to disk failure, the operating system crashes, it could get disconnected from the network. To prevent that the failure of a single member leads to failure of the cluster, by default Hazelcast synchronously backs up all map entries on another JVM, so if a member fails, the backup still contain the data. The backup-count can be configured using the 'backup-count' property:
\begin{lstlisting}[language=xml]
<hazelcast> 
    <map name="persons"> 
         <backup-count>1</backup-count>
    </map> 
</hazelcast>
\end{lstlisting}
The backup-count can be set to 0, if you favor performance over high availability, but you can also specify a higher value than 1 of you want to have more backups. The default is 1, so in a lot of cases you don't need to specify it.

By default the backup operations are synchronous; so you have the guarantee that the backup(s) are updated before continuing. But this guarantee comes at the cost of blocking and therefor the latency increases. In some cases having a low latency is more important, but working without a backup is not an option as long as the window for failure is small. That is why Hazelcast also supports asynchronous backups; so the backups are made at some point in time. This can be configured through the 'async-backup-count' property:
\begin{lstlisting}[language=xml]
<hazelcast> 
    <map name="persons"> 
         <async-backup-count>1<async-backup-count>
    </map> 
</hazelcast>
\end{lstlisting}
This property defaults to 0. 

By default Hazelcast provides sequential consistency, meaning that when a map entry is read, always the most recent change will be seen. This is done by routing the get request to the member that owns the key. But sequential consistency comes at a price since remoting is involved when the key is owned by a different member. Hazelcast provides the option to reduce consistency by allowing to read from backup data instead and potentially see stale data. This feature only is available when there is at least 1 backup (doesn't matter if it a synchronous or asynchronous backup) and can be enabled by setting the read-backup-data property:
\begin{lstlisting}[language=xml]
<hazelcast>
    <map name="persons">
        <backup-count>0</backup-count>
        <async-backup-count>1</async-backup-count>
        <read-backup-data>true</read-backup-data>
    </map>
</hazelcast>
\end{lstlisting}
In this example you can see a person map with a single asynchronous backup and where read backup data is enabled. This property defaults to false.

\section{Persistence}
In the previous section we talked about backups that protect against member failure, so if one member goes down, another member takes over. But it does not protect you against cluster failure, e.g. when a cluster is hosted in a single datacenter, and it goes down. Luckily Hazelcast provides a solution loading and storing data externally, e.g. in a database. This can be done using:
\begin{enumerate}
\item com.hazelcast.core.MapLoader: useful for reading entries from an external datasource, but changes don't need to be written back.
\item com.hazelcast.core.MapStore: useful for reading and writing map entries from and to an external datasource. The MapStore interface extends the MapLoader interface.
\end{enumerate}
 And one instance per Map per Node will be created.

The following example shows an extremely basic HSQLDB implementation of the MapStore where we load/store a simple Person object with a name field:
\begin{lstlisting}[language=java]
import com.hazelcast.core.MapStore;
import java.sql.*;
import java.util.*;
import static java.lang.String.format;
public class PersonMapStore implements MapStore<Long, Person> {
    private final Connection con;
    public PersonMapStore() {
        try {
            con = DriverManager.getConnection("jdbc:hsqldb:mydatabase", "SA", "");
            con.createStatement().executeUpdate(
                    "create table if not exists person (id bigint, name varchar(45))");
        } catch (SQLException e) {throw new RuntimeException(e);}
    }
    public synchronized void delete(Long key) {
        try {
            con.createStatement().executeUpdate(
                    format("delete from person where id = %s", key));
        } catch (SQLException e) {throw new RuntimeException(e);}
    }
    public synchronized void store(Long key, Person value) {
        try {
            con.createStatement().executeUpdate(
                    format("insert into person values(%s,'%s')", key, value.name));
        } catch (SQLException e) {throw new RuntimeException(e);}
    }
    public synchronized void storeAll(Map<Long, Person> map) {
        for (Map.Entry<Long, Person> entry : map.entrySet())
            store(entry.getKey(), entry.getValue());
    }
    public synchronized void deleteAll(Collection<Long> keys) {
       for(Long key: keys) delete(key);
    }
    public synchronized Person load(Long key) {
        try {
            ResultSet resultSet = con.createStatement().executeQuery(
                    format("select name from person where id =%s", key));
            try {
                if (!resultSet.next()) return null;
                String name = resultSet.getString(1);
                return new Person(name);
            } finally {
                resultSet.close();
            }
        } catch (SQLException e) {throw new RuntimeException(e);}
    }
    public synchronized Map<Long, Person> loadAll(Collection<Long> keys) {
        Map<Long, Person> result = new HashMap<Long, Person>();
        for (Long key : keys) result.put(key, load(key));
        return result;
    }
    public Set<Long> loadAllKeys() {
         return null;
    }
}
\end{lstlisting}
As you can see the implementation is quite simple and certainly can be improved, e.g. transactions, prevention against sql injection etc. Because the MapStore/MapLoader can be called by threads concurrently, this implementation make use of synchronization to deal with that correctly. Currently it relies on a course grained locked, but you could perhaps apply finer grained locking based on the key and a striped lock.

To connect the PersonMapStore to the persons map, we can configure it using the 'map-store' setting:
\begin{lstlisting}[language=xml]
<hazelcast>
    <map name="persons">
        <map-store enabled="true">
           <class-name>PersonMapStore</class-name>
        </map-store>
    </map>
</hazelcast>
\end{lstlisting}
In the following code fragment you can see a member that writes a person to the map exits the JVM. And you can see a member that loads the person and prints it. 
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class WriteMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
        IMap<Long, Person> personMap = hzInstance.getMap("personMap");
        personMap.put(1L, new Person("Peter"));
        System.exit(0);
    }
}

import com.hazelcast.core.*;
public class ReadMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
        IMap<Long, Person> personMap = hzInstance.getMap("personMap");
        Person p = personMap.get(1L);
        System.out.println(p);
    }
}
\end{lstlisting}
With the WriteMember you can see that the System.exit(0) is called at the end. This is done to release the HSQLDB so that it can be opened by the ReadMember. Calling System.exit is a safe way for Hazelcast to leave the cluster  due to a shutdown hook, and it waits for all backup operations to complete.

A word of caution: the MapLoader/MapStore should NOT call Map/Queue/MultiMap/List/Set/etc operations, otherwise you might run into deadlocks. 

\subsubsection{Pre-Populating the map}
With the MapLoader it is possible to pre-populate the Map so that when it is created, the important entries are loaded in memory. This can be done by letting the 'loadAllKeys' method return the Set of all 'hot' keys that need to be loaded for the partitions owned by the member. This also makes parallel loading possible, since each member can load its own keys.  If the 'loadAll' method return null, as we did in the example, then the map will not be pre-populated. Also important to know is that Map is created lazily by Hazelcast, so only when one of the members calls the 'HazelcastInstance.getMap(name)' the map is actually created and the MapLoader called. If your application requires that Map up front without really needing the content, you could wrap the map in a lazy proxy that calls the getMap method only when really needed.

I common mistake made is that the 'loadAllKeys' returns all keys in the database table. This could be problematic since you would pull the complete table in memory, but another important problem is that if each member returns the all keys, each member will load the complete table from the database. So if a you have 1.000.000 records in the database, and 10 members, then the total number of records loaded is 10.000.000 instead of 1.000.000. Of course the Map.size will still be 1.000.000. That is why a member should only load the records it owns, e.g. by adding the 

[todo: do not return all keys in the database; since each member is going to do the same. Is there any form of protection against this? Because when new members are added, they will load the map and therefor the load-all-keys method is called. So how can you do this once only?]
[todo: how to figure out in which partition you are]
[todo: how to figure out which data to load from the database]

You need to be aware of that the map only knows about map entries that are in memory, only when a get is done for an explicit key, then the map entry is loaded from the MapStore. This behavior is called a read through. So if the loadAll would return a subset of the keys in the database, then e.g. the Map.size() will show only the size of this subset, not the record count in the database. And the same goes for queries; these will only be executed on the entries in memory, not on the records in the database.

To make sure that you only keep hot entries in memory, you can configure the 'time-to-live-seconds' property on the Map. When a Map entry isn't used and the time to live expires, it will automatically be removed from the map without calling the MapStore.delete. 

\subsubsection*{Write Through vs Write Behind}
Although the MapStore makes durability possible, it also comes at a cost: every time that a change is made in the map, a write through to the your persistence mechanism happens. Write through operations increase latency since databases cause latency (e.g. disk access). In Hazelcast it is possible to use a write behind instead of a write through. When a change happens, the change is synchronously written to the backup partition (if that is configured), but the change to the database is done asynchronously. Enabling write behind can be done by configuring the 'write-delay-seconds' in the 'map-store' configuration section. It defaults to 0, which means a write through. A value higher than 0 indicates a write behind. Using write behind is not completely without danger, it could happen that the cluster fails before the write to the database has completed. In that case information could be lost.

\subsubsection*{MapLoaderLifecycleSupport}
In some cases your MapLoader needs to be notified of lifecycle events. This can be done by letting the MapLoader implementation implement the com.hazelcast.core.MapLoaderLifecycleSupport interface. This signals to Hazelcast that the implementation is interested in:
\begin{enumerate}
\item init: useful if you want to initialize resources like opening database connections. One of the parameters the init method receives is a Properties object. This is useful if you want to pass properties from the outside to the MapLoader implementation. If you make use of the xml configuration, in the map-store xml configuration you can specify the properties that need to be passed to the init method.
\item destroy: useful if need to cleanup resources like closing database connections.
\end{enumerate}

\section{Eviction}
By default all the map entries that are put in the map, will stay there. You can choose to delete them manually, but you can also rely on an eviction policy that takes care of deleting items automatically. This feature enables Hazelcast to be used as a distributed cache since hot data will be kept in memory. 

The eviction configuration can be done using the following parameters:
\begin{enumerate}
\item max-size: Maximum size of the map. When max size is reached, map is evicted based on the policy defined. Any integer between 0 and Integer.MAX VALUE. 0 means Integer.MAX\_VALUE and the default is 0. There also is a policy attribute, that determines how the max-size is going to interpreted:
   \begin{enumerate}
   		\item map\_size\_per\_jvm: maximum number of map entries in the JVM. This is the default policy.
   		\item cluster\_wide\_map\_size: total number of map entries in the cluster. 
   		\item partitions\_wide\_map\_size: Partitions (default 271) wide max map size.[todo]
   		\item used\_heap\_size:Max used heap size in MB (mega-bytes) per JVM.
   		\item used\_heap\_percentage:Max used heap size percentage per JVM.
   \end{enumerate}	
\item eviction-policy:
   \begin{enumerate}
   	 	\item NONE: No items will be evicted, so the max-size is ignored. This is the default policy. If you want max-size to work you need to set an eviction-policy other than NONE. Of course, you still can combine it with time-to-live-seconds and  max-idle-seconds.
	 	\item LRU: Least Recently Used
	 	\item LFU: Least Frequently Used.
	\end{enumerate}
\item time-to-live-seconds: Maximum number of seconds for each entry to stay in the map. Entries that are older than time-to-live-seconds and not updated for time-to-live-seconds will get automatically evicted from the map. Any integer between 0 and Integer.MAX\_VALUE. 0 means infinite. Default is 0.
\item max-idle-seconds:  Maximum number of seconds for each entry to stay idle in the map. Entries that are idle (not touched) for more than max-idle-seconds will get automatically evicted from the map. Entry is touched if get, put or containsKey is called. Any integer between 0 and Integer.MAX\_VALUE. 0 means infinite. Default is 0.
\item eviction-percentage:  When max. size is reached, specified percentage of the map will be evicted. Any integer between 0 and 100. If 25 is set for example, 25 \% of the entries will get evicted.
\item eviction-delay-seconds: Specifies when eviction will be started. Default value is 3. So every 3 (+up to 5 for performance reasons) seconds eviction will be kicked of. Eviction is costly operation, setting this number too low, can decrease the performance.
\end{enumerate}

An example configuration:
\begin{lstlisting}[language=xml]
<hazelcast>
    <map name="articles">
        <max-size policy="map_size_per_jvm">10000</max-size>
        <eviction-policy>LRU</eviction-policy>
        <max-idle-seconds>60</max-idle-seconds>
    </map>
</hazelcast>
\end{lstlisting}
This configures an articles Map that will start to evict map entries from a member, as soon as the map size within that member exceeds 10000. It will then start to remove map entries that are least recently used. Also when map entries are not used for more than 60 seconds, they will be evicted as well.

Important: When an item is evicted, the 'MapStore.delete(Object key)' method is not called.

\section{Near Cache}
Till so far all the map entries are owned by a single member, so if a value is needed, the member that owns the key is used to read the value. But in some cases data needs to be read very frequently by members that don't own the key. The problem is that most requests will require remoting, and therefor have a high latency. Luckily Hazelcast has a feature called the near cache that makes map entries locally available by adding a local cache attached to the the map. 

Imagine that we have a web-shop where articles can be ordered and where these articles are stored in a Hazelcast map. To enable local caching of frequently used articles, the near cache needs to be configured like this:
\begin{lstlisting}[language=xml]
<hazelcast>
    ...
    <map name="articles">
      <near-cache/>
    </map>
</hazelcast>
\end{lstlisting}
On the near cache the following properties can be configured:
\begin{enumerate}
\item max-size: the maximum number of cache entries per local cache. As soon as the maximum size has been reached, the cache will start to evict entries based on the eviction policy. max-size should be between 0 and Integer.MAX\_SIZE, where 0 will be interpreted as Integer.MAX\_SIZE. The default is 0, but it probably is better to explicitly configure max-size in combination with eviction-policy, or else time-to-live-seconds/max-idle-seconds to prevent OutOfMemoryErrors.
\item eviction-policy: the policy used to evict members from the cache when the near cache is full. The following options are available:
\begin{enumerate}
 \item NONE: No items will be evicted, so the max-size is ignored. This is the default policy. If you want max-size to work you need to set an eviction-policy other than NONE. Of course, you still can combine it with time-to-live-seconds and  max-idle-seconds.
 \item LRU: Least Recently Used
 \item LFU: Least Frequently Used.
\end{enumerate}
\item time-to-live-seconds: the number of seconds a map entry is allowed to remain in the cache. Valid values are 0 to Integer.MAX\_SIZE, and 0 will be interpreted as infinite. The default is 0.
\item max-idle-seconds: the maximum number of seconds a map entry is allowed to stay in the cache without being read. max-idle-seconds should be between 0 and Integer.MAX\_SIZE, where 0 will be interpreted as Integer.MAX\_SIZE. The default is 0. 
\item invalidate-on-change: should all the members listen to change of their cached entries and evict the entry when updated or deleted. Valid values are true/false and defaults to true. 
\end{enumerate}

An example configuration:
\begin{lstlisting}[language=xml]
<hazelcast>
    <map name="articles">
        <near-cache/>
            <max-size>10000</max-size>
            <eviction-policy>LRU</eviction-policy>
            <max-idle-seconds>60</max-idle-seconds>
        </near-cache>
    </map>
</hazelcast>
\end{lstlisting}
This configures an articles Map with a near-cache, that will start to evict near-cache entries from a member, as soon as the near-cache size within that member exceeds 10000. It will then start to remove near-cache entries that are least recently used. Also when near cache entries are not used for more than 60 seconds, they will be evicted as well.

In the previous 'Eviction' section we talked about evicting items from the map, but it is important to understand that the near cache and map eviction are two different things. The near cache is a local map that contains frequently accessed map entries from any member, the local map will only contain map entries it owns. You can even combine the eviction and the near cache; although their settings are independent. 

Some things worth considering when using a near cache:
\begin{enumerate}
\item increases memory usage since the near cache items need to be stored in the memory of the member
\item reduces consistency, especially when 'invalidate-on-change' is false: it could be that a cache entry never is refreshed.
\item it is best used for read only data. Especially when 'invalidate-on-change' is enabled, there is a lot of remoting involved to invalidate the cache entry, when a map entry is updated.
\end{enumerate}

The near cache obeys the 'cache-value' property on the map.

[todo: Are the near caches updates synchronously or asynchronously?]
[todo: on the map roughly the same properties can be found.. are they ignored? Can you enter both?]
[todo: does the near cache also cache the value or does it always return a deserialized version (so a fresh copy)]

\section{Concurrency Control}
The Hazelcast map itself is thread-safe just like the ConcurrentHashMap or the Collections.synchronizedMap, but in some cased your thread safety requirements are bigger than what hazelcast provides out of the box. Luckily Hazelcast provides multiple concurrency control solutions;  it can either be pessimistic using locks or optimistic using compare and swap operations. Todo: what about transactions?

Take a look at the following example; if run by multiple members in parallel the total amount would be 
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.io.Serializable;
public class RacyUpdateMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
        IMap<String, Value> map = hzInstance.getMap("map");
        String key = "1";
        map.put(key, new Value());
        System.out.println("Starting");
        for (int k = 0; k < 1000; k++) {
            if(k%100 == 0) System.out.println("At: "+k);
            Value value = map.get(key);
            Thread.sleep(10);
            value.field++;
            map.put(key, value);
        }
        System.out.println("Finished! Result = " + map.get(key).field);
    }
    static class Value implements Serializable {
        public int field;
    }
}
\end{lstlisting}

\subsection{Pessimistic Locking}
The classic way to solve the race problem is to use a lock. In Hazelcast there are various ways to lock, but for this example we'll use the locking functionality provided by the map using the map.lock/map.unlock methods.

[Todo: are these reentrant?]
[todo: are these fair?]
[also the regular locking rules apply, don't lock too long, don't lock too much.]

\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.io.Serializable;
public class PessimisticUpdateMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
        IMap<String, Value> map = hzInstance.getMap("map");
        String key = "1";
        map.put(key, new Value());
        System.out.println("Starting");
        for (int k = 0; k < 1000; k++) {
            map.lock(key);
            try {
                Value value = map.get(key);
                Thread.sleep(10);
                value.field++;
                map.put(key, value);
            } finally {
                map.unlock(key);
            }
        }
        System.out.println("Finished! Result = " + map.get(key).field);
    }
    static class Value implements Serializable {
        public int field;
    }
}
\end{lstlisting}
Another way to lock is to acquire some predictable Lock object from Hazelcast. You could give every value its own lock, but you could also create a stripe of locks. Although it potentially can increase contention, it will reduce space.

todo: when a key is locked, it still is possible to do a read, but not to do a write.

\subsection{Optimistic Locking}
TODO: Does optimistic rely on a lock or is truly using cas?

It is important to implement equals on the value, because this is used to determine of 2 objects are equal. With the ConcurrentHashMap it is based on object reference. On the keys the byte-array equals is used, but on the replace(key,oldValue,newValue) the equals is used. If you fail to forget it, your code will not work!

\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.io.Serializable;
//This code is broken on purpose.
public class OptimisticMember {
    public static void main(String[] args) throws Exception {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
        IMap<String, Value> map = hzInstance.getMap("map");
        String key = "1";
        map.put(key, new Value());
        System.out.println("Starting");
        for (int k = 0; k < 1000; k++) {
            if(k%10==0) System.out.println("At: "+k);
            for (; ; ) {
                Value oldValue = map.get(key);
                Value newValue = new Value(oldValue);
                //   Thread.sleep(10);
                newValue.field++;
                if(map.replace(key, oldValue, newValue))
                    break;
            }
        }
        System.out.println("Finished! Result = " + map.get(key).field);
    }
    static class Value implements Serializable {
        public int field;
        public Value(){}
        public Value(Value that) {
            this.field = that.field;
        }
        public boolean equals(Object o){
            if(o == this)return true;
            if(!(o instanceof Value))return false;
            Value that  = (Value)o;
            return that.field == this.field;
        }
    }
}
\end{lstlisting}
Aba problem; it can be that the following thing happens. And explain how it can be solved by adding a version; although all the other fields will be equal, the version field will prevent objects from being seen as equal.

\subsection{Pessimistic vs Optimistic}
TODO: When to choose one, when to choose the other. Normally optimistic performs better if there is not that much contention. What is the reason to do so in Hazelcast? Is it better performing?

\section{MultiMap}
In some cases you need to store multiple values for a single key. You could use a normal collection as value and store the 'real' values in this collection. The problem with this approach is that the whole collection needs to be deserialized for an operation like add. This can cause a lot of overhead, cpu, memory, network usage. To solve this problem Hazelcast provides a MultiMap where multiple values can be stored for a single key. 

The MultiMap doesn't implement the java.util.Map interface since the signatures of the method are different. The MultiMap does have support for most of the functionality provided by the IMap (so locking, listeners etc). But it doesn't support indexing, predicates and the MapStore.

To demonstrate the MultiMap where are going to create 2 member; in one member dummy data is created and in the other member it is read and written to the console.
\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
public class PutMember {
    public static void main(String[] args){
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
        MultiMap<String,String> map = hzInstance.getMultiMap("map");
        map.put("Peter","England");
        map.put("Peter","Holland");
        map.put("Talip","Turkey");
    }
}
\end{lstlisting}

\begin{lstlisting}[language=java]
import com.hazelcast.core.*;
import java.util.Collection;
public class PrintMember {
    public static void main(String[] args) {
        HazelcastInstance hzInstance = Hazelcast.newHazelcastInstance();
        MultiMap<String,String> map = hzInstance.getMultiMap("map");
        for(String key: map.keySet()){
            Collection<String> values = map.get(key);
            System.out.printf("%s -> %s\n",key,values);
        }
    }
}
\end{lstlisting}
It is important to realize that map.get(key) returns a copy of the values at some moment in time. Changes to this collection will result in an a UnsupportedOperationException. So if you want to change the values, you need to do it through the map.remove methods.

Removing items; you can remove items from the MultiMap. If the collection for a specific key is empty, this collection will not automatically be removed, so it can be that you need to clean up the MultiMap so that no empty value collections remains.

[todo: list and set based multimap]
[todo: config example]

The value collection is set based, so no duplicates and there is no ordering.Null value is not allowed (npe). But the MultiMap can also be configured to be list based: MultiMapConfig.setValueCollectionType(ValueCollectionType.LIST). 

One thing worth knowing; if a value collection with K is stored on member1 because K is owned by that member1, and member2 does a map.get(K), then the whole collection will transported from member1 to member 2. So if that value collection is big, it could lead to performance problems. A solution would be to send the whole operation to member1, so send the logic to the data instead of the data to the logic.

\section{Gotcha's}
When Map.entrySet(), Map.keySet() or Map.values() is called, a snapshot of the current state of the map is returned. Changes that are made in the map, so not reflect into changes these maps. Also when changes are made on these collections, an UnsupportedOperationException is thrown.

TODO: What about cache where serialization for local object is 'skipped?

\section{What is next}
In this chapter a lot of the basic operations on the Distributed Map were explained. In the next chapter we'll cover more advanced features.

[todo: tell about garbage collection.. or the lack of it..]
[todo: local entry listener]
[todo: local entry set.. makes it easy to create a clustered solution where each member takes care of some parts of the tasks.. no central coordination needed]
[TODO: ConcurrentMap operations never throw java.util.ConcurrentModificationException. Add somewhere.]
[todo: hazelcast 3 has 'executor on key']
[todo: continuous listener; listener with predicate]


